{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Submission: Toxic Language Classification \n",
    "**w207 Spring 2018 - Final Project Baseline**\n",
    "\n",
    "**Team: Paul, Walt, Yisang, Joe**\n",
    "\n",
    "\n",
    "\n",
    "### Project Description \n",
    "\n",
    "Our challenge is to build a multi-headed model thatâ€™s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.  The toxic language data set is sourced from Wikipedia and available as a public kaggle data set. \n",
    "\n",
    "Our goal is to use various machine learning techniques used in class to develop high quality ML models and pipelines.  \n",
    "\n",
    "1. Exercise and build upon concepts covered in class and test out at least 3 kinds of supervised models:\n",
    "    a. Regression (LASSO, Logistic)\n",
    "    b. Trees (RF, XGBoost)\n",
    "    c. DeepLearning (Tensorflow)\n",
    "2. Using stacking/ensembling methods for improving prediction metrics (K-Means, anomaly detection)\n",
    "3. Using unsupervised methods for feature engineering/selection\n",
    "\n",
    "For the baseline proposal, this file contains a first pass run through from data preprocessing to model evaluation using a regression model pipeline. \n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bokeh in /anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six>=1.5.2 in /anaconda3/lib/python3.6/site-packages (from bokeh)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /anaconda3/lib/python3.6/site-packages (from bokeh)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /anaconda3/lib/python3.6/site-packages (from bokeh)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /anaconda3/lib/python3.6/site-packages (from bokeh)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /anaconda3/lib/python3.6/site-packages (from bokeh)\n",
      "Requirement already satisfied: tornado>=4.3 in /anaconda3/lib/python3.6/site-packages (from bokeh)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /anaconda3/lib/python3.6/site-packages (from Jinja2>=2.7->bokeh)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named bokeh",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-96ad7eec18ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#! pip install bokeh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named bokeh"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "#import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 159571\n",
      "training data shape: (111745,)\n",
      "training label shape: (111745, 6)\n",
      "dev label shape: (47826, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "print 'total training observations:', train_df.shape[0]\n",
    "print 'training data shape:', train_data.shape\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how imblanced the label set is in order to have a better understanding with the label quality of the given data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bokeh.io import push_notebook\n",
    "#from bokeh.plotting import figure, show, output_file, output_notebook\n",
    "\n",
    "#target_counts = train_labels.apply(np.sum,0)\n",
    "#target_counts\n",
    "\n",
    "#output_notebook()\n",
    "\n",
    "\n",
    "#p = figure(x_range=target_names)\n",
    "#p.vbar(x=target_names, top = target_counts, width=0.9)\n",
    "\n",
    "#show(p)\n",
    "\n",
    "#train_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is fairly imbalanced when counting label occurrences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to consider\n",
    "- Sampling methods\n",
    "- Custom Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering/Selection (WIP)\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bonjour'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "def substitute_repeats_fixed_len(text, nchars, ntimes=4):\n",
    "    # Find substrings that consist of `nchars` non-space characters\n",
    "    # and that are repeated at least `ntimes` consecutive times,\n",
    "    # and replace them with a single occurrence.\n",
    "    # Examples: \n",
    "    # abbcccddddeeeee -> abcde (nchars = 1, ntimes = 2)\n",
    "    # abbcccddddeeeee -> abbcde (nchars = 1, ntimes = 3)\n",
    "    # abababcccababab -> abcccab (nchars = 2, ntimes = 2)\n",
    "    return re.sub(r\"(\\S{{{}}})(\\1{{{},}})\".format(nchars, ntimes-1),\n",
    "                      r\"\\1\", text)\n",
    "\n",
    "def substitute_repeats(text):\n",
    "    # Truncate consecutive repeats of short strings\n",
    "    ntimes=4\n",
    "    for nchars in range(1, 20):\n",
    "        text = substitute_repeats_fixed_len(text, nchars, ntimes)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train = train_data.apply(substitute_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is: 204\n",
      "Number of nonzero entries in matrix: 3834074\n"
     ]
    }
   ],
   "source": [
    "# Basic Count Vectorizer\n",
    "countVector = CountVectorizer(ngram_range=(1,1), analyzer = substitute_repeats)\n",
    "train_counts = countVector.fit_transform(processed_train)\n",
    "\n",
    "print(\"Vocabulary size is: {}\").format(len(countVector.vocabulary_))\n",
    "print(\"Number of nonzero entries in matrix: {}\").format(train_counts.nnz)\n",
    "\n",
    "#sample column wise sum, we can see that an observation can have multiple classes. \n",
    "#count_df = pd.DataFrame(train_labels.apply(np.sum,1), columns = [\"counts\"])\n",
    "#count_df = count_df[((count_df[\"counts\"] >= 1))]\n",
    "#count_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Pass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.758499040227\n",
      "CV score for class severe_toxic is 0.801726212433\n",
      "CV score for class obscene is 0.7641208495\n",
      "CV score for class threat is 0.673444625848\n",
      "CV score for class insult is 0.755260135879\n",
      "CV score for class identity_hate is 0.68428326399\n",
      "Mean ROC_AUC: 0.739555687979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "# Basic Logistic Regression Model/MultiLabel Edition\n",
    "\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver='sag') \n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, train_counts, train_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(name, cv_score))\n",
    "    classifier.fit(train_counts, train_labels[name])\n",
    "    \n",
    "print(\"Mean ROC_AUC: {}\").format(np.mean(scores_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev score for class toxic is 0.675413012295\n",
      "Dev score for class severe_toxic is 0.581701390358\n",
      "Dev score for class obscene is 0.631987934387\n",
      "Dev score for class threat is 0.503865476473\n",
      "Dev score for class insult is 0.60758283931\n",
      "Dev score for class identity_hate is 0.521895703782\n",
      "Mean(dev) ROC_AUC: 0.676147422434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "dev_Vector = CountVectorizer(ngram_range=(1,1))\n",
    "dev_counts = countVector.fit_transform(dev_data)\n",
    "\n",
    "pred_dt = pd.DataFrame()\n",
    "scores_dev = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver='sag') \n",
    "    classifier.fit(dev_counts, dev_labels[name])\n",
    "    scores_dev.append(cv_score)\n",
    "    output = classifier.predict(dev_counts)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dev_labels[name], output)\n",
    "    print('Dev score for class {} is {}'.format(name, metrics.auc(fpr,tpr)))\n",
    "    pred_dt[name] = classifier.predict_proba(dev_counts)[:, 1]\n",
    "    \n",
    "    \n",
    "print(\"Mean(dev) ROC_AUC: {}\").format(np.mean(scores_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on dev set is worse than training set, thus evidence of overfitting and a need for performance improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is multi-label since each observation can be classified as multiple fields.  This is an important distinction from multi-class where each prediction can only be one label.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "5         0\n",
       "6         1\n",
       "9         0\n",
       "10        0\n",
       "11        0\n",
       "13        0\n",
       "14        0\n",
       "17        0\n",
       "18        0\n",
       "21        0\n",
       "22        0\n",
       "24        0\n",
       "25        0\n",
       "26        0\n",
       "27        0\n",
       "29        0\n",
       "30        0\n",
       "31        0\n",
       "32        0\n",
       "35        0\n",
       "36        0\n",
       "38        0\n",
       "39        0\n",
       "41        0\n",
       "43        1\n",
       "44        1\n",
       "46        0\n",
       "         ..\n",
       "159535    0\n",
       "159536    0\n",
       "159537    0\n",
       "159538    0\n",
       "159539    0\n",
       "159540    0\n",
       "159541    1\n",
       "159542    0\n",
       "159544    0\n",
       "159545    0\n",
       "159546    1\n",
       "159547    0\n",
       "159549    0\n",
       "159550    0\n",
       "159551    0\n",
       "159554    1\n",
       "159555    0\n",
       "159557    0\n",
       "159558    0\n",
       "159559    0\n",
       "159560    0\n",
       "159561    0\n",
       "159562    0\n",
       "159563    0\n",
       "159564    0\n",
       "159565    0\n",
       "159566    0\n",
       "159567    0\n",
       "159569    0\n",
       "159570    0\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df\n",
    "train_labels[\"toxic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.889472      0.889208  0.889490  0.889300  0.889859   \n",
      "1  0000247867823ef7  0.250917      0.250663  0.250762  0.250821  0.250742   \n",
      "2  00013b17ad220c46  0.432613      0.432735  0.432661  0.432668  0.432595   \n",
      "3  00017563c3f7919a  0.068362      0.068237  0.068175  0.068064  0.068163   \n",
      "4  00017695ad8997eb  0.424004      0.424707  0.424460  0.424676  0.424557   \n",
      "5  0001ea8717f6de06  0.340763      0.340854  0.340989  0.340558  0.340962   \n",
      "6  00024115d4cbde0f  0.124536      0.124778  0.124653  0.124630  0.124505   \n",
      "7  000247e83dcc1211  0.452367      0.452056  0.452069  0.452164  0.452073   \n",
      "8  00025358d4737918  0.006227      0.006221  0.006210  0.006212  0.006193   \n",
      "9  00026d1092fe71cc  0.044093      0.044136  0.044161  0.044094  0.044203   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.888301  \n",
      "1       0.250729  \n",
      "2       0.432661  \n",
      "3       0.068241  \n",
      "4       0.424652  \n",
      "5       0.340869  \n",
      "6       0.124839  \n",
      "7       0.452120  \n",
      "8       0.006195  \n",
      "9       0.044119  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "# Basic Logistic Regression Model/MultiLabel Edition\n",
    "\n",
    "prediction_submission = pd.DataFrame()\n",
    "prediction_submission[\"id\"] = test_df[\"id\"]\n",
    "\n",
    "# new vector object for all train data for submission\n",
    "finalTrainVector = CountVectorizer()\n",
    "finalTrainCount = finalTrainVector.fit_transform(train_df[\"comment_text\"])\n",
    "\n",
    "# TODO: Using pipelines can clean up repeitive processes\n",
    "# test set up\n",
    "#testVector = CountVectorizer()\n",
    "testCount = finalTrainVector.transform(test_df[\"comment_text\"])\n",
    "\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver='sag') #sag is one kind of solver optimize for multi-label\n",
    "    clf = classifier.fit(finalTrainCount, train_df[\"toxic\"])\n",
    "    prediction_submission[name] = clf.predict_proba(testCount)[:, 1]\n",
    "    #print(prediction_submission)\n",
    "\n",
    "    \n",
    "print(prediction_submission.head(10)) # print frame output \n",
    "#prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frame contains the output for each class and is saved in a pandas data frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
