{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Submission: Toxic Language Classification \n",
    "**w207 Spring 2018 - Final Project Baseline**\n",
    "\n",
    "**Team: Paul, Walt, Yisang, Joe**\n",
    "\n",
    "\n",
    "\n",
    "### Project Description \n",
    "\n",
    "Our challenge is to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.  The toxic language data set is sourced from Wikipedia and available as a public kaggle data set. \n",
    "\n",
    "Our goal is to use various machine learning techniques used in class to develop high quality ML models and pipelines.  \n",
    "\n",
    "1. Exercise and build upon concepts covered in class and test out at least 3 kinds of supervised models:\n",
    "    a. Regression (LASSO, Logistic)\n",
    "    b. Trees (RF, XGBoost)\n",
    "    c. DeepLearning (Tensorflow)\n",
    "2. Using stacking/ensembling methods for improving prediction metrics (K-Means, anomaly detection)\n",
    "3. Using unsupervised methods for feature engineering/selection\n",
    "\n",
    "For the baseline proposal, this file contains a first pass run through from data preprocessing to model evaluation using a regression model pipeline. \n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burgew/Library/Python/2.7/lib/python/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/burgew/Library/Python/2.7/lib/python/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "\n",
    "\n",
    "#NLTK imports\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import punkt as punkt\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# These imports enable the use of NLTKPreprocessor in an sklearn Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "#General imports\n",
    "import pprint\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 159571\n",
      "training data shape: (111691,)\n",
      "training label shape: (111691, 6)\n",
      "dev label shape: (47880, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "np.random.seed(455)\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "print 'total training observations:', train_df.shape[0]\n",
    "print 'training data shape:', train_data.shape\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of         toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0           0             0        0       0       0              0\n",
       "1           0             0        0       0       0              0\n",
       "2           0             0        0       0       0              0\n",
       "3           0             0        0       0       0              0\n",
       "4           0             0        0       0       0              0\n",
       "8           0             0        0       0       0              0\n",
       "9           0             0        0       0       0              0\n",
       "10          0             0        0       0       0              0\n",
       "15          0             0        0       0       0              0\n",
       "18          0             0        0       0       0              0\n",
       "19          0             0        0       0       0              0\n",
       "21          0             0        0       0       0              0\n",
       "23          0             0        0       0       0              0\n",
       "25          0             0        0       0       0              0\n",
       "26          0             0        0       0       0              0\n",
       "27          0             0        0       0       0              0\n",
       "28          0             0        0       0       0              0\n",
       "29          0             0        0       0       0              0\n",
       "30          0             0        0       0       0              0\n",
       "31          0             0        0       0       0              0\n",
       "32          0             0        0       0       0              0\n",
       "33          0             0        0       0       0              0\n",
       "35          0             0        0       0       0              0\n",
       "36          0             0        0       0       0              0\n",
       "37          0             0        0       0       0              0\n",
       "38          0             0        0       0       0              0\n",
       "39          0             0        0       0       0              0\n",
       "42          1             0        1       0       1              1\n",
       "43          1             0        1       0       1              0\n",
       "44          1             0        0       0       0              0\n",
       "...       ...           ...      ...     ...     ...            ...\n",
       "159534      0             0        0       0       0              0\n",
       "159535      0             0        0       0       0              0\n",
       "159536      0             0        0       0       0              0\n",
       "159538      0             0        0       0       0              0\n",
       "159539      0             0        0       0       0              0\n",
       "159540      0             0        0       0       0              0\n",
       "159541      1             0        1       0       1              0\n",
       "159543      0             0        0       0       0              0\n",
       "159544      0             0        0       0       0              0\n",
       "159546      1             0        0       0       1              0\n",
       "159547      0             0        0       0       0              0\n",
       "159548      0             0        0       0       0              0\n",
       "159549      0             0        0       0       0              0\n",
       "159550      0             0        0       0       0              0\n",
       "159551      0             0        0       0       0              0\n",
       "159553      0             0        0       0       0              0\n",
       "159554      1             0        1       0       1              0\n",
       "159555      0             0        0       0       0              0\n",
       "159556      0             0        0       0       0              0\n",
       "159557      0             0        0       0       0              0\n",
       "159559      0             0        0       0       0              0\n",
       "159560      0             0        0       0       0              0\n",
       "159561      0             0        0       0       0              0\n",
       "159563      0             0        0       0       0              0\n",
       "159564      0             0        0       0       0              0\n",
       "159565      0             0        0       0       0              0\n",
       "159566      0             0        0       0       0              0\n",
       "159567      0             0        0       0       0              0\n",
       "159568      0             0        0       0       0              0\n",
       "159570      0             0        0       0       0              0\n",
       "\n",
       "[111691 rows x 6 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how imblanced the label set is in order to have a better understanding with the label quality of the given data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"55f84781-0cc2-4835-ae91-fc05b04c2ed8\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"55f84781-0cc2-4835-ae91-fc05b04c2ed8\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"55f84781-0cc2-4835-ae91-fc05b04c2ed8\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '55f84781-0cc2-4835-ae91-fc05b04c2ed8' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"55f84781-0cc2-4835-ae91-fc05b04c2ed8\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"55f84781-0cc2-4835-ae91-fc05b04c2ed8\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"55f84781-0cc2-4835-ae91-fc05b04c2ed8\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '55f84781-0cc2-4835-ae91-fc05b04c2ed8' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"55f84781-0cc2-4835-ae91-fc05b04c2ed8\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"2bd02600-a785-4d71-9bb9-7c4a46f74317\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"53209665-22f2-4a72-ae6f-579239d5650d\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"cdf6c34a-d7b2-4b53-b743-9944596ee1f2\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"de0b18b6-f146-4855-9a6a-efa95b23e6be\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"511a1680-b8d6-493b-b95c-bf3434e66d01\",\"type\":\"SaveTool\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"07669f8a-4034-4ad0-b9b8-853253e8810f\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"f1ed66a5-7691-41b4-a410-2660cb066291\",\"type\":\"BasicTicker\"}},\"id\":\"4ea53ce4-cdbb-45f5-9804-5d70832840cd\",\"type\":\"Grid\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"db72b6d2-2138-489f-9f80-40ecd6e380e1\",\"type\":\"PanTool\"},{\"id\":\"493b1cff-6b75-4429-9dce-37bdc606ede2\",\"type\":\"WheelZoomTool\"},{\"id\":\"8a110817-4fb3-4674-ba62-8f21640dd8fa\",\"type\":\"BoxZoomTool\"},{\"id\":\"511a1680-b8d6-493b-b95c-bf3434e66d01\",\"type\":\"SaveTool\"},{\"id\":\"de0b18b6-f146-4855-9a6a-efa95b23e6be\",\"type\":\"ResetTool\"},{\"id\":\"cdd11a19-5629-4c79-9002-e356948c0f05\",\"type\":\"HelpTool\"}]},\"id\":\"d58e276b-243b-42d2-8f87-2177138b6a8e\",\"type\":\"Toolbar\"},{\"attributes\":{\"data_source\":{\"id\":\"58eeb4a5-eea4-43cf-9460-945dd23c3edb\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"565fbebe-ac8c-42b9-8ec0-9a739a83947e\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"08506fef-084f-46cd-b537-ada2aac1e48e\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"d7280720-edce-48b7-bcc3-65e7a7233410\",\"type\":\"CDSView\"}},\"id\":\"c510ded6-6208-4250-b251-4adfadeabf86\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"formatter\":{\"id\":\"383e9d6d-a2a5-4e98-a5ca-8329fa0df420\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"07669f8a-4034-4ad0-b9b8-853253e8810f\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"f1ed66a5-7691-41b4-a410-2660cb066291\",\"type\":\"BasicTicker\"}},\"id\":\"7ae2babe-a42b-4cac-bb72-15e209383b88\",\"type\":\"LinearAxis\"},{\"attributes\":{\"source\":{\"id\":\"58eeb4a5-eea4-43cf-9460-945dd23c3edb\",\"type\":\"ColumnDataSource\"}},\"id\":\"d7280720-edce-48b7-bcc3-65e7a7233410\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"factors\":[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]},\"id\":\"c57e85f0-b321-4052-a9d8-4a27ac91f250\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"db72b6d2-2138-489f-9f80-40ecd6e380e1\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"775002e0-321c-4f1c-8cae-0a182eaa7158\",\"type\":\"LinearScale\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"29149bd3-046f-44e9-bb36-be2ef58f254f\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"383e9d6d-a2a5-4e98-a5ca-8329fa0df420\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.9},\"x\":{\"field\":\"x\"}},\"id\":\"565fbebe-ac8c-42b9-8ec0-9a739a83947e\",\"type\":\"VBar\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"top\"],\"data\":{\"top\":[10647,1103,5915,342,5512,987],\"x\":[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]}},\"id\":\"58eeb4a5-eea4-43cf-9460-945dd23c3edb\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"formatter\":{\"id\":\"4359830b-c1f9-4333-bbe2-fb4889379b6e\",\"type\":\"CategoricalTickFormatter\"},\"plot\":{\"id\":\"07669f8a-4034-4ad0-b9b8-853253e8810f\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"03bc35f3-5996-4906-aaff-ea51e7065e0f\",\"type\":\"CategoricalTicker\"}},\"id\":\"167e4073-d18e-451a-b48b-3092387dcc83\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"below\":[{\"id\":\"167e4073-d18e-451a-b48b-3092387dcc83\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"7ae2babe-a42b-4cac-bb72-15e209383b88\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"167e4073-d18e-451a-b48b-3092387dcc83\",\"type\":\"CategoricalAxis\"},{\"id\":\"8b38af4a-1780-4ab2-88af-63cb355349bd\",\"type\":\"Grid\"},{\"id\":\"7ae2babe-a42b-4cac-bb72-15e209383b88\",\"type\":\"LinearAxis\"},{\"id\":\"4ea53ce4-cdbb-45f5-9804-5d70832840cd\",\"type\":\"Grid\"},{\"id\":\"725c5cd8-c992-466b-b4d9-a9e0d89c0c03\",\"type\":\"BoxAnnotation\"},{\"id\":\"c510ded6-6208-4250-b251-4adfadeabf86\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"29149bd3-046f-44e9-bb36-be2ef58f254f\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"d58e276b-243b-42d2-8f87-2177138b6a8e\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"c57e85f0-b321-4052-a9d8-4a27ac91f250\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"cdf6c34a-d7b2-4b53-b743-9944596ee1f2\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"137deebd-fa60-4ca8-a865-8660599b4d7c\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"775002e0-321c-4f1c-8cae-0a182eaa7158\",\"type\":\"LinearScale\"}},\"id\":\"07669f8a-4034-4ad0-b9b8-853253e8810f\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"f1ed66a5-7691-41b4-a410-2660cb066291\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"03bc35f3-5996-4906-aaff-ea51e7065e0f\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"callback\":null},\"id\":\"137deebd-fa60-4ca8-a865-8660599b4d7c\",\"type\":\"DataRange1d\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.9},\"x\":{\"field\":\"x\"}},\"id\":\"08506fef-084f-46cd-b537-ada2aac1e48e\",\"type\":\"VBar\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"725c5cd8-c992-466b-b4d9-a9e0d89c0c03\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"overlay\":{\"id\":\"725c5cd8-c992-466b-b4d9-a9e0d89c0c03\",\"type\":\"BoxAnnotation\"}},\"id\":\"8a110817-4fb3-4674-ba62-8f21640dd8fa\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"4359830b-c1f9-4333-bbe2-fb4889379b6e\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"plot\":{\"id\":\"07669f8a-4034-4ad0-b9b8-853253e8810f\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"03bc35f3-5996-4906-aaff-ea51e7065e0f\",\"type\":\"CategoricalTicker\"}},\"id\":\"8b38af4a-1780-4ab2-88af-63cb355349bd\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"cdd11a19-5629-4c79-9002-e356948c0f05\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"493b1cff-6b75-4429-9dce-37bdc606ede2\",\"type\":\"WheelZoomTool\"}],\"root_ids\":[\"07669f8a-4034-4ad0-b9b8-853253e8810f\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.14\"}};\n",
       "  var render_items = [{\"docid\":\"53209665-22f2-4a72-ae6f-579239d5650d\",\"elementid\":\"2bd02600-a785-4d71-9bb9-7c4a46f74317\",\"modelid\":\"07669f8a-4034-4ad0-b9b8-853253e8810f\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "07669f8a-4034-4ad0-b9b8-853253e8810f"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.io import push_notebook\n",
    "from bokeh.plotting import figure, show, output_file, output_notebook\n",
    "\n",
    "target_counts = train_labels.apply(np.sum,0)\n",
    "target_counts\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "p = figure(x_range=target_names)\n",
    "p.vbar(x=target_names, top = target_counts, width=0.9)\n",
    "\n",
    "show(p)\n",
    "\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is fairly imbalanced when counting label occurrences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to consider\n",
    "- Sampling methods\n",
    "- Custom Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering/Selection (WIP)\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/burgew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Starting preprocessing of training data...\n",
      "Completed tokenization/preprocessing of training data in 588.66 seconds\n",
      "Starting vectorization of training data...\n",
      "Completed vectorization of training data in 3.33 seconds\n",
      "Starting preprocessing of dev data...\n",
      "Completed tokenization/preprocessing of dev data in 242.46 seconds\n",
      "\n",
      "Starting vectorization of dev data...\n",
      "Completed vectorization of dev data in 1.12 seconds\n",
      "\n",
      "Vocabulary (tfidf) size is: 15000\n",
      "Sample vocabulary from TfidfVectorizer:\n",
      "         count\n",
      "\"\"—          0\n",
      "(“           1\n",
      "(→           2\n",
      "(☎)          3\n",
      "(𒁳)         4\n",
      "(｡◕‿◕｡)      5\n",
      ")‎           6\n",
      ",”           7\n",
      "-•           8\n",
      ".·           9\n",
      "None\n",
      "...\n",
      "     count\n",
      "➨    14990\n",
      "・    14991\n",
      "水    14992\n",
      "竜龙   14993\n",
      "聖やや  14994\n",
      "見学   14995\n",
      "迷惑   14996\n",
      "連絡   14997\n",
      "雲    14998\n",
      "，    14999\n",
      "None\n",
      "Number of nonzero entries in matrix: 2855615\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     counts\n",
       "42        4\n",
       "43        3\n",
       "44        1\n",
       "51        2\n",
       "56        3\n",
       "59        1\n",
       "65        3\n",
       "79        2\n",
       "86        2\n",
       "151       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(unicode(document,'utf-8')):\n",
    "\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/burgew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Starting preprocessing of training data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "np.random.seed(455)\n",
    "\n",
    "# This preprocessor will be used to process data prior to vectorization\n",
    "nltkPreprocessor = NLTKPreprocessor()\n",
    "    \n",
    "# Note that this vectorizer is created with a passthru tokenizer(identity), no preprocessor and no lowercasing\n",
    "# This is to account for the NLTKPreprocessor already taking care of these.\n",
    "tfidfVector = TfidfVectorizer(ngram_range=(1,1), min_df=5, max_features=15000,\n",
    "                              tokenizer=identity, preprocessor=None, lowercase=False)\n",
    "\n",
    "print \"Starting preprocessing of training data...\"\n",
    "start_train_preproc = time.time()\n",
    "nltkPreprocessor.fit(train_data)\n",
    "train_preproc_data = nltkPreprocessor.transform(train_data)\n",
    "finish_train_preproc = time.time()\n",
    "print \"Completed tokenization/preprocessing of training data in {:.2f} seconds\".format(finish_train_preproc-start_train_preproc)\n",
    "\n",
    "print \"Starting vectorization of training data...\"\n",
    "start_train_vectors = time.time()\n",
    "train_tfidf_counts = tfidfVector.fit_transform(train_preproc_data)\n",
    "finish_train_vectors = time.time()\n",
    "print \"Completed vectorization of training data in {:.2f} seconds\".format(finish_train_vectors-start_train_vectors)\n",
    "\n",
    "print \"Starting preprocessing of dev data...\"\n",
    "start_dev_preproc = time.time()\n",
    "nltkPreprocessor.fit(dev_data)\n",
    "dev_preproc_data = nltkPreprocessor.transform(dev_data)\n",
    "finish_dev_preproc = time.time()\n",
    "print \"Completed tokenization/preprocessing of dev data in {:.2f} seconds\".format(finish_dev_preproc-start_dev_preproc)\n",
    "\n",
    "print \"\\nStarting vectorization of dev data...\"\n",
    "start_dev_vectors = time.time()\n",
    "dev_tfidf_counts = tfidfVector.transform(dev_preproc_data)\n",
    "finish_dev_vectors = time.time()\n",
    "print \"Completed vectorization of dev data in {:.2f} seconds\".format(finish_dev_vectors-start_dev_vectors)\n",
    "\n",
    "print(\"\\nVocabulary (tfidf) size is: {}\").format(len(tfidfVector.vocabulary_))\n",
    "vocab_entries = {k: tfidfVector.vocabulary_[k] for k in tfidfVector.vocabulary_.keys()}\n",
    "vocab_entries = pd.Series(vocab_entries).to_frame()\n",
    "vocab_entries.columns = ['count']\n",
    "vocab_entries = vocab_entries.sort_values(by='count')\n",
    "\n",
    "print(\"Sample vocabulary from TfidfVectorizer:\")\n",
    "print(pp.pprint(vocab_entries.head(10)))\n",
    "print(\"...\")\n",
    "print(pp.pprint(vocab_entries.tail(10)))\n",
    "print(\"Number of nonzero entries in matrix: {}\").format(train_tfidf_counts.nnz)\n",
    "\n",
    "# sample column wise sum, we can see that an observation can have multiple classes.\n",
    "count_df = pd.DataFrame(train_labels.apply(np.sum,1), columns = [\"counts\"])\n",
    "count_df = count_df[((count_df[\"counts\"] >= 1))]\n",
    "count_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSA on train counts with 1500 components...\n",
      "Train counts transform took 320.12 seconds.\n",
      "Starting LSA on dev counts with 1500 components...\n",
      "Dev counts transform took 301.86 seconds.\n"
     ]
    }
   ],
   "source": [
    "target_components = len(tfidfVector.vocabulary_)/10\n",
    "svd = TruncatedSVD(n_components=target_components, algorithm='arpack')\n",
    "print \"Starting LSA on train counts with {} components...\".format(target_components)\n",
    "train_start=time.time()\n",
    "lsa_train_counts = svd.fit_transform(train_tfidf_counts)\n",
    "train_stop=time.time()\n",
    "print \"Train counts transform took {:.2f} seconds.\".format(train_stop-train_start)\n",
    "print \"Starting LSA on dev counts with {} components...\".format(target_components)\n",
    "dev_start=time.time()\n",
    "lsa_dev_counts = svd.fit_transform(dev_tfidf_counts)\n",
    "dev_stop=time.time()\n",
    "print \"Dev counts transform took {:.2f} seconds.\".format(dev_stop-dev_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier (Neural Net) - shallow - both Train and Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "#from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split \n",
    "\n",
    "\n",
    "\n",
    "print(\"Modelling with MLPClassifier (shallow/wide Net)\")\n",
    "\n",
    "# Testing testing/cross-val with shallow/wide Neural Net for both train and dev dataprediction_output = []\n",
    "scores_output = []\n",
    "full_CV_start = time.time()\n",
    "for name in target_names:\n",
    "    label_CV_start = time.time()\n",
    "\n",
    "    # This Multi-Layer Perceptron classifier will be setup with hidden layers of 6 and 6 each, with tanh activation\n",
    "    # Running a 3-way cross-validation for a single label takes between 10 and 20 minutes, dependenging on the machine.\n",
    "    # The mean AUC for train and dev was 93%.\n",
    "    \n",
    "    # Changing the Net to (12,6) hidden layers gave an AUC of 94%. This was likely aided by the LSA that wasn't in place\n",
    "    # for the earlier 93% test.\n",
    "    \n",
    "    # Changing to try (18,6) hidden layers resulted in 93% again, for both Train and Dev\n",
    "    \n",
    "    # Changed back to (12,6) for both\n",
    "    \n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(12,6), activation='tanh', learning_rate='adaptive')\n",
    "    classifier.fit(lsa_train_counts, train_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, lsa_train_counts, train_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    label_CV_finish = time.time()\n",
    "    print('Train data CV score for class {} is {:.2f}, after {:.2f} minutes.'.format(name, cv_score, \n",
    "                                                                                (label_CV_finish-label_CV_start)/60))\n",
    "full_CV_finish = time.time()\n",
    "print(\"Full shallow/wide Train Neural Net cross-val across all labels with train data took {:.2f} minutes.\".format((full_CV_finish-full_CV_start)/60))\n",
    "\n",
    "print(\"Mean shallow/wide Train ROC_AUC for MLPClassifier: {:.2f}\".format(np.mean(scores_output)))\n",
    "\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "full_CV_start = time.time()\n",
    "for name in target_names:\n",
    "    label_CV_start = time.time()\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(12,6), activation='tanh', learning_rate='adaptive')\n",
    "    classifier.fit(lsa_dev_counts, dev_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, lsa_dev_counts, dev_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    label_CV_finish = time.time()\n",
    "    print('DEV data CV score for class {} is {:.2f}, after {:.2f} minutes.'.format(name, cv_score, \n",
    "                                                                                (label_CV_finish-label_CV_start)/60))\n",
    "full_CV_finish = time.time()\n",
    "print(\"Full shallow/wide Neural Net cross-val across all labels with dev data took {:.2f} minutes.\".format((full_CV_finish-full_CV_start)/60))\n",
    "print(\"Mean shallow/wide DEV ROC_AUC for MLPClassifier: {:.2f}\".format(np.mean(scores_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier (Neural Net) - shallow - just Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "#from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split \n",
    "\n",
    "\n",
    "\n",
    "print(\"Training with MLPClassifier (shallow/wide Net)\")\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(12,6), activation='tanh', learning_rate='adaptive')\n",
    "\n",
    "# Training with shallow/wide Neural Net \n",
    "for name in target_names:\n",
    "    label_CV_start = time.time()\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(12,6), activation='tanh', learning_rate='adaptive')\n",
    "    classifier.fit(lsa_train_counts, train_labels[name])\n",
    "    label_CV_finish = time.time()\n",
    "    print('Train data for class {} completed, after {:.2f} minutes.'.format(name, cv_score,\n",
    "                                                                            (label_CV_finish-label_CV_start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier (Neural Net) - deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "#from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split \n",
    "\n",
    "\n",
    "\n",
    "print(\"Modelling with MLPClassifier (deep/thinner)\")\n",
    "\n",
    "# Testing testing/cross-val with deep/thinner Neural Net for both train and dev data\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "full_CV_start = time.time()\n",
    "for name in target_names:\n",
    "    label_CV_start = time.time()\n",
    "    \n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(10,8,6), activation='tanh', learning_rate='adaptive')\n",
    "    classifier.fit(lsa_train_counts, train_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, lsa_train_counts, train_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    label_CV_finish = time.time()\n",
    "    print('Train data CV score for class {} is {:.2f}, after {:.2f} minutes.'.format(name, cv_score, \n",
    "                                                                                (label_CV_finish-label_CV_start)/60))\n",
    "full_CV_finish = time.time()\n",
    "print(\"Full deep/thin Neural Net Train cross-val across all labels took {:.2f} minutes.\".format((full_CV_finish-full_CV_start)/60))\n",
    "\n",
    "print(\"Mean deep/thin Train ROC_AUC for MLPClassifier: {:.2f}\".format(np.mean(scores_output)))\n",
    "\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "full_CV_start = time.time()\n",
    "for name in target_names:\n",
    "    label_CV_start = time.time()\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(10,8,6), activation='tanh', learning_rate='adaptive')\n",
    "    classifier.fit(lsa_dev_counts, dev_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, lsa_dev_counts, dev_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    label_CV_finish = time.time()\n",
    "    print('DEV data CV score for class {} is {:.2f}, after {:.2f} minutes.'.format(name, cv_score, \n",
    "                                                                                (label_CV_finish-label_CV_start)/60))\n",
    "full_CV_finish = time.time()\n",
    "print(\"Full deep/thin Neural Net DEV cross-val across all labels took {:.2f} minutes.\".format((full_CV_finish-full_CV_start)/60))\n",
    "print(\"Mean neep/thin DEV ROC_AUC for MLPClassifier: {:.2f}\".format(np.mean(scores_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### First Pass Logistic Regression with sag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "# Basic Logistic Regression Model/MultiLabel Edition\n",
    "\n",
    "solver = 'sag'\n",
    "\n",
    "print(\"Modelling with {} solver\".format(solver))\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver=solver)\n",
    "    classifier.fit(train_counts, train_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, train_counts, train_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('Training data CV score for class {} is {}'.format(name, cv_score))\n",
    "    \n",
    "print(\"Mean Training ROC_AUC for {} solver: {}\").format(solver, np.mean(scores_output))\n",
    "\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver=solver) \n",
    "    classifier.fit(dev_counts, dev_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, dev_counts, dev_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('Dev data CV score for class {} is {}'.format(name, cv_score))\n",
    "        \n",
    "print(\"Mean Dev ROC_AUC for {} solver: {}\").format(solver, np.mean(scores_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### First Pass Logistic Regression with saga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "# Basic Logistic Regression Model/MultiLabel Edition\n",
    "\n",
    "solver = 'saga'\n",
    "\n",
    "print(\"Modelling with {} solver\".format(solver))\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver=solver)\n",
    "    classifier.fit(train_counts, train_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, train_counts, train_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('Training data CV score for class {} is {}'.format(name, cv_score))\n",
    "    \n",
    "print(\"Mean Training ROC_AUC for {} solver: {}\").format(solver, np.mean(scores_output))\n",
    "\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver=solver) \n",
    "    classifier.fit(dev_counts, dev_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, dev_counts, dev_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('Dev data CV score for class {} is {}'.format(name, cv_score))\n",
    "        \n",
    "print(\"Mean Dev ROC_AUC for {} solver: {}\").format(solver, np.mean(scores_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Here's the same using tfidf and saga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "# Basic Logistic Regression Model/MultiLabel Edition\n",
    "\n",
    "solver = 'saga'\n",
    "\n",
    "print(\"Modelling with {} solver\".format(solver))\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver=solver)\n",
    "    classifier.fit(train_tfidf_counts, train_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, train_tfidf_counts, train_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('Training data CV score for class {} is {}'.format(name, cv_score))\n",
    "\n",
    "    \n",
    "print(\"Mean Training ROC_AUC for {} solver: {}\").format(solver, np.mean(scores_output))\n",
    "\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver=solver) \n",
    "    classifier.fit(dev_tfidf_counts, dev_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, dev_tfidf_counts, dev_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('Dev data CV score for class {} is {}'.format(name, cv_score))\n",
    "\n",
    "        \n",
    "print(\"Mean Dev ROC_AUC for {} solver: {}\").format(solver, np.mean(scores_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Original counts with saga and L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "# Basic Logistic Regression Model/MultiLabel Edition\n",
    "\n",
    "solver = 'saga'\n",
    "\n",
    "print(\"Modelling with {} solver\".format(solver))\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver=solver,penalty='l1')\n",
    "    classifier.fit(train_counts, train_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, train_counts, train_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('Training data CV score for class {} is {}'.format(name, cv_score))\n",
    "\n",
    "    \n",
    "print(\"Mean Training ROC_AUC for {} solver: {}\").format(solver, np.mean(scores_output))\n",
    "\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver=solver,penalty='l1') \n",
    "    classifier.fit(dev_counts, dev_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, dev_counts, dev_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('Dev data CV score for class {} is {}'.format(name, cv_score))\n",
    "\n",
    "        \n",
    "print(\"Mean Dev ROC_AUC for {} solver: {}\").format(solver, np.mean(scores_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf with saga and L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "# Basic Logistic Regression Model/MultiLabel Edition\n",
    "\n",
    "solver = 'saga'\n",
    "\n",
    "print(\"Modelling with {} solver\".format(solver))\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver=solver,penalty='l1')\n",
    "    classifier.fit(train_tfidf_counts, train_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, train_tfidf_counts, train_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('Training data CV score for class {} is {}'.format(name, cv_score))\n",
    "\n",
    "    \n",
    "print(\"Mean Training ROC_AUC for {} solver: {}\").format(solver, np.mean(scores_output))\n",
    "\n",
    "prediction_output = []\n",
    "scores_output = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver=solver,penalty='l1') \n",
    "    classifier.fit(dev_tfidf_counts, dev_labels[name])\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, dev_tfidf_counts, dev_labels[name], cv=3, scoring='roc_auc'))\n",
    "    scores_output.append(cv_score)\n",
    "    print('Dev data CV score for class {} is {}'.format(name, cv_score))\n",
    "\n",
    "        \n",
    "print(\"Mean Dev ROC_AUC for {} solver: {}\").format(solver, np.mean(scores_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "dev_Vector = CountVectorizer(ngram_range=(1,1))\n",
    "dev_counts = countVector.fit_transform(dev_data)\n",
    "\n",
    "pred_dt = pd.DataFrame()\n",
    "scores_dev = []\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver='sag') \n",
    "    classifier.fit(dev_counts, dev_labels[name])\n",
    "    scores_dev.append(cv_score)\n",
    "    output = classifier.predict(dev_counts)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dev_labels[name], output)\n",
    "    print('Dev score for class {} is {}'.format(name, metrics.auc(fpr,tpr)))\n",
    "    pred_dt[name] = classifier.predict_proba(dev_counts)[:, 1]\n",
    "    \n",
    "    \n",
    "print(\"Mean(dev) ROC_AUC: {}\").format(np.mean(scores_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score on dev set is worse than training set, thus evidence of overfitting and a need for performance improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is multi-label since each observation can be classified as multiple fields.  This is an important distinction from multi-class where each prediction can only be one label.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df\n",
    "train_labels[\"toxic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Text Preprocessing - training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting final preprocessing of training data...\n",
      "Completed tokenization/preprocessing of training data in 857.89 seconds\n",
      "Starting final preprocessing of test data...\n",
      "Completed tokenization/preprocessing of test data in 771.28 seconds\n",
      "Starting vectorization of training data...\n",
      "Completed vectorization of training data in 5.24 seconds\n",
      "Starting vectorization of test data...\n",
      "Completed vectorization of test data in 3.75 seconds\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "np.random.seed(455)\n",
    "\n",
    "# This preprocessor will be used to process data prior to vectorization\n",
    "nltkPreprocessor = NLTKPreprocessor()\n",
    "    \n",
    "# Note that this vectorizer is created with a passthru tokenizer(identity), no preprocessor and no lowercasing\n",
    "# This is to account for the NLTKPreprocessor already taking care of these.\n",
    "tfidfVector = TfidfVectorizer(ngram_range=(1,1), min_df=5, max_features=15000,\n",
    "                              tokenizer=identity, preprocessor=None, lowercase=False)\n",
    "\n",
    "print \"Starting final preprocessing of training data...\"\n",
    "start_train_preproc = time.time()\n",
    "trainPreprocData = nltkPreprocessor.fit_transform(train_df[\"comment_text\"])\n",
    "finish_train_preproc = time.time()\n",
    "print \"Completed tokenization/preprocessing of training data in {:.2f} seconds\".format((finish_train_preproc-start_train_preproc))\n",
    "\n",
    "print \"Starting final preprocessing of test data...\"\n",
    "start_test_preproc = time.time()\n",
    "testPreprocData = nltkPreprocessor.transform(test_df[\"comment_text\"])\n",
    "finish_test_preproc = time.time()\n",
    "print \"Completed tokenization/preprocessing of test data in {:.2f} seconds\".format((finish_test_preproc-start_test_preproc))\n",
    "\n",
    "print \"Starting vectorization of training data...\"\n",
    "start_train_vectors = time.time()\n",
    "finalTrainCounts = tfidfVector.fit_transform(trainPreprocData)\n",
    "finish_train_vectors = time.time()\n",
    "print \"Completed vectorization of training data in {:.2f} seconds\".format((finish_train_vectors-start_train_vectors))\n",
    "\n",
    "print \"Starting vectorization of test data...\"\n",
    "start_test_vectors = time.time()\n",
    "finalTestCounts = tfidfVector.transform(testPreprocData)\n",
    "finish_test_vectors = time.time()\n",
    "print \"Completed vectorization of test data in {:.2f} seconds\".format((finish_test_vectors-start_test_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final LSA Feature Selection - training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSA on train counts with 1500 components...\n",
      "Train counts transform took 351.07 seconds.\n",
      "Starting LSA on test counts with 1500 components...\n",
      "Test counts transform took 345.80 seconds.\n"
     ]
    }
   ],
   "source": [
    "target_components = len(tfidfVector.vocabulary_)/10\n",
    "svd = TruncatedSVD(n_components=target_components, algorithm='arpack')\n",
    "print \"Starting LSA on train counts with {} components...\".format(target_components)\n",
    "train_start=time.time()\n",
    "lsaTrainCounts = svd.fit_transform(finalTraincounts)\n",
    "train_stop=time.time()\n",
    "print \"Train counts transform took {:.2f} seconds.\".format(train_stop-train_start)\n",
    "\n",
    "target_components = len(tfidfVector.vocabulary_)/10\n",
    "svd = TruncatedSVD(n_components=target_components, algorithm='arpack')\n",
    "print \"Starting LSA on test counts with {} components...\".format(target_components)\n",
    "train_start=time.time()\n",
    "lsaTestCounts = svd.fit_transform(finalTestCounts)\n",
    "train_stop=time.time()\n",
    "print \"Test counts transform took {:.2f} seconds.\".format(train_stop-train_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final MLPClassifier Training and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with MLPClassifier (shallow/wide Net)\n",
      "Training for class toxic completed, after 8.02 minutes.\n",
      "Prediction for class toxic completed, after 0.13 minutes.\n",
      "Training for class severe_toxic completed, after 3.23 minutes.\n",
      "Prediction for class severe_toxic completed, after 0.01 minutes.\n",
      "Training for class obscene completed, after 4.65 minutes.\n",
      "Prediction for class obscene completed, after 0.01 minutes.\n",
      "Training for class threat completed, after 1.46 minutes.\n",
      "Prediction for class threat completed, after 0.01 minutes.\n",
      "Training for class insult completed, after 5.89 minutes.\n",
      "Prediction for class insult completed, after 0.08 minutes.\n",
      "Training for class identity_hate completed, after 3.03 minutes.\n",
      "Prediction for class identity_hate completed, after 0.01 minutes.\n",
      "                 id         toxic  severe_toxic       obscene    threat  \\\n",
      "0  00001cee341fdb12  9.413830e-04  3.019389e-06  8.532108e-01  0.000011   \n",
      "1  0000247867823ef7  6.041182e-06  1.038509e-06  2.992874e-07  0.000010   \n",
      "2  00013b17ad220c46  4.357374e-05  7.132383e-06  2.466334e-02  0.000015   \n",
      "3  00017563c3f7919a  6.782458e-08  8.802607e-06  1.052156e-07  0.000005   \n",
      "4  00017695ad8997eb  1.593374e-10  2.194619e-06  4.212840e-10  0.000027   \n",
      "5  0001ea8717f6de06  4.094924e-06  3.493270e-08  2.018844e-07  0.000012   \n",
      "6  00024115d4cbde0f  1.302152e-10  1.785313e-06  1.571592e-07  0.000005   \n",
      "7  000247e83dcc1211  5.608019e-01  1.183878e-03  1.119971e-05  0.000033   \n",
      "8  00025358d4737918  1.770417e-04  1.229857e-04  8.791228e-01  0.000022   \n",
      "9  00026d1092fe71cc  9.960947e-03  6.831042e-08  1.852537e-11  0.000012   \n",
      "\n",
      "         insult  identity_hate  \n",
      "0  8.018887e-01   9.251315e-08  \n",
      "1  6.112533e-04   6.839443e-06  \n",
      "2  8.597253e-07   1.655742e-05  \n",
      "3  1.098730e-04   1.015704e-07  \n",
      "4  8.720402e-09   7.495807e-08  \n",
      "5  4.073561e-08   8.389998e-05  \n",
      "6  1.083564e-08   7.126852e-08  \n",
      "7  6.550980e-05   2.550736e-06  \n",
      "8  7.175040e-05   5.294053e-06  \n",
      "9  9.835920e-09   7.126948e-08  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "#from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split \n",
    "\n",
    "prediction_submission = pd.DataFrame()\n",
    "prediction_submission[\"id\"] = test_df[\"id\"]\n",
    "\n",
    "print(\"Training with MLPClassifier (shallow/wide Net)\")\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(12,6), activation='tanh', learning_rate='adaptive')\n",
    "\n",
    "# Training with shallow/wide Neural Net \n",
    "for name in target_names:\n",
    "    \n",
    "    label_train_start = time.time()\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(12,6), activation='tanh', learning_rate='adaptive')\n",
    "    classifier.fit(lsaTrainCounts, train_df[name])\n",
    "    label_train_finish = time.time()\n",
    "    print('Training for class {} completed, after {:.2f} minutes.'.format(name, \n",
    "                                                                          (label_train_finish-label_train_start)/60))\n",
    "    label_predict_start = time.time()\n",
    "    prediction_submission[name] = classifier.predict_proba(lsaTestCounts)[:, 1]\n",
    "    label_predict_finish = time.time()\n",
    "    print('Prediction for class {} completed, after {:.2f} minutes.'.format(name,\n",
    "                                                                    (label_predict_finish-label_predict_start)/60))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "print(prediction_submission.head(10)) # print frame output \n",
    "prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission - based on test preprocessing, LSA feature selection and MLPClassifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_submission = pd.DataFrame()\n",
    "prediction_submission[\"id\"] = test_df[\"id\"]\n",
    "\n",
    "# new vector object for all train data for submission\n",
    "finalTrainVector = CountVectorizer()\n",
    "finalTrainCount = finalTrainVector.fit_transform(train_df[\"comment_text\"])\n",
    "\n",
    "# TODO: Using pipelines can clean up repetitive processes\n",
    "# test set up\n",
    "#testVector = CountVectorizer()\n",
    "testCount = finalTrainVector.transform(test_df[\"comment_text\"])\n",
    "\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver='sag') #sag is one kind of solver optimize for multi-label\n",
    "    clf = classifier.fit(finalTrainCount, train_df[name])\n",
    "    prediction_submission[name] = clf.predict_proba(testCount)[:, 1]\n",
    "    #print(prediction_submission)\n",
    "\n",
    "    \n",
    "print(prediction_submission.head(10)) # print frame output \n",
    "prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "# SK-learn libraries for cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "# Basic Logistic Regression Model/MultiLabel Edition\n",
    "\n",
    "prediction_submission = pd.DataFrame()\n",
    "prediction_submission[\"id\"] = test_df[\"id\"]\n",
    "\n",
    "# new vector object for all train data for submission\n",
    "finalTrainVector = CountVectorizer()\n",
    "finalTrainCount = finalTrainVector.fit_transform(train_df[\"comment_text\"])\n",
    "\n",
    "# TODO: Using pipelines can clean up repetitive processes\n",
    "# test set up\n",
    "#testVector = CountVectorizer()\n",
    "testCount = finalTrainVector.transform(test_df[\"comment_text\"])\n",
    "\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver='sag') #sag is one kind of solver optimize for multi-label\n",
    "    clf = classifier.fit(finalTrainCount, train_df[name])\n",
    "    prediction_submission[name] = clf.predict_proba(testCount)[:, 1]\n",
    "    #print(prediction_submission)\n",
    "\n",
    "    \n",
    "print(prediction_submission.head(10)) # print frame output \n",
    "prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frame contains the output for each class and is saved in a pandas data frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
