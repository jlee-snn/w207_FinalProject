{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Submission: Toxic Language Classification \n",
    "**w207 Spring 2018 - Final Project Baseline**\n",
    "\n",
    "**Team: Paul, Walt, Yisang, Joe**\n",
    "\n",
    "\n",
    "\n",
    "### Project Description \n",
    "\n",
    "Our challenge is to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.  The toxic language data set is sourced from Wikipedia and available as a public kaggle data set. \n",
    "\n",
    "Our goal is to use various machine learning techniques used in class to develop high quality ML models and pipelines.  \n",
    "\n",
    "1. Exercise and build upon concepts covered in class and test out at least 3 kinds of supervised models:\n",
    "    a. Regression (LASSO, Logistic)\n",
    "    b. Trees (RF, XGBoost)\n",
    "    c. DeepLearning (Tensorflow)\n",
    "2. Using stacking/ensembling methods for improving prediction metrics (K-Means, anomaly detection)\n",
    "3. Using unsupervised methods for feature engineering/selection\n",
    "\n",
    "For the baseline proposal, this file contains a first pass run through from data preprocessing to model evaluation using a regression model pipeline. \n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burgew/Library/Python/2.7/lib/python/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/burgew/Library/Python/2.7/lib/python/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#NLTK imports\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import punkt as punkt\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# These imports enable the use of NLTKPreprocessor in an sklearn Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#General imports\n",
    "import pprint\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 159571\n",
      "training data shape: (111691,)\n",
      "training label shape: (111691, 6)\n",
      "dev label shape: (47880, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "np.random.seed(455)\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "print 'total training observations:', train_df.shape[0]\n",
    "print 'training data shape:', train_data.shape\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/burgew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Starting preprocessing of training data...\n",
      "Completed tokenization/preprocessing of training data in 588.66 seconds\n",
      "Starting vectorization of training data...\n",
      "Completed vectorization of training data in 3.33 seconds\n",
      "Starting preprocessing of dev data...\n",
      "Completed tokenization/preprocessing of dev data in 242.46 seconds\n",
      "\n",
      "Starting vectorization of dev data...\n",
      "Completed vectorization of dev data in 1.12 seconds\n",
      "\n",
      "Vocabulary (tfidf) size is: 15000\n",
      "Sample vocabulary from TfidfVectorizer:\n",
      "         count\n",
      "\"\"—          0\n",
      "(“           1\n",
      "(→           2\n",
      "(☎)          3\n",
      "(𒁳)         4\n",
      "(｡◕‿◕｡)      5\n",
      ")‎           6\n",
      ",”           7\n",
      "-•           8\n",
      ".·           9\n",
      "None\n",
      "...\n",
      "     count\n",
      "➨    14990\n",
      "・    14991\n",
      "水    14992\n",
      "竜龙   14993\n",
      "聖やや  14994\n",
      "見学   14995\n",
      "迷惑   14996\n",
      "連絡   14997\n",
      "雲    14998\n",
      "，    14999\n",
      "None\n",
      "Number of nonzero entries in matrix: 2855615\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     counts\n",
       "42        4\n",
       "43        3\n",
       "44        1\n",
       "51        2\n",
       "56        3\n",
       "59        1\n",
       "65        3\n",
       "79        2\n",
       "86        2\n",
       "151       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(unicode(document,'utf-8')):\n",
    "\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Text Preprocessing - training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting final preprocessing of training data...\n",
      "Completed tokenization/preprocessing of training data in 857.89 seconds\n",
      "Starting final preprocessing of test data...\n",
      "Completed tokenization/preprocessing of test data in 771.28 seconds\n",
      "Starting vectorization of training data...\n",
      "Completed vectorization of training data in 5.24 seconds\n",
      "Starting vectorization of test data...\n",
      "Completed vectorization of test data in 3.75 seconds\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "np.random.seed(455)\n",
    "\n",
    "# This preprocessor will be used to process data prior to vectorization\n",
    "nltkPreprocessor = NLTKPreprocessor()\n",
    "    \n",
    "# Note that this vectorizer is created with a passthru tokenizer(identity), no preprocessor and no lowercasing\n",
    "# This is to account for the NLTKPreprocessor already taking care of these.\n",
    "tfidfVector = TfidfVectorizer(ngram_range=(1,1), min_df=5, max_features=15000,\n",
    "                              tokenizer=identity, preprocessor=None, lowercase=False)\n",
    "\n",
    "print \"Starting final preprocessing of training data...\"\n",
    "start_train_preproc = time.time()\n",
    "trainPreprocData = nltkPreprocessor.fit_transform(train_df[\"comment_text\"])\n",
    "finish_train_preproc = time.time()\n",
    "print \"Completed tokenization/preprocessing of training data in {:.2f} seconds\".format((finish_train_preproc-start_train_preproc))\n",
    "\n",
    "print \"Starting final preprocessing of test data...\"\n",
    "start_test_preproc = time.time()\n",
    "testPreprocData = nltkPreprocessor.transform(test_df[\"comment_text\"])\n",
    "finish_test_preproc = time.time()\n",
    "print \"Completed tokenization/preprocessing of test data in {:.2f} seconds\".format((finish_test_preproc-start_test_preproc))\n",
    "\n",
    "print \"Starting vectorization of training data...\"\n",
    "start_train_vectors = time.time()\n",
    "finalTrainCounts = tfidfVector.fit_transform(trainPreprocData)\n",
    "finish_train_vectors = time.time()\n",
    "print \"Completed vectorization of training data in {:.2f} seconds\".format((finish_train_vectors-start_train_vectors))\n",
    "\n",
    "print \"Starting vectorization of test data...\"\n",
    "start_test_vectors = time.time()\n",
    "finalTestCounts = tfidfVector.transform(testPreprocData)\n",
    "finish_test_vectors = time.time()\n",
    "print \"Completed vectorization of test data in {:.2f} seconds\".format((finish_test_vectors-start_test_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final LSA Feature Selection - training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSA on train counts with 1500 components...\n",
      "Train counts transform took 351.07 seconds.\n",
      "Starting LSA on test counts with 1500 components...\n",
      "Test counts transform took 345.80 seconds.\n"
     ]
    }
   ],
   "source": [
    "target_components = len(tfidfVector.vocabulary_)/10\n",
    "svd = TruncatedSVD(n_components=target_components, algorithm='arpack')\n",
    "print \"Starting LSA on train counts with {} components...\".format(target_components)\n",
    "train_start=time.time()\n",
    "lsaTrainCounts = svd.fit_transform(finalTraincounts)\n",
    "train_stop=time.time()\n",
    "print \"Train counts transform took {:.2f} seconds.\".format(train_stop-train_start)\n",
    "\n",
    "target_components = len(tfidfVector.vocabulary_)/10\n",
    "svd = TruncatedSVD(n_components=target_components, algorithm='arpack')\n",
    "print \"Starting LSA on test counts with {} components...\".format(target_components)\n",
    "train_start=time.time()\n",
    "lsaTestCounts = svd.fit_transform(finalTestCounts)\n",
    "train_stop=time.time()\n",
    "print \"Test counts transform took {:.2f} seconds.\".format(train_stop-train_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final MLPClassifier Training and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with MLPClassifier (shallow/wide Net)\n",
      "Training for class toxic completed, after 8.02 minutes.\n",
      "Prediction for class toxic completed, after 0.13 minutes.\n",
      "Training for class severe_toxic completed, after 3.23 minutes.\n",
      "Prediction for class severe_toxic completed, after 0.01 minutes.\n",
      "Training for class obscene completed, after 4.65 minutes.\n",
      "Prediction for class obscene completed, after 0.01 minutes.\n",
      "Training for class threat completed, after 1.46 minutes.\n",
      "Prediction for class threat completed, after 0.01 minutes.\n",
      "Training for class insult completed, after 5.89 minutes.\n",
      "Prediction for class insult completed, after 0.08 minutes.\n",
      "Training for class identity_hate completed, after 3.03 minutes.\n",
      "Prediction for class identity_hate completed, after 0.01 minutes.\n",
      "                 id         toxic  severe_toxic       obscene    threat  \\\n",
      "0  00001cee341fdb12  9.413830e-04  3.019389e-06  8.532108e-01  0.000011   \n",
      "1  0000247867823ef7  6.041182e-06  1.038509e-06  2.992874e-07  0.000010   \n",
      "2  00013b17ad220c46  4.357374e-05  7.132383e-06  2.466334e-02  0.000015   \n",
      "3  00017563c3f7919a  6.782458e-08  8.802607e-06  1.052156e-07  0.000005   \n",
      "4  00017695ad8997eb  1.593374e-10  2.194619e-06  4.212840e-10  0.000027   \n",
      "5  0001ea8717f6de06  4.094924e-06  3.493270e-08  2.018844e-07  0.000012   \n",
      "6  00024115d4cbde0f  1.302152e-10  1.785313e-06  1.571592e-07  0.000005   \n",
      "7  000247e83dcc1211  5.608019e-01  1.183878e-03  1.119971e-05  0.000033   \n",
      "8  00025358d4737918  1.770417e-04  1.229857e-04  8.791228e-01  0.000022   \n",
      "9  00026d1092fe71cc  9.960947e-03  6.831042e-08  1.852537e-11  0.000012   \n",
      "\n",
      "         insult  identity_hate  \n",
      "0  8.018887e-01   9.251315e-08  \n",
      "1  6.112533e-04   6.839443e-06  \n",
      "2  8.597253e-07   1.655742e-05  \n",
      "3  1.098730e-04   1.015704e-07  \n",
      "4  8.720402e-09   7.495807e-08  \n",
      "5  4.073561e-08   8.389998e-05  \n",
      "6  1.083564e-08   7.126852e-08  \n",
      "7  6.550980e-05   2.550736e-06  \n",
      "8  7.175040e-05   5.294053e-06  \n",
      "9  9.835920e-09   7.126948e-08  \n"
     ]
    }
   ],
   "source": [
    "prediction_submission = pd.DataFrame()\n",
    "prediction_submission[\"id\"] = test_df[\"id\"]\n",
    "\n",
    "print(\"Training with MLPClassifier (shallow/wide Net)\")\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(12,6), activation='tanh', learning_rate='adaptive')\n",
    "\n",
    "# Training with shallow/wide Neural Net \n",
    "for name in target_names:\n",
    "    \n",
    "    label_train_start = time.time()\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(12,6), activation='tanh', learning_rate='adaptive')\n",
    "    classifier.fit(lsaTrainCounts, train_df[name])\n",
    "    label_train_finish = time.time()\n",
    "    print('Training for class {} completed, after {:.2f} minutes.'.format(name, \n",
    "                                                                          (label_train_finish-label_train_start)/60))\n",
    "    label_predict_start = time.time()\n",
    "    prediction_submission[name] = classifier.predict_proba(lsaTestCounts)[:, 1]\n",
    "    label_predict_finish = time.time()\n",
    "    print('Prediction for class {} completed, after {:.2f} minutes.'.format(name,\n",
    "                                                                    (label_predict_finish-label_predict_start)/60))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "print(prediction_submission.head(10)) # print frame output \n",
    "prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission - based on test preprocessing, LSA feature selection and MLPClassifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_submission = pd.DataFrame()\n",
    "prediction_submission[\"id\"] = test_df[\"id\"]\n",
    "\n",
    "# new vector object for all train data for submission\n",
    "finalTrainVector = CountVectorizer()\n",
    "finalTrainCount = finalTrainVector.fit_transform(train_df[\"comment_text\"])\n",
    "\n",
    "# TODO: Using pipelines can clean up repetitive processes\n",
    "# test set up\n",
    "#testVector = CountVectorizer()\n",
    "testCount = finalTrainVector.transform(test_df[\"comment_text\"])\n",
    "\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver='sag') #sag is one kind of solver optimize for multi-label\n",
    "    clf = classifier.fit(finalTrainCount, train_df[name])\n",
    "    prediction_submission[name] = clf.predict_proba(testCount)[:, 1]\n",
    "    #print(prediction_submission)\n",
    "\n",
    "    \n",
    "print(prediction_submission.head(10)) # print frame output \n",
    "prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_submission = pd.DataFrame()\n",
    "prediction_submission[\"id\"] = test_df[\"id\"]\n",
    "\n",
    "# new vector object for all train data for submission\n",
    "finalTrainVector = CountVectorizer()\n",
    "finalTrainCount = finalTrainVector.fit_transform(train_df[\"comment_text\"])\n",
    "\n",
    "# TODO: Using pipelines can clean up repetitive processes\n",
    "# test set up\n",
    "#testVector = CountVectorizer()\n",
    "testCount = finalTrainVector.transform(test_df[\"comment_text\"])\n",
    "\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver='sag') #sag is one kind of solver optimize for multi-label\n",
    "    clf = classifier.fit(finalTrainCount, train_df[name])\n",
    "    prediction_submission[name] = clf.predict_proba(testCount)[:, 1]\n",
    "    #print(prediction_submission)\n",
    "\n",
    "    \n",
    "print(prediction_submission.head(10)) # print frame output \n",
    "prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frame contains the output for each class and is saved in a pandas data frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
