{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Submission: Toxic Language Classification \n",
    "**w207 Spring 2018 - Final Project Baseline**\n",
    "\n",
    "**Team: Paul, Walt, Yisang, Joe**\n",
    "\n",
    "\n",
    "\n",
    "### Project Description \n",
    "\n",
    "Our challenge is to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.  The toxic language data set is sourced from Wikipedia and available as a public kaggle data set. \n",
    "\n",
    "Our goal is to use various machine learning techniques used in class to develop high quality ML models and pipelines.  \n",
    "\n",
    "1. Exercise and build upon concepts covered in class and test out at least 3 kinds of supervised models:\n",
    "    a. Regression (LASSO, Logistic)\n",
    "    b. Trees (RF, XGBoost)\n",
    "    c. DeepLearning (Tensorflow)\n",
    "2. Using stacking/ensembling methods for improving prediction metrics (K-Means, anomaly detection)\n",
    "3. Using unsupervised methods for feature engineering/selection\n",
    "\n",
    "For the baseline proposal, this file contains a first pass run through from data preprocessing to model evaluation using a regression model pipeline. \n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "\n",
    "\n",
    "#NLTK imports\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import punkt as punkt\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# These imports enable the use of NLTKPreprocessor in an sklearn Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "#General imports\n",
    "import pprint\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 159571\n",
      "training data shape: (111906,)\n",
      "training label shape: (111906, 6)\n",
      "dev label shape: (47665, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "np.random.seed(455)\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "print 'total training observations:', train_df.shape[0]\n",
    "print 'training data shape:', train_data.shape\n",
    "print 'training label shape:', train_labels.shape\n",
    "\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "0           0             0        0       0       0              0\n",
      "1           0             0        0       0       0              0\n",
      "2           0             0        0       0       0              0\n",
      "3           0             0        0       0       0              0\n",
      "6           1             1        1       0       1              0\n",
      "7           0             0        0       0       0              0\n",
      "8           0             0        0       0       0              0\n",
      "9           0             0        0       0       0              0\n",
      "10          0             0        0       0       0              0\n",
      "11          0             0        0       0       0              0\n",
      "12          1             0        0       0       0              0\n",
      "13          0             0        0       0       0              0\n",
      "15          0             0        0       0       0              0\n",
      "16          1             0        0       0       0              0\n",
      "18          0             0        0       0       0              0\n",
      "19          0             0        0       0       0              0\n",
      "20          0             0        0       0       0              0\n",
      "23          0             0        0       0       0              0\n",
      "25          0             0        0       0       0              0\n",
      "26          0             0        0       0       0              0\n",
      "28          0             0        0       0       0              0\n",
      "29          0             0        0       0       0              0\n",
      "30          0             0        0       0       0              0\n",
      "32          0             0        0       0       0              0\n",
      "33          0             0        0       0       0              0\n",
      "35          0             0        0       0       0              0\n",
      "37          0             0        0       0       0              0\n",
      "38          0             0        0       0       0              0\n",
      "39          0             0        0       0       0              0\n",
      "40          0             0        0       0       0              0\n",
      "...       ...           ...      ...     ...     ...            ...\n",
      "159529      0             0        0       0       0              0\n",
      "159530      0             0        0       0       0              0\n",
      "159531      0             0        0       0       0              0\n",
      "159532      0             0        0       0       0              0\n",
      "159533      0             0        0       0       0              0\n",
      "159534      0             0        0       0       0              0\n",
      "159535      0             0        0       0       0              0\n",
      "159536      0             0        0       0       0              0\n",
      "159537      0             0        0       0       0              0\n",
      "159539      0             0        0       0       0              0\n",
      "159540      0             0        0       0       0              0\n",
      "159541      1             0        1       0       1              0\n",
      "159543      0             0        0       0       0              0\n",
      "159544      0             0        0       0       0              0\n",
      "159545      0             0        0       0       0              0\n",
      "159546      1             0        0       0       1              0\n",
      "159548      0             0        0       0       0              0\n",
      "159549      0             0        0       0       0              0\n",
      "159550      0             0        0       0       0              0\n",
      "159551      0             0        0       0       0              0\n",
      "159552      0             0        0       0       0              0\n",
      "159553      0             0        0       0       0              0\n",
      "159555      0             0        0       0       0              0\n",
      "159556      0             0        0       0       0              0\n",
      "159558      0             0        0       0       0              0\n",
      "159560      0             0        0       0       0              0\n",
      "159561      0             0        0       0       0              0\n",
      "159562      0             0        0       0       0              0\n",
      "159566      0             0        0       0       0              0\n",
      "159570      0             0        0       0       0              0\n",
      "\n",
      "[111906 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/burgew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Text preprocessor using NLTK tokenization and Lemmatization\n",
    "\n",
    "    This class is to be used in an sklean Pipeline, prior to other processers like PCA/LSA/classification\n",
    "    Attributes:\n",
    "        lower: A boolean indicating whether text should be lowercased by preprocessor\n",
    "                default: True\n",
    "        strip: A boolean indicating whether text should be stripped of surrounding whitespace, underscores and '*'\n",
    "                default: True\n",
    "        stopwords: A set of words to be used as stop words and thus ignored during tokenization\n",
    "                default: built-in English stop words\n",
    "        punct: A set of punctuation characters that should be ignored\n",
    "                default: None\n",
    "        lemmatizer: An object that should be used to lemmatize tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        \"\"\"Initialize method for NLTKPreprocessor instance\n",
    "\n",
    "        Simple initialization of specified instance variables:\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            stopwords: set of words to ignore as stop words, or a default set for English will be used\n",
    "            punct: set of punctuation characters to strip, or a default set will be used\n",
    "            lower: indicator of whether to convert all characters to lowercase, defaults to True\n",
    "            strip: indicator of whether to strip whitespace, defaults to True\n",
    "\n",
    "        Returns:\n",
    "            N/A: instance initializer\n",
    "\n",
    "        \"\"\"\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit model with X and optional y\n",
    "\n",
    "        This function does nothing but return self, since as a processor in the sklearn Pipeline this preprocessor\n",
    "        has nothing analogous to \"fit\" logic. The tokenization logic is independent of specific dataset training, \n",
    "        and is fully realized in the transform() function. \n",
    "        This function exists as implementation of sklearn.BaseEstimator, for use in Pipeline.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X (array-like): independent variable\n",
    "            y (array-like): dependent variable\n",
    "            \n",
    "        Returns:\n",
    "            NLTKPreprocessor: self\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Function exists as implementation of sklearn.BaseEstimator, for use in Pipeline.\n",
    "        This is simply for complying with interface.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X (array-like): input documents\n",
    "            \n",
    "        Returns:\n",
    "            string: joined documents\n",
    "        \"\"\"\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform input X to produce output to be processed by next element in sklearn Pipeline\n",
    "\n",
    "        This triggers the tokenization/lemmatization of the source documents.\n",
    "        This is invoked by the sklearn Pipeline.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X: input documents to be tokenized\n",
    "            \n",
    "        Returns:\n",
    "            list: tokenized documents reduced to simplest lemma form\n",
    "        \"\"\"\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    \n",
    "    def tokenize(self, document):\n",
    "        \"\"\"Tokenize an input document, converting from a block of text into sentences, into tagged tokens,\n",
    "        generating a set of lemmas.\n",
    "\n",
    "        This method does the preprocessing work of sentence-based tokenization and then reduces words to lemmas\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X (array-like): independent variable\n",
    "            y (array-like): dependent variable\n",
    "            \n",
    "        Returns:\n",
    "            Iterator[str]: an iterator over the tokens produced from the input documents\n",
    "        \"\"\"\n",
    "        # Break the document into sentences. This is necessary for part-of-speech tagging.\n",
    "        for sent in sent_tokenize(unicode(document,'utf-8')):\n",
    "\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "                \n",
    "    def lemmatize(self, token, tag):\n",
    "        \"\"\"Convert a token into the appropriate lemma\n",
    "\n",
    "        Method uses the NLTK WordNetLemmatizer for part-of-speech tag-based lemmatization of words.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            token: input word\n",
    "            tag: part-of-speech tag\n",
    "            \n",
    "        Returns:\n",
    "            string: lemma\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\" Simple identity function works as a passthrough.\n",
    "\n",
    "        This function will be used with the Vectorizer classes, when tokenization will have been performed already.\n",
    "        In this scenario, the Vectorizer class will call this function in the place of its normal tokenization feature\n",
    "        and this function will simply return the input token.\n",
    "        \n",
    "        Args:\n",
    "            token (string): text token being evaluated by CountVectorizer or TfidfVectorizer\n",
    "            \n",
    "        Returns:\n",
    "            string: input token unchanged (processed earlier by NLTK) will tbe returned\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Text Preprocessing - training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "This block uses the NLTKPreprocessor to tokenize the input data and then the TfidfVectorizer to vectorize it. The NLTKPreprocessor will ignore English stop words and will lemmatize where possible. The vectorizer ignores words occuring in fewer than 5 documents, which sufficed to reduce the size of the words vector significantly. Also, the vectorizer will limit the total features (words) to 15000, prioritizing the most valuable ones with highest TF-IDF score.\n",
    "\n",
    "Note that in this case the tokenization available by default in TfidfVectorizer is disabled, since that is handled by the NLTKPreprocessor. This made it clear that tokenization is by far more expensive (time) than vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(455)\n",
    "\n",
    "# Set the maximum features to be produced by the preprocessing pipelne\n",
    "max_features = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing input data, gererating output of 6000 maximum features.\n",
      "Starting vectorization of training data...\n",
      "Completed vectorization of training data in 8.20 seconds\n",
      "Starting vectorization of dev data...\n",
      "Completed vectorization of dev data in 3.43 seconds\n",
      "\n",
      "Vocabulary (tfidf) size is: 6000\n",
      "Sample vocabulary from TfidfVectorizer:\n",
      "               count\n",
      ",”                 0\n",
      ".—                 1\n",
      ".”                 2\n",
      "0                  3\n",
      "0 0                4\n",
      "0 cellpadding      5\n",
      "0 cellspacing      6\n",
      "00                 7\n",
      "000                8\n",
      "000000             9\n",
      "None\n",
      "...\n",
      "              count\n",
      "• five         5990\n",
      "• notability   5991\n",
      "• sock         5992\n",
      "• spam         5993\n",
      "• talk         5994\n",
      "• upload       5995\n",
      "• vandalism    5996\n",
      "…              5997\n",
      "←              5998\n",
      "→              5999\n",
      "None\n",
      "Number of nonzero entries in matrix: 2957584\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    counts\n",
       "6        4\n",
       "12       1\n",
       "16       1\n",
       "42       4\n",
       "43       3\n",
       "44       1\n",
       "51       2\n",
       "55       4\n",
       "56       3\n",
       "58       2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "\n",
    "# This preprocessor will be used to process data prior to vectorization\n",
    "nltkPreprocessor = NLTKPreprocessor()\n",
    "    \n",
    "# Note that this vectorizer is created with a passthru tokenizer(identity), no preprocessor and no lowercasing\n",
    "# This is to account for the NLTKPreprocessor already taking care of tokenization.\n",
    "tfidfVector = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=.7, max_features=max_features,\n",
    "                              tokenizer=identity, preprocessor=None, lowercase=False, stop_words={'english'})\n",
    "\n",
    "print(\"Preprocessing input data, gererating output of \"+ str(max_features) + \" maximum features.\")\n",
    "\n",
    "# Check if there is a serialized copy of the preprocessed training data, and if not the perform text preprocessing and\n",
    "# save the serialized result for reuse.\n",
    "pickle_file_name = 'train_preproc_data.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    print \"Starting preprocessing of training data...\"\n",
    "    start_train_preproc = time.time()\n",
    "    nltkPreprocessor.fit(train_data)\n",
    "    train_preproc_data = nltkPreprocessor.transform(train_data)\n",
    "    finish_train_preproc = time.time()\n",
    "    print \"Completed tokenization/preprocessing of training data in {:.2f} seconds\".format(finish_train_preproc-start_train_preproc)\n",
    "    \n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(train_preproc_data,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        train_preproc_data = pickle.load(pickle_file)\n",
    "\n",
    "tfidfVector.fit(train_preproc_data)\n",
    "        \n",
    "# Check if there is a serialized copy of the vectorized counts, and if not regenerate the matrix and save the\n",
    "# serialized result for reuse.        \n",
    "pickle_file_name = 'train_tfidf_counts.'+str(max_features)+'.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    \n",
    "    # Generating new TF-IDF train counts means we need to then re-apply LSA to the results, so remove the LSA results\n",
    "    #remove_file('lsa_train_counts.pickle')\n",
    "    print \"Starting vectorization of training data...\"\n",
    "    start_train_vectors = time.time()\n",
    "    train_tfidf_counts = tfidfVector.transform(train_preproc_data)\n",
    "    finish_train_vectors = time.time()\n",
    "    print \"Completed vectorization of training data in {:.2f} seconds\".format(finish_train_vectors-start_train_vectors)\n",
    "    \n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(train_tfidf_counts,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        train_tfidf_counts = pickle.load(pickle_file)\n",
    "    \n",
    "# Check if there is a serialized copy of the preprocessed dev data, and if not the perform text preprocessing and\n",
    "# save the serialized result for reuse.\n",
    "pickle_file_name = 'dev_preproc_data.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    print \"\\nStarting preprocessing of dev data...\"\n",
    "    start_dev_preproc = time.time()\n",
    "    nltkPreprocessor.fit(dev_data)\n",
    "    dev_preproc_data = nltkPreprocessor.transform(dev_data)\n",
    "    finish_dev_preproc = time.time()\n",
    "    print \"Completed tokenization/preprocessing of dev data in {:.2f} seconds\".format(finish_dev_preproc-start_dev_preproc)\n",
    "\n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(dev_preproc_data,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        dev_preproc_data = pickle.load(pickle_file)\n",
    "    \n",
    "pickle_file_name = 'dev_tfidf_counts.'+str(max_features)+'.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    \n",
    "    \n",
    "    # Generating new TF-IDF dev counts means we need to then re-apply LSA to the results, so remove the LSA results\n",
    "    #remove_file('lsa_dev_counts.pickle')\n",
    "    \n",
    "    print \"Starting vectorization of dev data...\"\n",
    "    start_dev_vectors = time.time()\n",
    "    dev_tfidf_counts = tfidfVector.transform(dev_preproc_data)\n",
    "    finish_dev_vectors = time.time()\n",
    "    print \"Completed vectorization of dev data in {:.2f} seconds\".format(finish_dev_vectors-start_dev_vectors)\n",
    "\n",
    "\n",
    "    print(\"\\nVocabulary (tfidf) size is: {}\").format(len(tfidfVector.vocabulary_))\n",
    "    vocab_entries = {k: tfidfVector.vocabulary_[k] for k in tfidfVector.vocabulary_.keys()}\n",
    "    vocab_entries = pd.Series(vocab_entries).to_frame()\n",
    "    vocab_entries.columns = ['count']\n",
    "    vocab_entries = vocab_entries.sort_values(by='count')\n",
    "\n",
    "    print(\"Sample vocabulary from TfidfVectorizer:\")\n",
    "    print(pp.pprint(vocab_entries.head(10)))\n",
    "    print(\"...\")\n",
    "    print(pp.pprint(vocab_entries.tail(10)))\n",
    "    print(\"Number of nonzero entries in matrix: {}\").format(train_tfidf_counts.nnz)\n",
    "\n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(dev_tfidf_counts,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        dev_tfidf_counts = pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "# Print sample column wise sum, we can see that an observation can have multiple classes.\n",
    "count_df = pd.DataFrame(train_labels.apply(np.sum,1), columns = [\"counts\"])\n",
    "count_df = count_df[((count_df[\"counts\"] >= 1))]\n",
    "count_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final LSA Feature Selection - training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA/LSA\n",
    "Principal Component Analysis (PCA) and Latent Semantic Analysis (LSA) are both operations that use Singular Value Decomposition to reduce the dimensionality of a dataset. PCA is applied to a term-covariance matrix, whereas LSA is applied to a term-document matrix. As such, LSA is appropriate for machine learning algorithms using scikit-learn TfidfVectorizer. Additionally PCA, as implemented in scikit-learn, cannot handle the sparse matrices that are produced by such vectorization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSA on train counts with 4000 components...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k must be between 1 and min(A.shape), k=4000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-128204883833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Starting LSA on train counts with {} components...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlsa_train_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tfidf_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Train counts transform took {:.2f} minutes.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_stop\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrain_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/burgew/Library/Python/2.7/lib/python/site-packages/sklearn/decomposition/truncated_svd.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"arpack\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;31m# svds doesn't abide by scipy.linalg.svd/randomized_svd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# conventions, so reverse its outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/burgew/Library/Python/2.7/lib/python/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.pyc\u001b[0m in \u001b[0;36msvds\u001b[0;34m(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"k must be between 1 and min(A.shape), k=%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: k must be between 1 and min(A.shape), k=4000"
     ]
    }
   ],
   "source": [
    "# Set the number of principal components to identify for use in classification processes\n",
    "target_components = 4000\n",
    "\n",
    "# Check if there is a serialized copy of the Principal Components data for the training dataset, and if not then\n",
    "# perform LSA processing and save the serialized result for reuse.\n",
    "pickle_file_name = 'lsa_train_counts.4000.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    svd = TruncatedSVD(n_components=target_components, algorithm='arpack')\n",
    "    print \"Starting LSA on train counts with {} components...\".format(target_components)\n",
    "    train_start=time.time()\n",
    "    lsa_train_counts = svd.fit_transform(train_tfidf_counts)\n",
    "    train_stop=time.time()\n",
    "    print \"Train counts transform took {:.2f} minutes.\".format((train_stop-train_start)/60)\n",
    "    \n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(lsa_train_counts,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        lsa_train_counts = pickle.load(pickle_file)\n",
    " \n",
    "# Check if there is a serialized copy of the Principal Components data for the dev dataset, and if not then\n",
    "# perform LSA processing and save the serialized result for reuse.\n",
    "pickle_file_name = 'lsa_dev_counts.4000.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    print \"Starting LSA on dev counts with {} components...\".format(target_components)\n",
    "    dev_start=time.time()\n",
    "    lsa_dev_counts = svd.fit_transform(dev_tfidf_counts)\n",
    "    dev_stop=time.time()\n",
    "    print \"Dev counts transform took {:.2f} minutes.\".format((dev_stop-dev_start)/60)\n",
    "    \n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(lsa_dev_counts,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        lsa_dev_counts = pickle.load(pickle_file)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final MLPClassifier Training and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification with Neural Net (sklearn.MLPClassifier)\n",
    "In choosing a neural net model for text classification, the output layer should have the same number of nodes as the number of classification labels. In this case, there are 6 labels and as such not only will the output layer have 6 nodes, but the final hidden layer as well. The input layer will have the same number of nodes as features, normally, and ideally the initial hidden layer will be between that and the number of classes.\n",
    "\n",
    "In this case, we're limiting our feature set to 5,000 principal components, and it was not possible to use a number of initial hidden layer nodes at all close to that, running this process on a Macbook. So, setting the initial hidden layer to 12 gave at least some benefit of being less than the number of features and greater than the number of output classes. This (12,6) model is the one that ended up producing best (most accurate) results.\n",
    "\n",
    "Note that, nod toward deeper learning, a (10,8,6) model was also tested, but this ended up demonstrating overfitting, with a signficantly higher accuracy score on test data than on dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting train data completed, after 0.63 minutes.\n",
      "Accuracy score from dev predict: 0.913689289835\n",
      "Precision score from dev predict: [0.93378227 0.72222222 0.91603053 0.         0.81189591 0.        ]\n",
      "Recall score from dev predict: [0.54190187 0.02708333 0.56096611 0.         0.45747801 0.        ]\n",
      "Re-fitting and scoring for per-label roc_auc scores...\n",
      "ROC AUC score from dev predict:  [0.9541632946310213, 0.985883468439829, 0.9796801694886588, 0.9524828728756594, 0.9714389768200706, 0.959351666985402]\n",
      "Precision, recall, fbeta_score, support and ROC AUC:\n",
      "                   toxic  severe_toxic      obscene      threat       insult  \\\n",
      "precision       0.933782      0.722222     0.916031    0.000000     0.811896   \n",
      "recall          0.541902      0.027083     0.560966    0.000000     0.457478   \n",
      "fbeta_score     0.685808      0.052209     0.695820    0.000000     0.585209   \n",
      "support      4606.000000    480.000000  2567.000000  155.000000  2387.000000   \n",
      "roc_auc         0.954163      0.985883     0.979680    0.952483     0.971439   \n",
      "\n",
      "             identity_hate  \n",
      "precision         0.000000  \n",
      "recall            0.000000  \n",
      "fbeta_score       0.000000  \n",
      "support         415.000000  \n",
      "roc_auc           0.959352  \n",
      "      toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
      "0  0.050357      0.003582  0.016484  0.002792  0.017528       0.005905\n",
      "1  0.010328      0.001599  0.004336  0.001832  0.005498       0.003244\n",
      "2  0.060978      0.003960  0.019440  0.002943  0.020229       0.006362\n",
      "3  0.005205      0.001135  0.002451  0.001532  0.003350       0.002515\n",
      "4  0.024093      0.002452  0.008814  0.002291  0.010179       0.004457\n",
      "5  0.018341      0.002135  0.007008  0.002131  0.008342       0.004022\n",
      "6  0.001874      0.000682  0.001049  0.001174  0.001603       0.001722\n",
      "7  0.434237      0.013397  0.133168  0.005575  0.109065       0.015720\n",
      "8  0.054439      0.003731  0.017627  0.002852  0.018579       0.006087\n",
      "9  0.006066      0.001225  0.002784  0.001594  0.003742       0.002662\n"
     ]
    }
   ],
   "source": [
    "max_features = 6000\n",
    "    \n",
    "# This MLPClassifier will be fit using training data and subsequently used to predict labels using dev\n",
    "# data, for scoring.\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(12,6), solver='adam', early_stopping=False, activation='relu',\n",
    "                           tol=1e-13, alpha=1, learning_rate='adaptive', learning_rate_init=0.01, )\n",
    "\n",
    "train_pickle_file = 'train_tfidf_counts.'+str(max_features)+'.pickle'\n",
    "with open(train_pickle_file,'r') as pickle_file:\n",
    "     train_input_data = pickle.load(pickle_file)\n",
    "\n",
    "dev_pickle_file = 'dev_tfidf_counts.'+str(max_features)+'.pickle'\n",
    "with open(dev_pickle_file,'r') as pickle_file:\n",
    "     dev_input_data = pickle.load(pickle_file)\n",
    "        \n",
    "# Fit using the training data and time the process for reference\n",
    "full_train_start = time.time()\n",
    "classifier.fit(train_input_data, train_labels)\n",
    "full_train_stop = time.time()\n",
    "\n",
    "duration = (full_train_stop-full_train_start)/60\n",
    "print('Fitting train data completed, after {:.2f} minutes.'.format(duration))\n",
    "\n",
    "# Generate predictions using the dev LSA data and collect a series of scores\n",
    "dev_pred = classifier.predict(dev_input_data)\n",
    "acc_score = metrics.accuracy_score(dev_labels, dev_pred)\n",
    "\n",
    "# Note that, since this is multilabel data, an F1 score must be evaluated with either results weighted across labels or\n",
    "# as samples taken from each.\n",
    "precision_recall_fscore = metrics.precision_recall_fscore_support(dev_labels, dev_pred, average=None)\n",
    "precision = metrics.precision_score(dev_labels, dev_pred, average=None)\n",
    "recall = metrics.recall_score(dev_labels, dev_pred, average=None)\n",
    "\n",
    "# Prediction probabilities will be saved for comparison with other models and processing by ensembles\n",
    "predict_probs = classifier.predict_proba(dev_input_data)\n",
    "\n",
    "print(\"Accuracy score from dev predict: {}\".format(acc_score))\n",
    "print(\"Precision score from dev predict: {}\".format(precision))\n",
    "print(\"Recall score from dev predict: {}\".format(recall))\n",
    "\n",
    "# Fitting again with binarized labels and predicting again to support per-label roc_auc scores\n",
    "binarized_train_labels = label_binarize(train_labels, classes=[0, 1, 2, 3, 4, 5])\n",
    "binarized_dev_labels = label_binarize(dev_labels, classes=[0, 1, 2, 3, 4, 5])\n",
    "\n",
    "# While this is multilabel data, the sklearn ROC AUC scoring feature doesn't support multilabel data directly.\n",
    "# So, instead the model will be re-trained with binarized training labels and the predicted probabilities used\n",
    "# To derive ROC AUC for each label. This is mainly for comparison with other models, since these numbers won't\n",
    "# be directly related to the multilabel classification.\n",
    "print(\"Re-fitting and scoring for per-label roc_auc scores...\")\n",
    "y_score = classifier.fit(train_input_data, binarized_train_labels).predict_proba(dev_input_data)\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = []\n",
    "for ind, label in enumerate(target_names):\n",
    "    fpr[ind], tpr[ind], _ = metrics.roc_curve(binarized_dev_labels[:, ind], y_score[:, ind])\n",
    "    roc_auc.append(metrics.auc(fpr[ind], tpr[ind]))\n",
    "\n",
    "print \"ROC AUC score from dev predict: \", roc_auc\n",
    "\n",
    "# In order to Save the complete collection of scores, a pandas.DataFrame will be created and used to create\n",
    "# \"scoring.csv\".\n",
    "scoring_arr = np.asarray(precision_recall_fscore)\n",
    "scoring_arr = np.vstack([scoring_arr,roc_auc])\n",
    "#means = np.sum(scoring_arr,axis=1)/np.size(scoring_arr,axis=1)\n",
    "#means.shape = (np.size(scoring_arr,axis=0),1)\n",
    "#scoring_arr = np.append(scoring_arr, means, axis=1)\n",
    "scoring_submission = pd.DataFrame(data=scoring_arr, columns=target_names, index=['precision', 'recall', \n",
    "                                                                                 'fbeta_score', 'support',\n",
    "                                                                                 'roc_auc'])\n",
    "print(\"Precision, recall, fbeta_score, support and ROC AUC:\")\n",
    "print(scoring_submission)\n",
    "scoring_submission.to_csv(\"../data/NN.scoring.\"+str(max_features)+\".csv\")\n",
    "\n",
    "# The predicted probabilities from the initial version of the model will be saved in CSV file \"submission.csv\"\n",
    "prediction_submission = pd.DataFrame(data=predict_probs,columns=target_names)\n",
    "print(prediction_submission[0:10]) # print frame output \n",
    "prediction_submission.to_csv(\"../data/NN.submission.\"+str(max_features)+\".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
