{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Submission: Toxic Language Classification \n",
    "**w207 Spring 2018 - Final Project Baseline**\n",
    "\n",
    "**Team: Paul, Walt, Yisang, Joe**\n",
    "\n",
    "\n",
    "\n",
    "### Project Description \n",
    "\n",
    "Our challenge is to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.  The toxic language data set is sourced from Wikipedia and available as a public kaggle data set. \n",
    "\n",
    "Our goal is to use various machine learning techniques used in class to develop high quality ML models and pipelines.  \n",
    "\n",
    "1. Exercise and build upon concepts covered in class and test out at least 3 kinds of supervised models:\n",
    "    a. Regression (LASSO, Logistic)\n",
    "    b. Trees (RF, XGBoost)\n",
    "    c. DeepLearning (Tensorflow)\n",
    "2. Using stacking/ensembling methods for improving prediction metrics (K-Means, anomaly detection)\n",
    "3. Using unsupervised methods for feature engineering/selection\n",
    "\n",
    "For the baseline proposal, this file contains a first pass run through from data preprocessing to model evaluation using a regression model pipeline. \n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "\n",
    "\n",
    "#NLTK imports\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import punkt as punkt\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# These imports enable the use of NLTKPreprocessor in an sklearn Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "#General imports\n",
    "import pprint\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 159571\n",
      "training data shape: (111906,)\n",
      "training label shape: (111906, 6)\n",
      "dev label shape: (47665, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "np.random.seed(455)\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "print 'total training observations:', train_df.shape[0]\n",
    "print 'training data shape:', train_data.shape\n",
    "print 'training label shape:', train_labels.shape\n",
    "\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
      "0           0             0        0       0       0              0\n",
      "1           0             0        0       0       0              0\n",
      "2           0             0        0       0       0              0\n",
      "3           0             0        0       0       0              0\n",
      "6           1             1        1       0       1              0\n",
      "7           0             0        0       0       0              0\n",
      "8           0             0        0       0       0              0\n",
      "9           0             0        0       0       0              0\n",
      "10          0             0        0       0       0              0\n",
      "11          0             0        0       0       0              0\n",
      "12          1             0        0       0       0              0\n",
      "13          0             0        0       0       0              0\n",
      "15          0             0        0       0       0              0\n",
      "16          1             0        0       0       0              0\n",
      "18          0             0        0       0       0              0\n",
      "19          0             0        0       0       0              0\n",
      "20          0             0        0       0       0              0\n",
      "23          0             0        0       0       0              0\n",
      "25          0             0        0       0       0              0\n",
      "26          0             0        0       0       0              0\n",
      "28          0             0        0       0       0              0\n",
      "29          0             0        0       0       0              0\n",
      "30          0             0        0       0       0              0\n",
      "32          0             0        0       0       0              0\n",
      "33          0             0        0       0       0              0\n",
      "35          0             0        0       0       0              0\n",
      "37          0             0        0       0       0              0\n",
      "38          0             0        0       0       0              0\n",
      "39          0             0        0       0       0              0\n",
      "40          0             0        0       0       0              0\n",
      "...       ...           ...      ...     ...     ...            ...\n",
      "159529      0             0        0       0       0              0\n",
      "159530      0             0        0       0       0              0\n",
      "159531      0             0        0       0       0              0\n",
      "159532      0             0        0       0       0              0\n",
      "159533      0             0        0       0       0              0\n",
      "159534      0             0        0       0       0              0\n",
      "159535      0             0        0       0       0              0\n",
      "159536      0             0        0       0       0              0\n",
      "159537      0             0        0       0       0              0\n",
      "159539      0             0        0       0       0              0\n",
      "159540      0             0        0       0       0              0\n",
      "159541      1             0        1       0       1              0\n",
      "159543      0             0        0       0       0              0\n",
      "159544      0             0        0       0       0              0\n",
      "159545      0             0        0       0       0              0\n",
      "159546      1             0        0       0       1              0\n",
      "159548      0             0        0       0       0              0\n",
      "159549      0             0        0       0       0              0\n",
      "159550      0             0        0       0       0              0\n",
      "159551      0             0        0       0       0              0\n",
      "159552      0             0        0       0       0              0\n",
      "159553      0             0        0       0       0              0\n",
      "159555      0             0        0       0       0              0\n",
      "159556      0             0        0       0       0              0\n",
      "159558      0             0        0       0       0              0\n",
      "159560      0             0        0       0       0              0\n",
      "159561      0             0        0       0       0              0\n",
      "159562      0             0        0       0       0              0\n",
      "159566      0             0        0       0       0              0\n",
      "159570      0             0        0       0       0              0\n",
      "\n",
      "[111906 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/burgew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Text preprocessor using NLTK tokenization and Lemmatization\n",
    "\n",
    "    This class is to be used in an sklean Pipeline, prior to other processers like PCA/LSA/classification\n",
    "    Attributes:\n",
    "        lower: A boolean indicating whether text should be lowercased by preprocessor\n",
    "                default: True\n",
    "        strip: A boolean indicating whether text should be stripped of surrounding whitespace, underscores and '*'\n",
    "                default: True\n",
    "        stopwords: A set of words to be used as stop words and thus ignored during tokenization\n",
    "                default: built-in English stop words\n",
    "        punct: A set of punctuation characters that should be ignored\n",
    "                default: None\n",
    "        lemmatizer: An object that should be used to lemmatize tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        \"\"\"Initialize method for NLTKPreprocessor instance\n",
    "\n",
    "        Simple initialization of specified instance variables:\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            stopwords: set of words to ignore as stop words, or a default set for English will be used\n",
    "            punct: set of punctuation characters to strip, or a default set will be used\n",
    "            lower: indicator of whether to convert all characters to lowercase, defaults to True\n",
    "            strip: indicator of whether to strip whitespace, defaults to True\n",
    "\n",
    "        Returns:\n",
    "            N/A: instance initializer\n",
    "\n",
    "        \"\"\"\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit model with X and optional y\n",
    "\n",
    "        This function does nothing but return self, since as a processor in the sklearn Pipeline this preprocessor\n",
    "        has nothing analogous to \"fit\" logic. The tokenization logic is independent of specific dataset training, \n",
    "        and is fully realized in the transform() function. \n",
    "        This function exists as implementation of sklearn.BaseEstimator, for use in Pipeline.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X (array-like): independent variable\n",
    "            y (array-like): dependent variable\n",
    "            \n",
    "        Returns:\n",
    "            NLTKPreprocessor: self\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Function exists as implementation of sklearn.BaseEstimator, for use in Pipeline.\n",
    "        This is simply for complying with interface.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X (array-like): input documents\n",
    "            \n",
    "        Returns:\n",
    "            string: joined documents\n",
    "        \"\"\"\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform input X to produce output to be processed by next element in sklearn Pipeline\n",
    "\n",
    "        This triggers the tokenization/lemmatization of the source documents.\n",
    "        This is invoked by the sklearn Pipeline.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X: input documents to be tokenized\n",
    "            \n",
    "        Returns:\n",
    "            list: tokenized documents reduced to simplest lemma form\n",
    "        \"\"\"\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    \n",
    "    def tokenize(self, document):\n",
    "        \"\"\"Tokenize an input document, converting from a block of text into sentences, into tagged tokens,\n",
    "        generating a set of lemmas.\n",
    "\n",
    "        This method does the preprocessing work of sentence-based tokenization and then reduces words to lemmas\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X (array-like): independent variable\n",
    "            y (array-like): dependent variable\n",
    "            \n",
    "        Returns:\n",
    "            Iterator[str]: an iterator over the tokens produced from the input documents\n",
    "        \"\"\"\n",
    "        # Break the document into sentences. This is necessary for part-of-speech tagging.\n",
    "        for sent in sent_tokenize(unicode(document,'utf-8')):\n",
    "\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "                \n",
    "    def lemmatize(self, token, tag):\n",
    "        \"\"\"Convert a token into the appropriate lemma\n",
    "\n",
    "        Method uses the NLTK WordNetLemmatizer for part-of-speech tag-based lemmatization of words.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            token: input word\n",
    "            tag: part-of-speech tag\n",
    "            \n",
    "        Returns:\n",
    "            string: lemma\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\" Simple identity function works as a passthrough.\n",
    "\n",
    "        This function will be used with the Vectorizer classes, when tokenization will have been performed already.\n",
    "        In this scenario, the Vectorizer class will call this function in the place of its normal tokenization feature\n",
    "        and this function will simply return the input token.\n",
    "        \n",
    "        Args:\n",
    "            token (string): text token being evaluated by CountVectorizer or TfidfVectorizer\n",
    "            \n",
    "        Returns:\n",
    "            string: input token unchanged (processed earlier by NLTK) will tbe returned\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Text Preprocessing - training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "This block uses the NLTKPreprocessor to tokenize the input data and then the TfidfVectorizer to vectorize it. The NLTKPreprocessor will ignore English stop words and will lemmatize where possible. The vectorizer ignores words occuring in fewer than 5 documents, which sufficed to reduce the size of the words vector significantly. Also, the vectorizer will limit the total features (words) to 15000, prioritizing the most valuable ones with highest TF-IDF score.\n",
    "\n",
    "Note that in this case the tokenization available by default in TfidfVectorizer is disabled, since that is handled by the NLTKPreprocessor. This made it clear that tokenization is by far more expensive (time) than vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vectorization of training data...\n",
      "Completed vectorization of training data in 21.59 seconds\n",
      "Starting vectorization of dev data...\n",
      "Completed vectorization of dev data in 3.38 seconds\n",
      "\n",
      "Vocabulary (tfidf) size is: 1000\n",
      "Sample vocabulary from TfidfVectorizer:\n",
      "     count\n",
      "0        0\n",
      "000      1\n",
      "1        2\n",
      "10       3\n",
      "100      4\n",
      "11       5\n",
      "12       6\n",
      "13       7\n",
      "14       8\n",
      "15       9\n",
      "None\n",
      "...\n",
      "      count\n",
      "year    990\n",
      "yes     991\n",
      "yet     992\n",
      "·       993\n",
      "–       994\n",
      "—       995\n",
      "’       996\n",
      "“       997\n",
      "”       998\n",
      "•       999\n",
      "None\n",
      "Number of nonzero entries in matrix: 1976418\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    counts\n",
       "6        4\n",
       "12       1\n",
       "16       1\n",
       "42       4\n",
       "43       3\n",
       "44       1\n",
       "51       2\n",
       "55       4\n",
       "56       3\n",
       "58       2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "np.random.seed(455)\n",
    "\n",
    "# This preprocessor will be used to process data prior to vectorization\n",
    "nltkPreprocessor = NLTKPreprocessor()\n",
    "    \n",
    "# Note that this vectorizer is created with a passthru tokenizer(identity), no preprocessor and no lowercasing\n",
    "# This is to account for the NLTKPreprocessor already taking care of tokenization.\n",
    "tfidfVector = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=.7, max_features=6000,\n",
    "                              tokenizer=identity, preprocessor=None, lowercase=False, stop_words={'english'})\n",
    "\n",
    "# Check if there is a serialized copy of the preprocessed training data, and if not the perform text preprocessing and\n",
    "# save the serialized result for reuse.\n",
    "pickle_file_name = 'train_preproc_data.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    print \"Starting preprocessing of training data...\"\n",
    "    start_train_preproc = time.time()\n",
    "    nltkPreprocessor.fit(train_data)\n",
    "    train_preproc_data = nltkPreprocessor.transform(train_data)\n",
    "    finish_train_preproc = time.time()\n",
    "    print \"Completed tokenization/preprocessing of training data in {:.2f} seconds\".format(finish_train_preproc-start_train_preproc)\n",
    "    \n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(train_preproc_data,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        train_preproc_data = pickle.load(pickle_file)\n",
    "\n",
    "# Check if there is a serialized copy of the vectorized counts, and if not regenerate the matrix and save the\n",
    "# serialized result for reuse.        \n",
    "pickle_file_name = 'train_tfidf_counts.6000.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    \n",
    "    # Generating new TF-IDF train counts means we need to then re-apply LSA to the results, so remove the LSA results\n",
    "    #remove_file('lsa_train_counts.pickle')\n",
    "    print \"Starting vectorization of training data...\"\n",
    "    start_train_vectors = time.time()\n",
    "    train_tfidf_counts = tfidfVector.fit_transform(train_preproc_data)\n",
    "    finish_train_vectors = time.time()\n",
    "    print \"Completed vectorization of training data in {:.2f} seconds\".format(finish_train_vectors-start_train_vectors)\n",
    "    \n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(train_tfidf_counts,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        train_tfidf_counts = pickle.load(pickle_file)\n",
    "    \n",
    "# Check if there is a serialized copy of the preprocessed dev data, and if not the perform text preprocessing and\n",
    "# save the serialized result for reuse.\n",
    "pickle_file_name = 'dev_preproc_data.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    print \"\\nStarting preprocessing of dev data...\"\n",
    "    start_dev_preproc = time.time()\n",
    "    nltkPreprocessor.fit(dev_data)\n",
    "    dev_preproc_data = nltkPreprocessor.transform(dev_data)\n",
    "    finish_dev_preproc = time.time()\n",
    "    print \"Completed tokenization/preprocessing of dev data in {:.2f} seconds\".format(finish_dev_preproc-start_dev_preproc)\n",
    "\n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(dev_preproc_data,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        dev_preproc_data = pickle.load(pickle_file)\n",
    "    \n",
    "pickle_file_name = 'dev_tfidf_counts.6000.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    \n",
    "    \n",
    "    # Generating new TF-IDF dev counts means we need to then re-apply LSA to the results, so remove the LSA results\n",
    "    #remove_file('lsa_dev_counts.pickle')\n",
    "    \n",
    "    print \"Starting vectorization of dev data...\"\n",
    "    start_dev_vectors = time.time()\n",
    "    dev_tfidf_counts = tfidfVector.transform(dev_preproc_data)\n",
    "    finish_dev_vectors = time.time()\n",
    "    print \"Completed vectorization of dev data in {:.2f} seconds\".format(finish_dev_vectors-start_dev_vectors)\n",
    "\n",
    "\n",
    "    print(\"\\nVocabulary (tfidf) size is: {}\").format(len(tfidfVector.vocabulary_))\n",
    "    vocab_entries = {k: tfidfVector.vocabulary_[k] for k in tfidfVector.vocabulary_.keys()}\n",
    "    vocab_entries = pd.Series(vocab_entries).to_frame()\n",
    "    vocab_entries.columns = ['count']\n",
    "    vocab_entries = vocab_entries.sort_values(by='count')\n",
    "\n",
    "    print(\"Sample vocabulary from TfidfVectorizer:\")\n",
    "    print(pp.pprint(vocab_entries.head(10)))\n",
    "    print(\"...\")\n",
    "    print(pp.pprint(vocab_entries.tail(10)))\n",
    "    print(\"Number of nonzero entries in matrix: {}\").format(train_tfidf_counts.nnz)\n",
    "\n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(dev_tfidf_counts,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        dev_tfidf_counts = pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "# Print sample column wise sum, we can see that an observation can have multiple classes.\n",
    "count_df = pd.DataFrame(train_labels.apply(np.sum,1), columns = [\"counts\"])\n",
    "count_df = count_df[((count_df[\"counts\"] >= 1))]\n",
    "count_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final LSA Feature Selection - training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA/LSA\n",
    "Principal Component Analysis (PCA) and Latent Semantic Analysis (LSA) are both operations that use Singular Value Decomposition to reduce the dimensionality of a dataset. PCA is applied to a term-covariance matrix, whereas LSA is applied to a term-document matrix. As such, LSA is appropriate for machine learning algorithms using scikit-learn TfidfVectorizer. Additionally PCA, as implemented in scikit-learn, cannot handle the sparse matrices that are produced by such vectorization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSA on train counts with 500 components...\n",
      "Train counts transform took 0.20 minutes.\n",
      "Starting LSA on dev counts with 500 components...\n",
      "Dev counts transform took 0.11 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Set the number of principal components to identify for use in classification processes\n",
    "target_components = 3000\n",
    "\n",
    "# Check if there is a serialized copy of the Principal Components data for the training dataset, and if not then\n",
    "# perform LSA processing and save the serialized result for reuse.\n",
    "pickle_file_name = 'lsa_train_counts.3000.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    svd = TruncatedSVD(n_components=target_components, algorithm='arpack')\n",
    "    print \"Starting LSA on train counts with {} components...\".format(target_components)\n",
    "    train_start=time.time()\n",
    "    lsa_train_counts = svd.fit_transform(train_tfidf_counts)\n",
    "    train_stop=time.time()\n",
    "    print \"Train counts transform took {:.2f} minutes.\".format((train_stop-train_start)/60)\n",
    "    \n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(lsa_train_counts,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        lsa_train_counts = pickle.load(pickle_file)\n",
    " \n",
    "# Check if there is a serialized copy of the Principal Components data for the dev dataset, and if not then\n",
    "# perform LSA processing and save the serialized result for reuse.\n",
    "pickle_file_name = 'lsa_dev_counts.3000.pickle'\n",
    "if (not os.path.exists(pickle_file_name)):\n",
    "    print \"Starting LSA on dev counts with {} components...\".format(target_components)\n",
    "    dev_start=time.time()\n",
    "    lsa_dev_counts = svd.fit_transform(dev_tfidf_counts)\n",
    "    dev_stop=time.time()\n",
    "    print \"Dev counts transform took {:.2f} minutes.\".format((dev_stop-dev_start)/60)\n",
    "    \n",
    "    with open(pickle_file_name,'w') as pickle_file:\n",
    "        pickle.dump(lsa_dev_counts,pickle_file)\n",
    "else:\n",
    "    # If the serialized file already exists, simply load it for the next step of the process.\n",
    "    with open(pickle_file_name,'r') as pickle_file:\n",
    "        lsa_dev_counts = pickle.load(pickle_file)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final MLPClassifier Training and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification with Neural Net (sklearn.MLPClassifier)\n",
    "In choosing a neural net model for text classification, the output layer should have the same number of nodes as the number of classification labels. In this case, there are 6 labels and as such not only will the output layer have 6 nodes, but the final hidden layer as well. The input layer will have the same number of nodes as features, normally, and ideally the initial hidden layer will be between that and the number of classes.\n",
    "\n",
    "In this case, we're limiting our feature set to 5,000 principal components, and it was not possible to use a number of initial hidden layer nodes at all close to that, running this process on a Macbook. So, setting the initial hidden layer to 12 gave at least some benefit of being less than the number of features and greater than the number of output classes. This (12,6) model is the one that ended up producing best (most accurate) results.\n",
    "\n",
    "Note that, nod toward deeper learning, a (10,8,6) model was also tested, but this ended up demonstrating overfitting, with a signficantly higher accuracy score on test data than on dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GridSearchCV AUC score using training data: 0.941247516302\n",
      "Training/scoring for train data completed, after 1.00 minutes.\n",
      "ROC AUC score from dev predict: 0.941247516302\n",
      "Accuracy score from dev predict: 0.878443302213\n",
      "Precision score from dev predict: 0.487031745423\n",
      "Recall score from dev predict: 0.158435438266\n",
      "Shape of predict_probs: (47665, 6)\n",
      "      toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
      "0  0.139470      0.007826  0.058516  0.003474  0.053242       0.009309\n",
      "1  0.037722      0.003636  0.017706  0.002312  0.019433       0.005263\n",
      "2  0.145024      0.008020  0.060742  0.003519  0.054953       0.009480\n",
      "3  0.009268      0.001673  0.005140  0.001532  0.006868       0.002955\n",
      "4  0.044411      0.003986  0.020482  0.002427  0.021965       0.005637\n",
      "5  0.006157      0.001338  0.003594  0.001361  0.005082       0.002502\n",
      "6  0.000381      0.000300  0.000328  0.000612  0.000676       0.000812\n",
      "7  0.294801      0.013023  0.124354  0.004557  0.101382       0.013599\n",
      "8  0.341348      0.014612  0.146237  0.004847  0.116699       0.014816\n",
      "9  0.154105      0.008334  0.064394  0.003592  0.057743       0.009756\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "# This MLPClassifier will be fit using training data and subsequently used to predict labels using dev\n",
    "# data, for scoring.\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(12,6), solver='adam', early_stopping=False, activation='relu',\n",
    "                           tol=1e-13, alpha=1, learning_rate='adaptive', learning_rate_init=0.01)\n",
    "\n",
    "# Fit using the training data and time the process for reference\n",
    "# This uses GridSearchCV to identify the best ROC AUC score.\n",
    "full_train_start = time.time()\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid={}, scoring='roc_auc')\n",
    "grid_search.fit(lsa_train_counts, train_labels)\n",
    "full_train_stop = time.time()\n",
    "\n",
    "print(\"Best GridSearchCV AUC score using training data: \" + str(grid_search.best_score_))\n",
    "duration = (full_train_stop-full_train_start)/60\n",
    "print('Training/scoring for train data completed, after {:.2f} minutes.'.format(duration))\n",
    "\n",
    "# Generate predictions using the dev LSA data and collect a series of scores\n",
    "dev_pred = grid_search.best_estimator_.predict(lsa_dev_counts)\n",
    "roc_auc = grid_search.best_score_\n",
    "\n",
    "# The following ROC AUC score is computed based on the dev data preduction, and will be different from that computed\n",
    "# by the grid search\n",
    "acc_score = metrics.accuracy_score(dev_labels, dev_pred)\n",
    "\n",
    "# Note that, since this is multilabel data, an F1 score must be evaluated with either results weighted across labels or\n",
    "# as samples taken from each.\n",
    "precision = metrics.precision_score(dev_labels, dev_pred, average='weighted')\n",
    "recall = metrics.recall_score(dev_labels, dev_pred, average='weighted')\n",
    "\n",
    "print(\"ROC AUC score from dev predict: {}\".format(roc_auc))\n",
    "print(\"Accuracy score from dev predict: {}\".format(acc_score))\n",
    "print(\"Precision score from dev predict: {}\".format(precision))\n",
    "print(\"Recall score from dev predict: {}\".format(recall))\n",
    "\n",
    "predict_probs = grid_search.best_estimator_.predict_proba(lsa_dev_counts)\n",
    "print(\"Shape of predict_probs: {}\".format(predict_probs.shape))\n",
    "\n",
    "prediction_submission = pd.DataFrame(data=predict_probs,columns=target_names)\n",
    "print(prediction_submission[0:10]) # print frame output \n",
    "prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For comparison, in the following loop a new classifier will be trained with each individual label and after each\n",
    "# fit operation will be used to predict using test data. The predict_proba call produces probabilities that can be\n",
    "# used as input to ensembles.\n",
    "\n",
    "for name in target_names:\n",
    "    \n",
    "    label_train_start = time.time()\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(12,6), solver='adam', early_stopping=False, activation='relu',\n",
    "                               tol=1e-13, alpha=1, learning_rate='adaptive', learning_rate_init=0.01)    \n",
    "    classifier.fit(lsa_train_counts, train_labels[name])\n",
    "    label_train_stop = time.time()\n",
    "    print('Training for class {} completed, after {:.2f} minutes.'.format(name, \n",
    "                                                                          (label_train_stop-label_train_start)/60))\n",
    "    \n",
    "    # Generate predictions using the dev LSA data and time the operation for reference\n",
    "    label_predict_start = time.time()\n",
    "    dev_pred = classifier.predict(lsa_dev_counts)\n",
    "    label_predict_stop = time.time()\n",
    "    print('Prediction for class {} completed, after {:.2f} minutes.'.format(name,\n",
    "                                                                    (label_predict_stop-label_predict_start)/60))\n",
    "    \n",
    "    # Collect the same scores as evaluated above across all labels\n",
    "    dev_pred = classifier.predict(lsa_dev_counts)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dev_labels[name], dev_pred)\n",
    "\n",
    "\n",
    "    auc = metrics.auc(fpr,tpr)\n",
    "    acc_score = metrics.accuracy_score(dev_labels[name], dev_pred)\n",
    "    f_one_score = metrics.f1_score(dev_labels[name], dev_pred,average='micro')\n",
    "        \n",
    "    print(\"Classifier trained with label {} used for following scores:\".format(name))\n",
    "    print(\"AUC Score from dev predict: {}\".format(auc))\n",
    "    print(\"Accuracy Score from dev predict: {}\".format(acc_score))\n",
    "    print(\"F1 score from dev predict: {}\".format(f_one_score))\n",
    "\n",
    "    prediction_submission = classifier.predict_proba(lsa_dev_counts)\n",
    "    print(prediction_submission[0:9,:]) # print frame output \n",
    "    break\n",
    "    \n",
    "#print(prediction_submission[0:9,:] # print frame output \n",
    "#prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission - based on test preprocessing, LSA feature selection and MLPClassifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_submission = pd.DataFrame()\n",
    "prediction_submission[\"id\"] = test_df[\"id\"]\n",
    "\n",
    "# new vector object for all train data for submission\n",
    "finalTrainVector = CountVectorizer()\n",
    "finalTrainCount = finalTrainVector.fit_transform(train_df[\"comment_text\"])\n",
    "\n",
    "# TODO: Using pipelines can clean up repetitive processes\n",
    "# test set up\n",
    "#testVector = CountVectorizer()\n",
    "testCount = finalTrainVector.transform(test_df[\"comment_text\"])\n",
    "\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver='sag') #sag is one kind of solver optimize for multi-label\n",
    "    clf = classifier.fit(finalTrainCount, train_df[name])\n",
    "    prediction_submission[name] = clf.predict_proba(testCount)[:, 1]\n",
    "    #print(prediction_submission)\n",
    "\n",
    "    \n",
    "print(prediction_submission.head(10)) # print frame output \n",
    "prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_submission = pd.DataFrame()\n",
    "prediction_submission[\"id\"] = test_df[\"id\"]\n",
    "\n",
    "# new vector object for all train data for submission\n",
    "finalTrainVector = CountVectorizer()\n",
    "finalTrainCount = finalTrainVector.fit_transform(train_df[\"comment_text\"])\n",
    "\n",
    "# TODO: Using pipelines can clean up repetitive processes\n",
    "# test set up\n",
    "#testVector = CountVectorizer()\n",
    "testCount = finalTrainVector.transform(test_df[\"comment_text\"])\n",
    "\n",
    "for name in target_names:\n",
    "    classifier = LogisticRegression(solver='sag') #sag is one kind of solver optimize for multi-label\n",
    "    clf = classifier.fit(finalTrainCount, train_df[name])\n",
    "    prediction_submission[name] = clf.predict_proba(testCount)[:, 1]\n",
    "    #print(prediction_submission)\n",
    "\n",
    "    \n",
    "print(prediction_submission.head(10)) # print frame output \n",
    "prediction_submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frame contains the output for each class and is saved in a pandas data frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
