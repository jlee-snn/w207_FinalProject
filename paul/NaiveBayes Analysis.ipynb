{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of things were tested for this model\n",
    "\n",
    "* A variety of different parameters to countVectorizer and tfidfVectorizor including\n",
    "  * a number of different preprocessing steps\n",
    "    * regular expressions to remove some oddities\n",
    "    * an attempt to correct spellings and conjoined words\n",
    "  * ngrams from (1,10) - code not here as it did not have any success\n",
    "* Use of GridSearchCV to find the optimal alpha for each classifier\n",
    "  \n",
    "Interesting observations\n",
    "\n",
    "* Spell checking can be very intensive and so care has to be given to it.  The inital attempt\n",
    "was very slow and needed to be optimized.  It went from 10 minutes per 1000 messages (in our\n",
    "data set) to 3.5 minutes with some rearrangment.  Even with that, the total time on my laptop\n",
    "for a single use of it was 8 hours for transforming dev and training sets.\n",
    "* There was no single good model or set of parameters that worked best for for all labels in the development\n",
    "set.  The choice of scoring metric would determine which was best.  When we look at AUC Bernoulli Naive \n",
    "Bayes scores best, however Multinomial give the highest F1 scores.\n",
    "* Preprocessing was ineffective.  We suspect though that this may simply have been the choice of preprocessing\n",
    "style and that further effort may have yielded gains.\n",
    "\n",
    "Results:  \n",
    "Best scores for AUC:\n",
    "\n",
    "<table>\n",
    "<tr><th> label </th><th> model </th><th> alpha </th><th> type </th><th> preprocessor </th><th> tokenizer </th><th> max_features </th><th> stop_words </th><th> lowercase </th><th> strip_accents </th><th> aucdev</th></tr>\n",
    "<tr><td> toxic </td><td> bern </td><td> 0.5 </td><td> count </td><td> 0 </td><td> 0 </td><td>  </td><td> english </td><td> TRUE </td><td> unicode </td><td> 0.852321727</td></tr>\n",
    "<tr><td> severe_toxic </td><td> bern </td><td> 2.0 </td><td> count </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.942104059</td></tr>\n",
    "<tr><td> obscene </td><td> bern </td><td> 10.0 </td><td> count </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.893736805</td></tr>\n",
    "<tr><td> threat </td><td> bern </td><td> 0.5 </td><td> count </td><td> 0 </td><td> 0 </td><td> 5000 </td><td> english </td><td> TRUE </td><td>  </td><td> 0.838735331</td></tr>\n",
    "<tr><td> insult </td><td> bern </td><td> 10.0 </td><td> count </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.878101445</td></tr>\n",
    "<tr><td> identity_hate </td><td> bern </td><td> 2.0 </td><td> count </td><td> 0 </td><td> 0 </td><td> 6000 </td><td>  </td><td> TRUE </td><td> ascii </td><td> 0.82812169</td></tr>\n",
    "<tr><td> </td></tr>\n",
    "<tr><td> Average </td><td> &nbsp; </td><td> &nbsp;</td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td>  </td><td> &nbsp; </td><td> &nbsp; </td><td> 0.872187</td></tr>\n",
    "<tr><td> </td></tr>\n",
    "</table>\n",
    "\n",
    "Best scores for F1:\n",
    "<table>\n",
    "<tr><th> label </th><th> model </th><th> alpha </th><th> type </th><th> preprocessor </th><th> tokenizer </th><th> max_features </th><th> stop_words </th><th> lowercase </th><th> strip_accents </th><th> f1dev </th></tr>\n",
    "<tr><td> toxic </td><td> multi </td><td> 0.1 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 6000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.950819672 </td></tr>\n",
    "<tr><td> severe_toxic </td><td> multi </td><td> 0.5 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 10000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.990763086 </td></tr>\n",
    "<tr><td> obscene </td><td> multi </td><td> 0.5 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.9735999 </td></tr>\n",
    "<tr><td> threat </td><td> multi </td><td> 0.1 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td>  </td><td> 0.996879421 </td></tr>\n",
    "<tr><td> insult </td><td> multi </td><td> 0.1 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 5000 </td><td>  </td><td> TRUE </td><td> ascii </td><td> 0.969231089 </td></tr>\n",
    "<tr><td> identity_hate </td><td> multi </td><td> 0.5 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 6000 </td><td> english </td><td> TRUE </td><td>\n",
    " ascii </td><td> 0.991303986 </td></tr>\n",
    "<tr><td> Average </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> 0.978766 </td></tr>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 159571\n",
      "training data shape: (111503,)\n",
      "training label shape: (111503, 6)\n",
      "dev label shape: (48068, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "print('total training observations:', train_df.shape[0])\n",
    "print('training data shape:', train_data.shape)\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print('labels names:', target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports etc. used in this analysis\n",
    "import string\n",
    "import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from enchant import DictWithPWL\n",
    "from enchant.checker import SpellChecker\n",
    "import difflib\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "punctuation = \"[\\!\\?\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\,\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\\']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://norvig.com/spell-correct.html\n",
    "# This is the Norvig spell checker and requires the storage of a \"big.txt\"\n",
    "# file with a corpus of words that it uses for predictions\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../data/big.txt').read()))\n",
    "\n",
    "def norvig_P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def norvig_correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(norvig_candidates(word), key=norvig_P)\n",
    "\n",
    "def norvig_candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My own spell checking and corrections, using a combination of Norvig\n",
    "# and pyenchant for spelling and a customer word splitter that uses\n",
    "# both to verify words\n",
    "\n",
    "## Custom Dictionary for pyenchant in ./data/mywords.txt.  There are many missing words\n",
    "# mywords.txt currently contains:\n",
    "# - list of firstnames and surnames gathered from several internet searches\n",
    "#         http://www.birkenhoerdt.net/surnames-all.php?tree=1\n",
    "# - List of swear words from: https://www.noswearing.com/dictionary\n",
    "# - Custom entries that were flagged as misspelled but are known\n",
    "\n",
    "my_dict=DictWithPWL('en_US', \"../data/mywords.txt\")\n",
    "my_checker = SpellChecker(my_dict)\n",
    "\n",
    "\n",
    "def trysplit(word):\n",
    "    \"\"\"This function looks for between 2 and 4 words which have been conjoined\n",
    "    likethisforexample.  It uses pyenchant to recognize the words and then the\n",
    "    probabilities assigned by Norvig, and returns the highest combined probability\n",
    "    for the parsed block with all valid words\n",
    "    \n",
    "    Note: it also only accepts 'a' and 'i' as legitimate single letter words.  Various\n",
    "    dictionaries define all individual letters as nouns, however they are rarely used\n",
    "    in writing and if they are in conjoined words it will make them too difficult to\n",
    "    process.\n",
    "    \n",
    "    Also no spelling corrections are attempted here, if the words are both misspelled\n",
    "    and conjoined we give up:-)\n",
    "    \n",
    "    Args:\n",
    "        word (string) : A word that is suspected to be conjoined\n",
    "    Returns:\n",
    "        list of strings : A list of up to 4 valid subwords\n",
    "    \"\"\"\n",
    "    split_candidates = []\n",
    "    max_proba = 0.0\n",
    "    for i in range(1,len(word)):\n",
    "        # I will only allow single letters of 'a' and 'i', all others ignored.  Pyenchant allows for\n",
    "        # any single letter to be a legitimate word, and so too does norvig.  The dictionary defines\n",
    "        # them as nouns that represent the letter, however even though several can be used in slang\n",
    "        # (e.g. k->okay, c->see, u->you) using them in conjoined words would make the splitting far\n",
    "        # too difficult and also human understanding much more difficult #howucthisk, u c?\n",
    "        if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "            len(word[i:]) != 1 or (word[i:].lower() == 'a' or word[i:].lower() == 'i')):\n",
    "            if my_checker.check(word[:i]) and my_checker.check(word[i:]):\n",
    "                norvig_score = norvig_P(word[:i]) + norvig_P(word[i:])\n",
    "                if norvig_score > max_proba:\n",
    "                    max_proba = norvig_score\n",
    "                    split_candidates = [word[:i],word[i:]]\n",
    "                    \n",
    "    for i in range(1,len(word)):\n",
    "        for j in range(i+1,len(word)):        \n",
    "            if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "                len(word[i:j]) != 1 or (word[i:j].lower() == 'a' or word[i:j].lower() == 'i')) and (\n",
    "                len(word[i:]) != 1 or (word[i:].lower() == 'a' or word[i:].lower() == 'i')):\n",
    "                \n",
    "                if my_checker.check(word[:i]) and my_checker.check(word[i:j]) and my_checker.check(word[j:]):\n",
    "                    norvig_score = norvig_P(word[:i]) + norvig_P(word[i:j]) + norvig_P(word[j:])\n",
    "                    if norvig_score > max_proba:\n",
    "                        max_proba = norvig_score\n",
    "                        split_candidates = [word[:i],word[i:j],word[j:]]\n",
    "                        \n",
    "    for i in range(1,len(word)):\n",
    "        for j in range(i+1,len(word)):\n",
    "            for k in range(j+1,len(word)):\n",
    "                if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "                    len(word[i:j]) != 1 or (word[i:j].lower() == 'a' or word[i:j].lower() == 'i')) and (\n",
    "                    len(word[j:k]) != 1 or (word[j:k].lower() == 'a' or word[j:k].lower() == 'i')) and (\n",
    "                    len(word[k:]) != 1 or (word[k:].lower() == 'a' or word[k:].lower() == 'i')):\n",
    "                    if my_checker.check(word[:i]) and my_checker.check(word[i:j]) and my_checker.check(word[j:k]) and my_checker.check(word[k:]):\n",
    "                        norvig_score = norvig_P(word[:i]) + norvig_P(word[i:j]) + norvig_P(word[j:k]) + norvig_P(word[k:])\n",
    "                        if norvig_score > max_proba:\n",
    "                            max_proba = norvig_score\n",
    "                            split_candidates = [word[:i],word[i:j],word[j:k],word[k:]]\n",
    "                            \n",
    "    return split_candidates\n",
    "\n",
    "\n",
    "def get_best_candidates(word):\n",
    "    \"\"\" This function returns the highest probability candidate(s) for a\n",
    "    word using pyenchant\n",
    "    \n",
    "    Args:\n",
    "        word (string): single word that needs to be corrected\n",
    "    Returns:\n",
    "        list of equal probabilty spelling corrections\n",
    "    \"\"\"\n",
    "    best_words = []\n",
    "    best_ratio = 0\n",
    "    a = set(my_checker.suggest(word))\n",
    "    for b in a:\n",
    "        if not '-' in b:\n",
    "            tmp = difflib.SequenceMatcher(None, word, b).ratio()\n",
    "            if tmp > best_ratio:\n",
    "                best_words=[b]\n",
    "                best_ratio=tmp\n",
    "            elif tmp == best_ratio:\n",
    "                best_words.append(b)\n",
    "    return best_words\n",
    "\n",
    "    \n",
    "def fix_spellings(textinput):\n",
    "    \"\"\"This function takes the input text, parses it and then checks all words to correct\n",
    "    any misspellings or conjoined words\n",
    "    \n",
    "    Args:\n",
    "        block of text (string): a message to be split and checked for errors\n",
    "    Returns:\n",
    "        List of the split and corrected words\n",
    "    \"\"\"\n",
    "    words = textinput.split()\n",
    "    return_list = []\n",
    "    for word in words:\n",
    "        if my_checker.check(word) or my_checker.check(word.lower()) or word in punctuation or\\\n",
    "            any(i.isdigit() for i in word) or (word[-1].lower() == 's' and my_checker.check(word[:-1].lower())):\n",
    "            return_list.append(word)\n",
    "            # continue\n",
    "        else:            \n",
    "            candidates = get_best_candidates(word)\n",
    "            if len(candidates) == 1:\n",
    "                return_list.append(candidates.pop())\n",
    "            elif len(candidates) > 1:\n",
    "                # try another spell checker\n",
    "                nv_candidates = norvig_candidates(word)\n",
    "                tmp_set = set(nv_candidates).intersection(set(candidates))\n",
    "                if len(tmp_set) == 1:\n",
    "                    # only 1 overlap, should be correct\n",
    "                    return_list.append(tmp_set.pop())\n",
    "                elif len(nv_candidates) == 1 and next(iter(nv_candidates)) == word:\n",
    "                        # this is suspicious, pyenchants' \"suggest\" method always returns something, however if\n",
    "                        # norvigs method cannot find a suitable match within a short distance then it simply\n",
    "                        # returns the orignal word.  This section is for potentially conjoined words\n",
    "                        tmp_list=trysplit(word)\n",
    "\n",
    "                        # If we get back a list of split words then use these\n",
    "                        if len(tmp_list) != 0:\n",
    "                            return_list.extend(tmp_list)\n",
    "                            continue\n",
    "                else:\n",
    "                    # arbitrary now, just going to use the first one found from pyenchant, even though\n",
    "                    # I have seen norvig get the correct word sometimes when pyenchant gets it wrong\n",
    "                    return_list.append(candidates[0])\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions:\n",
    "\n",
    "def my_preprocessor_eng(textblock):\n",
    "    \"\"\" This function is a simple set of regular expressions to remove/replace some punctuation\n",
    "    and replace some abbreviations\n",
    "    \n",
    "    Args:\n",
    "        textbloc (string): a string of words to run the expressions against\n",
    "    Returns:\n",
    "        a string of adjusted text\n",
    "    \"\"\"\n",
    "    return_words = textblock\n",
    "    return_words = re.sub(r\"[^A-Za-z0-9]?!'`:´()\", \" \", return_words)\n",
    "    return_words = re.sub(r\",\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\.\\.+\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\.\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\(\",\" \", return_words)\n",
    "    return_words = re.sub(r\"\\)\",\" \", return_words)\n",
    "    return_works = re.sub(r\"\\;\", \" \", return_words)\n",
    "    return_words = re.sub(r\":\",\" \", return_words)\n",
    "    return_words = re.sub(r\"´\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"`\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"''+\", \"'\", return_words)\n",
    "    return_words = re.sub(r\" '\", \" \", return_words)\n",
    "    return_words = re.sub(r\"' \", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\\"\", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\/\", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\!\\!+\", \"!!\", return_words)\n",
    "    return_words = re.sub(r\"\\?\\?+\", \"?!\", return_words)\n",
    "    return_words = re.sub(r\"\\!\", \" !\", return_words)\n",
    "    return_words = re.sub(r\"\\?\", \" ?\", return_words)\n",
    "    return_words = re.sub(r\"\\b\\d+\\b\", \"999\", return_words)\n",
    "    # slang and abbreviations, need to be aware of capitolization and spaces\n",
    "    return_words = re.sub(r\"[Ww]on't\", \"will not\", return_words)\n",
    "    return_words = re.sub(r\"n't\", \" not\", return_words)\n",
    "    return_words = re.sub(r\"'s\\b\", \" is\", return_words)\n",
    "    return_words = re.sub(r\"\\b[Aa]bt\\b\", \"about\", return_words)\n",
    "    return return_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of scores on dev set and training set\n",
    "def score_f1_auc_on_train_dev(dev_vector, train_vector, name, ctype='multi'):\n",
    "    \"\"\"This function creates a Naive Bayes classifier with the input vectors\n",
    "    and then calculates both the AUC score and F1 score for the training and dev data.\n",
    "    \n",
    "    Args:\n",
    "        dev_vector: the processed vector of dev data\n",
    "        train_vector: the processed vector of training data\n",
    "        name (string) : the label name to test\n",
    "        ctype: multi, gaus or bern, choses between multinomial or bernoulli\n",
    "    Returns:\n",
    "        alpha: the best alpha value for this classifier\n",
    "        f1scoredev: the F1 score for dev\n",
    "        aucdev: the AUC score for dev\n",
    "        f1scoretrain: the F1 score for training\n",
    "        auctrain: the AUC score for training\n",
    "    \"\"\"\n",
    "    alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "\n",
    "    if ctype == 'multi':\n",
    "        nb_class = MultinomialNB().fit(train_vector, train_labels[name])\n",
    "    elif ctype == 'bern':\n",
    "        nb_class = BernoulliNB().fit(train_vector, train_labels[name])\n",
    "    elif ctype == 'gaus':\n",
    "        nb_class = GaussianNB().fit(train_vector, train_labels[name])\n",
    "    else:\n",
    "        print('ctype = %s, error' % (ctype))\n",
    "    \n",
    "    # use this to generate the best fitting model\n",
    "    clf = GridSearchCV(nb_class, param_grid = alphas)\n",
    "    clf.fit(train_vector, train_labels[name])\n",
    "    \n",
    "    predicted_labels_dev = clf.predict(dev_vector)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dev_labels[name], predicted_labels_dev)\n",
    "    \n",
    "    predicted_labels_train = clf.predict(train_vector)\n",
    "    fpr1, tpr1, thresholds1 = metrics.roc_curve(train_labels[name], predicted_labels_train)\n",
    "    \n",
    "    f1scoredev = metrics.f1_score(dev_labels[name],predicted_labels_dev,average='micro')\n",
    "    f1scoretrain = metrics.f1_score(train_labels[name],predicted_labels_train,average='micro')\n",
    "    \n",
    "    aucdev = metrics.auc(fpr,tpr)\n",
    "    auctrain = metrics.auc(fpr1,tpr1)\n",
    "    \n",
    "    return clf.best_params_, f1scoredev,aucdev,f1scoretrain,auctrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:56:07.267736\n",
      "18:56:07.268452: Doing i = None\n",
      "19:11:24.643492: Doing i = 1000\n",
      "19:24:12.875430: Doing i = 4000\n",
      "19:42:47.614669: Doing i = 5000\n",
      "20:09:50.301146: Doing i = 6000\n",
      "20:45:42.239538: Doing i = 10000\n",
      "21:48:07.579829\n",
      "(288, 9)\n"
     ]
    }
   ],
   "source": [
    "vectors_all=pd.DataFrame(columns=['vectortrain', 'vectordata','type','preprocessor', 'tokenizer',\n",
    "                                  'max_features', 'stop_words', 'lowercase', 'strip_accents' ])\n",
    "\n",
    "# this set of loops works through the chosen parameters creating a count and tfidf vectorizer\n",
    "# and using the preprocessor or not (4 per iteration).  These are stored in the vectors_all\n",
    "# dataframe along with a list of the parameters that were used to create each on\n",
    "print(str(datetime.datetime.now().time()))\n",
    "for i in None, 1000, 4000, 5000, 6000, 10000:\n",
    "    print('%s: Doing i = %s' %(str(datetime.datetime.now().time()), i))\n",
    "    for x in None, 'english':\n",
    "        for y in None, 'ascii', 'unicode':\n",
    "            for z in True, False:\n",
    "                vect = CountVectorizer(max_features=i, stop_words=x, strip_accents=y, lowercase=z)\n",
    "                vect_train = vect.fit_transform(train_data)\n",
    "                vect_dev = vect.transform(dev_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 0, 0, i, x, z, y]\n",
    "                # Same but with the preprocessor\n",
    "                vect = CountVectorizer(max_features=i, stop_words=x, \n",
    "                                strip_accents=y, lowercase=z, preprocessor=my_preprocessor_eng)\n",
    "                vect_train = vect.fit_transform(train_data)\n",
    "                vect_dev = vect.transform(dev_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 1, 0, i, x, z, y]\n",
    "                vect = TfidfVectorizer(max_features=i, stop_words=x, strip_accents=y, lowercase=z)\n",
    "                vect_train = vect.fit_transform(train_data)\n",
    "                vect_dev = vect.transform(dev_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 0, 0, i, x, z, y]\n",
    "                # Same but with the preprocessor\n",
    "                vect = TfidfVectorizer(max_features=i, stop_words=x, \n",
    "                                strip_accents=y, lowercase=z, preprocessor=my_preprocessor_eng)\n",
    "                vect_train = vect.fit_transform(train_data)\n",
    "                vect_dev = vect.transform(dev_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 1, 0, i, x, z, y]\n",
    "\n",
    "print(str(datetime.datetime.now().time()))\n",
    "print(vectors_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1auc_all_models (vectors_all):\n",
    "    \"\"\"This function takes a vector of type vectors_all (defined above) acts as a wrapper\n",
    "    to send each vector to the score_f1_auc_on_train_dev function selecting first multinomialNB\n",
    "    and after that Bernoulli.  It collects the resulting scores and the best alpha and stores\n",
    "    the results in a dataframe which is returned once all the results are calculated\n",
    "    \n",
    "    Args:\n",
    "        Vectors_all (datafram) : a datafram defined above that stores the vector data in each row\n",
    "    Returns\n",
    "        dataframe: A dataframe of all the resulting scores and the details for each model\n",
    "    \"\"\"\n",
    "    data_all=pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'f1dev','aucdev','f1train','auctrain'])\n",
    "    for index,row in vectors_all.iterrows():\n",
    "        for name in target_names:\n",
    "            alpha, tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(\n",
    "                train_vector=row['vectortrain'],\n",
    "                dev_vector=row[1],name=name, ctype='multi')\n",
    "            data_all.loc[data_all.shape[0]] = [index,name,'multi',alpha,row['type'], \n",
    "                                row['preprocessor'], row['tokenizer'], row['max_features'], \n",
    "                                row['stop_words'], row['lowercase'], row['strip_accents'],\n",
    "                                tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "            alpha, tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(\n",
    "                train_vector=row[0],\n",
    "                dev_vector=row[1],name=name, ctype='bern')\n",
    "            data_all.loc[data_all.shape[0]] = [index,name,'bern',alpha,row['type'], \n",
    "                                row['preprocessor'], row['tokenizer'], row['max_features'], \n",
    "                                row['stop_words'], row['lowercase'], row['strip_accents'],\n",
    "                                tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    print('done')\n",
    "    return data_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separated out.  This took over 24 hours to run as the fix_spellings tokenizer\n",
    "# seems to have been very slow.  As an alternative we will preprocess our data\n",
    "# before generating the dataframe\n",
    "\n",
    "print(str(datetime.datetime.now().time()))\n",
    "count_vect_plain_pre_token10k = CountVectorizer(tokenizer=fix_spellings, max_features=10000, \n",
    "                                                strip_accents='ascii', lowercase=True)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "X_train_counts_plain_pre_token10k = count_vect_plain_pre_token6k.fit_transform(train_data)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "X_dev_counts_plain_pre_token10k = count_vect_plain_pre_token6k.transform(dev_data)\n",
    "print(str(datetime.datetime.now().time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the vectors from the previous cell into the large dataframe of vectors\n",
    "vectors_all.loc[vectors_all.shape[0]] = [X_train_counts_plain_pre_token6k, \n",
    "                                         X_dev_counts_plain_pre_token6k,\n",
    "                                         'count', 0, 1, 10000, None, True, 'ascii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'top_auc_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-abca9fff89c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Print the top AUC and F1 Scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_auc_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'top_auc_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Finally pull everything together and look for the top results for\n",
    "# both F1 and AUC scores\n",
    "\n",
    "top_aucf1_results = pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'f1dev','aucdev','f1train','auctrain'])\n",
    "\n",
    "# calculate the f1 and auc for all the models\n",
    "resultdf = calculate_f1auc_all_models(vectors_all)\n",
    "\n",
    "# extract the top F1 and AUC scores from the calculations\n",
    "for label in target_names:\n",
    "    df_tmp = resultdf.loc[resultdf['label'] == label]\n",
    "    top_aucf1_results.loc[top_aucf1_results.shape[0]] = df_tmp.loc[df_tmp['aucdev'].idxmax()]\n",
    "    top_aucf1_results.loc[top_aucf1_results.shape[0]] = df_tmp.loc[df_tmp['f1dev'].idxmax()]\n",
    "\n",
    "# Print the top AUC and F1 Scores\n",
    "print(top_aucf1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    vectorno          label  model            alpha   type  preprocessor  \\\n",
      "0         40          toxic   bern   {'alpha': 0.5}  count             0   \n",
      "1        226          toxic  multi   {'alpha': 0.1}  tfidf             0   \n",
      "2        128   severe_toxic   bern   {'alpha': 2.0}  count             0   \n",
      "3        274   severe_toxic  multi   {'alpha': 0.5}  tfidf             0   \n",
      "4        128        obscene   bern  {'alpha': 10.0}  count             0   \n",
      "5        130        obscene  multi   {'alpha': 0.5}  tfidf             0   \n",
      "6        168         threat   bern   {'alpha': 0.5}  count             0   \n",
      "7        122         threat  multi   {'alpha': 0.1}  tfidf             0   \n",
      "8        128         insult   bern  {'alpha': 10.0}  count             0   \n",
      "9        154         insult  multi   {'alpha': 0.1}  tfidf             0   \n",
      "10       200  identity_hate   bern   {'alpha': 2.0}  count             0   \n",
      "11       226  identity_hate  multi   {'alpha': 0.5}  tfidf             0   \n",
      "\n",
      "    tokenizer max_features stop_words lowercase strip_accents     f1dev  \\\n",
      "0           0         None    english      True       unicode  0.899621   \n",
      "1           0         6000    english      True         ascii  0.950820   \n",
      "2           0         4000    english      True         ascii  0.950424   \n",
      "3           0        10000    english      True         ascii  0.990763   \n",
      "4           0         4000    english      True         ascii  0.938046   \n",
      "5           0         4000    english      True         ascii  0.973600   \n",
      "6           0         5000    english      True          None  0.977844   \n",
      "7           0         4000    english      True          None  0.996879   \n",
      "8           0         4000    english      True         ascii  0.933698   \n",
      "9           0         5000       None      True         ascii  0.969231   \n",
      "10          0         6000       None      True         ascii  0.903699   \n",
      "11          0         6000    english      True         ascii  0.991304   \n",
      "\n",
      "      aucdev   f1train  auctrain  \n",
      "0   0.852322  0.901204  0.892508  \n",
      "1   0.764679  0.953526  0.779729  \n",
      "2   0.942104  0.949221  0.934426  \n",
      "3   0.582934  0.991068  0.617740  \n",
      "4   0.893737  0.939804  0.900252  \n",
      "5   0.780474  0.974422  0.792376  \n",
      "6   0.838735  0.976458  0.916973  \n",
      "7   0.554372  0.997444  0.606999  \n",
      "8   0.878101  0.934692  0.880254  \n",
      "9   0.736152  0.970449  0.758221  \n",
      "10  0.828122  0.905402  0.884632  \n",
      "11  0.555632  0.992108  0.579837  \n"
     ]
    }
   ],
   "source": [
    "# Print the top AUC and F1 Scores\n",
    "print(top_aucf1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_aucf1_results).to_csv('f1auc_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    vectorno          label  model            alpha   type  preprocessor  \\\n",
      "0         40          toxic   bern   {'alpha': 0.5}  count             0   \n",
      "1        226          toxic  multi   {'alpha': 0.1}  tfidf             0   \n",
      "2        128   severe_toxic   bern   {'alpha': 2.0}  count             0   \n",
      "3        274   severe_toxic  multi   {'alpha': 0.5}  tfidf             0   \n",
      "4        128        obscene   bern  {'alpha': 10.0}  count             0   \n",
      "5        130        obscene  multi   {'alpha': 0.5}  tfidf             0   \n",
      "6        168         threat   bern   {'alpha': 0.5}  count             0   \n",
      "7        122         threat  multi   {'alpha': 0.1}  tfidf             0   \n",
      "8        128         insult   bern  {'alpha': 10.0}  count             0   \n",
      "9        154         insult  multi   {'alpha': 0.1}  tfidf             0   \n",
      "10       200  identity_hate   bern   {'alpha': 2.0}  count             0   \n",
      "11       226  identity_hate  multi   {'alpha': 0.5}  tfidf             0   \n",
      "\n",
      "    tokenizer max_features stop_words lowercase strip_accents     f1dev  \\\n",
      "0           0         None    english      True       unicode  0.899621   \n",
      "1           0         6000    english      True         ascii  0.950820   \n",
      "2           0         4000    english      True         ascii  0.950424   \n",
      "3           0        10000    english      True         ascii  0.990763   \n",
      "4           0         4000    english      True         ascii  0.938046   \n",
      "5           0         4000    english      True         ascii  0.973600   \n",
      "6           0         5000    english      True          None  0.977844   \n",
      "7           0         4000    english      True          None  0.996879   \n",
      "8           0         4000    english      True         ascii  0.933698   \n",
      "9           0         5000       None      True         ascii  0.969231   \n",
      "10          0         6000       None      True         ascii  0.903699   \n",
      "11          0         6000    english      True         ascii  0.991304   \n",
      "\n",
      "      aucdev   f1train  auctrain  \n",
      "0   0.852322  0.901204  0.892508  \n",
      "1   0.764679  0.953526  0.779729  \n",
      "2   0.942104  0.949221  0.934426  \n",
      "3   0.582934  0.991068  0.617740  \n",
      "4   0.893737  0.939804  0.900252  \n",
      "5   0.780474  0.974422  0.792376  \n",
      "6   0.838735  0.976458  0.916973  \n",
      "7   0.554372  0.997444  0.606999  \n",
      "8   0.878101  0.934692  0.880254  \n",
      "9   0.736152  0.970449  0.758221  \n",
      "10  0.828122  0.905402  0.884632  \n",
      "11  0.555632  0.992108  0.579837  \n"
     ]
    }
   ],
   "source": [
    "top_aucf1_results = pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'f1dev','aucdev','f1train','auctrain'])\n",
    "\n",
    "for label in target_names:\n",
    "    df_tmp = resultdf.loc[resultdf['label'] == label]\n",
    "    top_aucf1_results.loc[top_aucf1_results.shape[0]] = df_tmp.loc[df_tmp['aucdev'].idxmax()]\n",
    "    top_aucf1_results.loc[top_aucf1_results.shape[0]] = df_tmp.loc[df_tmp['f1dev'].idxmax()]\n",
    "    \n",
    "print(top_aucf1_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
