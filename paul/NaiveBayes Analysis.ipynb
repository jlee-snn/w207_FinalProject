{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of things were tested for this model\n",
    "\n",
    "* A variety of different parameters to countVectorizer and tfidfVectorizor including\n",
    "  * a number of different preprocessing steps\n",
    "  * Replacement of the tokenizer with something that will do spelling and error corrections\n",
    "  * ngrams from (1,10) - code not here as it did not have any success\n",
    "  \n",
    "Interesting observations\n",
    "\n",
    "* Spell checking can be very intensive and so care has to be given to it.  The inital attempt\n",
    "was very slow and needed to be optimized.  It went from 10 minutes per 1000 messages (in our\n",
    "data set) to 3.5 minutes with some rearrangment.  Even with that, the total time on my laptop\n",
    "for a single use of it was 8 hours for transforming dev and training sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 159571\n",
      "training data shape: (111853,)\n",
      "training label shape: (111853, 6)\n",
      "dev label shape: (47718, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "print('total training observations:', train_df.shape[0])\n",
    "print('training data shape:', train_data.shape)\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print('labels names:', target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports etc. used in this analysis\n",
    "import string\n",
    "import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from enchant import DictWithPWL\n",
    "from enchant.checker import SpellChecker\n",
    "import difflib\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "punctuation = \"[\\!\\?\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\,\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\\']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://norvig.com/spell-correct.html\n",
    "# This is the Norvig spell checker and requires the storage of a \"big.txt\"\n",
    "# file with a corpus of words that it uses for predictions\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../data/big.txt').read()))\n",
    "\n",
    "def norvig_P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def norvig_correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(norvig_candidates(word), key=norvig_P)\n",
    "\n",
    "def norvig_candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My own spell checking and corrections, using a combination of Norvig\n",
    "# and pyenchant for spelling and a customer word splitter that uses\n",
    "# both to verify words\n",
    "\n",
    "## Custom Dictionary for pyenchant in ./data/mywords.txt.  There are many missing words\n",
    "# mywords.txt currently contains:\n",
    "# - list of firstnames and surnames gathered from several internet searches\n",
    "#         http://www.birkenhoerdt.net/surnames-all.php?tree=1\n",
    "# - List of swear words from: https://www.noswearing.com/dictionary\n",
    "# - Custom entries that were flagged as misspelled but are known\n",
    "\n",
    "my_dict=DictWithPWL('en_US', \"../data/mywords.txt\")\n",
    "my_checker = SpellChecker(my_dict)\n",
    "\n",
    "\n",
    "def trysplit(word):\n",
    "    \"\"\"This function looks for between 2 and 4 words which have been conjoined\n",
    "    likethisforexample.  It uses pyenchant to recognize the words and then the\n",
    "    probabilities assigned by Norvig, and returns the highest combined probability\n",
    "    for the parsed block with all valid words\n",
    "    \n",
    "    Note: it also only accepts 'a' and 'i' as legitimate single letter words.  Various\n",
    "    dictionaries define all individual letters as nouns, however they are rarely used\n",
    "    in writing and if they are in conjoined words it will make them too difficult to\n",
    "    process.\n",
    "    \n",
    "    Also no spelling corrections are attempted here, if the words are both misspelled\n",
    "    and conjoined we give up:-)\n",
    "    \n",
    "    Args:\n",
    "        word (string) : A word that is suspected to be conjoined\n",
    "    Returns:\n",
    "        list of strings : A list of up to 4 valid subwords\n",
    "    \"\"\"\n",
    "    split_candidates = []\n",
    "    max_proba = 0.0\n",
    "    for i in range(1,len(word)):\n",
    "        # I will only allow single letters of 'a' and 'i', all others ignored.  Pyenchant allows for\n",
    "        # any single letter to be a legitimate word, and so too does norvig.  The dictionary defines\n",
    "        # them as nouns that represent the letter, however even though several can be used in slang\n",
    "        # (e.g. k->okay, c->see, u->you) using them in conjoined words would make the splitting far\n",
    "        # too difficult and also human understanding much more difficult #howucthisk, u c?\n",
    "        if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "            len(word[i:]) != 1 or (word[i:].lower() == 'a' or word[i:].lower() == 'i')):\n",
    "            if my_checker.check(word[:i]) and my_checker.check(word[i:]):\n",
    "                norvig_score = norvig_P(word[:i]) + norvig_P(word[i:])\n",
    "                if norvig_score > max_proba:\n",
    "                    max_proba = norvig_score\n",
    "                    split_candidates = [word[:i],word[i:]]\n",
    "                    \n",
    "    for i in range(1,len(word)):\n",
    "        for j in range(i+1,len(word)):        \n",
    "            if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "                len(word[i:j]) != 1 or (word[i:j].lower() == 'a' or word[i:j].lower() == 'i')) and (\n",
    "                len(word[i:]) != 1 or (word[i:].lower() == 'a' or word[i:].lower() == 'i')):\n",
    "                \n",
    "                if my_checker.check(word[:i]) and my_checker.check(word[i:j]) and my_checker.check(word[j:]):\n",
    "                    norvig_score = norvig_P(word[:i]) + norvig_P(word[i:j]) + norvig_P(word[j:])\n",
    "                    if norvig_score > max_proba:\n",
    "                        max_proba = norvig_score\n",
    "                        split_candidates = [word[:i],word[i:j],word[j:]]\n",
    "                        \n",
    "    for i in range(1,len(word)):\n",
    "        for j in range(i+1,len(word)):\n",
    "            for k in range(j+1,len(word)):\n",
    "                if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "                    len(word[i:j]) != 1 or (word[i:j].lower() == 'a' or word[i:j].lower() == 'i')) and (\n",
    "                    len(word[j:k]) != 1 or (word[j:k].lower() == 'a' or word[j:k].lower() == 'i')) and (\n",
    "                    len(word[k:]) != 1 or (word[k:].lower() == 'a' or word[k:].lower() == 'i')):\n",
    "                    if my_checker.check(word[:i]) and my_checker.check(word[i:j]) and my_checker.check(word[j:k]) and my_checker.check(word[k:]):\n",
    "                        norvig_score = norvig_P(word[:i]) + norvig_P(word[i:j]) + norvig_P(word[j:k]) + norvig_P(word[k:])\n",
    "                        if norvig_score > max_proba:\n",
    "                            max_proba = norvig_score\n",
    "                            split_candidates = [word[:i],word[i:j],word[j:k],word[k:]]\n",
    "                            \n",
    "    return split_candidates\n",
    "\n",
    "\n",
    "def get_best_candidates(word):\n",
    "    \"\"\" This function returns the highest probability candidate(s) for a\n",
    "    word using pyenchant\n",
    "    \n",
    "    Args:\n",
    "        word (string): single word that needs to be corrected\n",
    "    Returns:\n",
    "        list of equal probabilty spelling corrections\n",
    "    \"\"\"\n",
    "    best_words = []\n",
    "    best_ratio = 0\n",
    "    a = set(my_checker.suggest(word))\n",
    "    for b in a:\n",
    "        if not '-' in b:\n",
    "            tmp = difflib.SequenceMatcher(None, word, b).ratio()\n",
    "            if tmp > best_ratio:\n",
    "                best_words=[b]\n",
    "                best_ratio=tmp\n",
    "            elif tmp == best_ratio:\n",
    "                best_words.append(b)\n",
    "    return best_words\n",
    "\n",
    "    \n",
    "def fix_spellings(textinput):\n",
    "    \"\"\"This function takes the input text, parses it and then checks all words to correct\n",
    "    any misspellings or conjoined words\n",
    "    \n",
    "    Args:\n",
    "        block of text (string): a message to be split and checked for errors\n",
    "    Returns:\n",
    "        List of the split and corrected words\n",
    "    \"\"\"\n",
    "    words = textinput.split()\n",
    "    return_list = []\n",
    "    for word in words:\n",
    "        if my_checker.check(word) or my_checker.check(word.lower()) or word in punctuation or\\\n",
    "            any(i.isdigit() for i in word) or (word[-1].lower() == 's' and my_checker.check(word[:-1].lower())):\n",
    "            return_list.append(word)\n",
    "            # continue\n",
    "        else:            \n",
    "            candidates = get_best_candidates(word)\n",
    "            if len(candidates) == 1:\n",
    "                return_list.append(candidates.pop())\n",
    "            elif len(candidates) > 1:\n",
    "                # try another spell checker\n",
    "                nv_candidates = norvig_candidates(word)\n",
    "                tmp_set = set(nv_candidates).intersection(set(candidates))\n",
    "                if len(tmp_set) == 1:\n",
    "                    # only 1 overlap, should be correct\n",
    "                    return_list.append(tmp_set.pop())\n",
    "                elif len(nv_candidates) == 1 and next(iter(nv_candidates)) == word:\n",
    "                        # this is suspicious, pyenchants' \"suggest\" method always returns something, however if\n",
    "                        # norvigs method cannot find a suitable match within a short distance then it simply\n",
    "                        # returns the orignal word.  This section is for potentially conjoined words\n",
    "                        tmp_list=trysplit(word)\n",
    "\n",
    "                        # If we get back a list of split words then use these\n",
    "                        if len(tmp_list) != 0:\n",
    "                            return_list.extend(tmp_list)\n",
    "                            continue\n",
    "                else:\n",
    "                    # arbitrary now, just going to use the first one found from pyenchant, even though\n",
    "                    # I have seen norvig get the correct word sometimes when pyenchant gets it wrong\n",
    "                    return_list.append(candidates[0])\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions:\n",
    "\n",
    "def my_preprocessor_eng(textblock):\n",
    "    return_words = textblock\n",
    "    return_words = re.sub(r\"[^A-Za-z0-9]?!'`:´\", \" \", return_words)\n",
    "    return_words = re.sub(r\",\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\.\\.+\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\.\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\(\",\" \", return_words)\n",
    "    return_words = re.sub(r\"\\)\",\" \", return_words)\n",
    "    return_works = re.sub(r\"\\;\", \" \", return_words)\n",
    "    return_words = re.sub(r\":\",\" \", return_words)\n",
    "    return_words = re.sub(r\"´\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"`\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"''+\", \"'\", return_words)\n",
    "    return_words = re.sub(r\" '\", \" \", return_words)\n",
    "    return_words = re.sub(r\"' \", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\\"\", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\/\", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\!\\!+\", \"!!\", return_words)\n",
    "    return_words = re.sub(r\"\\?\\?+\", \"?!\", return_words)\n",
    "    return_words = re.sub(r\"\\!\", \" !\", return_words)\n",
    "    return_words = re.sub(r\"\\?\", \" ?\", return_words)\n",
    "    return_words = re.sub(r\"\\b\\d+\\b\", \"999\", return_words)\n",
    "    # slang and abbreviations, need to be aware of capitolization and spaces\n",
    "    return_words = re.sub(r\"[Ww]on't\", \"will not\", return_words)\n",
    "    return_words = re.sub(r\"n't\", \" not\", return_words)\n",
    "    return_words = re.sub(r\"'s\\b\", \" is\", return_words)\n",
    "    return_words = re.sub(r\"\\b[Aa]bt\\b\", \"about\", return_words)\n",
    "    return return_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of scores on dev set and training set\n",
    "def score_f1_auc_on_train_dev(dev_vector, train_vector, name, ctype='multi'):\n",
    "    \"\"\"This function creates a Naive Bayes classifier with the input vectors\n",
    "    and then calculates both the AUC score and F1 score for the training and dev data\n",
    "    \n",
    "    Args:\n",
    "        dev_vector: the processed vector of dev data\n",
    "        train_vector: the processed vector of training data\n",
    "        name (string) : the label name to test\n",
    "        ctype: multi, gaus or bern, choses between multinomial or bernoulli\n",
    "    Returns:\n",
    "        f1scoredev: the F1 score for dev\n",
    "        aucdev: the AUC score for dev\n",
    "        f1scoretrain: the F1 score for training\n",
    "        auctrain: the AUC score for training\n",
    "    \"\"\"\n",
    "    if ctype == 'multi':\n",
    "        nb_class = MultinomialNB().fit(train_vector, train_labels[name])\n",
    "    elif ctype == 'bern':\n",
    "        nb_class = BernoulliNB().fit(train_vector, train_labels[name])\n",
    "    elif ctype == 'gaus':\n",
    "        nb_class = GaussianNB().fit(train_vector, train_labels[name])\n",
    "    else:\n",
    "        print('ctype = %s, error' % (ctype))\n",
    "    \n",
    "    predicted_labels_dev = nb_class.predict(dev_vector)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dev_labels[name], predicted_labels_dev)\n",
    "    \n",
    "    predicted_labels_train = nb_class.predict(train_vector)\n",
    "    fpr1, tpr1, thresholds1 = metrics.roc_curve(train_labels[name], predicted_labels_train)\n",
    "    \n",
    "    f1scoredev = metrics.f1_score(dev_labels[name],predicted_labels_dev,average='micro')\n",
    "    f1scoretrain = metrics.f1_score(train_labels[name],predicted_labels_train,average='micro')\n",
    "    \n",
    "    aucdev = metrics.auc(fpr,tpr)\n",
    "    auctrain = metrics.auc(fpr1,tpr1)\n",
    "    \n",
    "    return f1scoredev,aucdev,f1scoretrain,auctrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:58:44.270299\n",
      "19:58:44.272527: Doing i = None\n"
     ]
    }
   ],
   "source": [
    "#scores_all=pd.DataFrame(columns=['vector', 'label', 'model', 'f1dev','aucdev','f1train','auctrain'])\n",
    "vectors_all=pd.DataFrame(columns=['vectortrain', 'vectordata','type','preprocessor', 'tokenizer',\n",
    "                                  'max_features', 'stop_words', 'lowercase', 'strip_accents' ])\n",
    "\n",
    "print(str(datetime.datetime.now().time()))\n",
    "for i in None, 1000, 4000, 5000, 6000, 10000:\n",
    "    print('%s: Doing i = %s' %(str(datetime.datetime.now().time()), i))\n",
    "    for x in None, 'english':\n",
    "        for y in None, 'ascii', 'unicode':\n",
    "            for z in True, False:\n",
    "                vect = CountVectorizer(max_features=i, stop_words=x, strip_accents=y, lowercase=True)\n",
    "                vect_train = vect.fit_transform(train_data)\n",
    "                vect_dev = vect.transform(dev_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 0, 0, i, x, z, y]\n",
    "                # Same but with the preprocessor\n",
    "                vect = CountVectorizer(max_features=i, stop_words=x, \n",
    "                                strip_accents=y, lowercase=z, preprocessor=my_preprocessor_eng)\n",
    "                vect_train = vect.fit_transform(train_data)\n",
    "                vect_dev = vect.transform(dev_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 1, 0, i, x, z, y]\n",
    "                vect = TfidfVectorizer(max_features=i, stop_words=x, strip_accents=y, lowercase=True)\n",
    "                vect_train = vect.fit_transform(train_data)\n",
    "                vect_dev = vect.transform(dev_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 0, 0, i, x, z, y]\n",
    "                # Same but with the preprocessor\n",
    "                vect = TfidfVectorizer(max_features=i, stop_words=x, \n",
    "                                strip_accents=y, lowercase=z, preprocessor=my_preprocessor_eng)\n",
    "                vect_train = vect.fit_transform(train_data)\n",
    "                vect_dev = vect.transform(dev_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 1, 0, i, x, z, y]\n",
    "\n",
    "print(str(datetime.datetime.now().time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_all=pd.DataFrame(columns=['vector', 'label', 'model', 'f1dev','aucdev','f1train','auctrain'])\n",
    "\n",
    "for index,row in vectors_all.iterrows():\n",
    "    print('%s: testint row %d' % (str(datetime.datetime.now().time()), index))\n",
    "    for name in target_names:\n",
    "        tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(\n",
    "            train_vector=row['vectortrain'],\n",
    "            dev_vector=row[1],name=name, ctype='multi')\n",
    "        scores_all.loc[scores_all.shape[0]] = [index,name,'multi',tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "        tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(\n",
    "            train_vector=row[0],\n",
    "            dev_vector=row[1],name=name, ctype='bern')\n",
    "        scores_all.loc[scores_all.shape[0]] = [index,name,'bern',tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_all=pd.DataFrame(columns=['vector', 'label', 'model', 'f1dev','aucdev','f1train','auctrain'])\n",
    "# vectors_all=pd.DataFrame(columns=['vectortrain', 'vectordata','preprocessor', 'tokenizer',\n",
    "#                                   'max_features', 'stop_words', 'lowercase', 'strip_accents' ])\n",
    "data_all=pd.DataFrame(columns=['label', 'model', 'type', 'preprocessor', 'tokenizer', 'max_features',\n",
    "                                'stop_words', 'lowercase', 'strip_accents', 'f1dev',\n",
    "                                'aucdev','f1train','auctrain'])\n",
    "for index,row in scores_all.iterrows():\n",
    "    df_row = vectors_all.loc[int(row['vector'])]\n",
    "    data_all.loc[data_all.shape[0]] = [row['label'], row['model'], df_row['type'], df_row['preprocessor'],\n",
    "                                        df_row['tokenizer'], df_row['max_features'], \n",
    "                                        df_row['stop_words'], df_row['lowercase'],\n",
    "                                      df_row['strip_accents'], row['f1dev'],row['aucdev'],\n",
    "                                       row['f1train'],row['auctrain']]\n",
    "\n",
    "pd.DataFrame(data_all).to_csv('results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label                toxic\n",
      "model                multi\n",
      "preprocessor             0\n",
      "tokenizer                0\n",
      "max_features         10000\n",
      "stop_words            None\n",
      "lowercase             True\n",
      "strip_accents        ascii\n",
      "f1dev            0.9397712\n",
      "aucdev           0.8491642\n",
      "f1train           0.943381\n",
      "auctrain         0.8659385\n",
      "Name: 1488, dtype: object\n",
      "label            severe_toxic\n",
      "model                    bern\n",
      "preprocessor                0\n",
      "tokenizer                   0\n",
      "max_features            10000\n",
      "stop_words            english\n",
      "lowercase                True\n",
      "strip_accents           ascii\n",
      "f1dev               0.9569974\n",
      "aucdev              0.9350653\n",
      "f1train             0.9568541\n",
      "auctrain            0.9431089\n",
      "Name: 1635, dtype: object\n",
      "label              obscene\n",
      "model                multi\n",
      "preprocessor             0\n",
      "tokenizer                0\n",
      "max_features         10000\n",
      "stop_words            None\n",
      "lowercase             True\n",
      "strip_accents      unicode\n",
      "f1dev            0.9599313\n",
      "aucdev           0.8713992\n",
      "f1train          0.9622272\n",
      "auctrain         0.8895068\n",
      "Name: 1540, dtype: object\n",
      "label               threat\n",
      "model                 bern\n",
      "preprocessor             0\n",
      "tokenizer                0\n",
      "max_features          4000\n",
      "stop_words            None\n",
      "lowercase             True\n",
      "strip_accents      unicode\n",
      "f1dev            0.9530575\n",
      "aucdev           0.8675296\n",
      "f1train          0.9522766\n",
      "auctrain         0.9257576\n",
      "Name: 679, dtype: object\n",
      "label               insult\n",
      "model                multi\n",
      "preprocessor             0\n",
      "tokenizer                0\n",
      "max_features         10000\n",
      "stop_words            None\n",
      "lowercase             True\n",
      "strip_accents         None\n",
      "f1dev            0.9552161\n",
      "aucdev            0.856307\n",
      "f1train          0.9580968\n",
      "auctrain          0.878623\n",
      "Name: 1448, dtype: object\n",
      "label            identity_hate\n",
      "model                     bern\n",
      "preprocessor                 0\n",
      "tokenizer                    0\n",
      "max_features              5000\n",
      "stop_words             english\n",
      "lowercase                 True\n",
      "strip_accents            ascii\n",
      "f1dev                0.9386605\n",
      "aucdev               0.8890465\n",
      "f1train              0.9394026\n",
      "auctrain             0.9228805\n",
      "Name: 1067, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for label in target_names:\n",
    "    df_tmp = data_all.loc[data_all['label'] == label]\n",
    "    print(df_tmp.loc[df_tmp['aucdev'].idxmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_plain_pre = CountVectorizer(preprocessor=my_preprocessor)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "X_train_counts_plain_pre = count_vect_plain_pre.fit_transform(tiny_data)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "count_vect_plain_pre_token6k = CountVectorizer(tokenizer=fix_spellings, max_features=10000, strip_accents='ascii', lowercase=True)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "X_train_counts_plain_pre_token6k = count_vect_plain_pre_token6k.fit_transform(train_data)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "X_dev_counts_plain_pre_token6k = count_vect_plain_pre_token6k.transform(dev_data)\n",
    "print(str(datetime.datetime.now().time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
