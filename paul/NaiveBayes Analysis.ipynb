{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of things were tested for this model\n",
    "\n",
    "* A variety of different parameters and vectorizers\n",
    "  * Count and Tfidf vectors\n",
    "  * Variety of feature sizes\n",
    "  * Data preprocessed or not\n",
    "  * Removing stop words and accents\n",
    "* Calculation of three types of scoring, precision, recall and auc\n",
    "\n",
    "Lessons learned\n",
    "* Experience is key: being novice has made this process take much longer\n",
    "\n",
    "* Interpreters (I'm sure everyone knows this) are slow, and for the moment this is single threaded so even slower.  A side requiment of this is that hardware matters, particularly faster cores and plenty of memory.\n",
    "\n",
    "* While we can in some cases brute force the best parameters there are situations where unless we have time and a cluster for processing power, we must instead rely on educated guesswork and compromises.  The educated guesswork can obviously be helped by research and experience.\n",
    "\n",
    "* Gaussian Naive Bayes is not suitable for the very sparse inputs we are using so not testing these out.\n",
    "\n",
    "\n",
    "Results:  \n",
    "\n",
    "<table>\n",
    "<tr><th> label </th><th> model </th><th> alpha </th><th> type </th><th> preprocessor </th><th> tokenizer </th><th> max_features </th><th> stop_words </th><th> lowercase </th><th> strip_accents </th><th> score_type </th><th> score </th></tr>\n",
    "<tr><td> toxic </td><td> multi </td><td> 10 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> None </td><td> None </td><td> TRUE </td><td> None </td><td> precision </td><td> 1 </td></tr>\n",
    "<tr><td> severe_toxic </td><td> multi </td><td> 10 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 6000 </td><td> english </td><td> FALSE </td><td> None </td><td> precision </td><td> 0.8 </td></tr>\n",
    "<tr><td> obscene </td><td> multi </td><td> 2 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> None </td><td> None </td><td> TRUE </td><td> None </td><td> precision </td><td> 1 </td></tr>\n",
    "<tr><td> threat </td><td> multi </td><td> 0.5 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> None </td><td> None </td><td> FALSE </td><td> None </td><td> precision </td><td> 1 </td></tr>\n",
    "<tr><td> insult </td><td> multi </td><td> 2 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> None </td><td> None </td><td> FALSE </td><td> None </td><td> precision </td><td> 1 </td></tr>\n",
    "<tr><td> identity_hate </td><td> multi </td><td> 2 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td> None </td><td> precision </td><td> 0.882353 </td></tr>\n",
    "    <tr><td> <strong><font color=red>Average</font></strong> </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> <strong><font color=red>0.947058833 </td></tr>\n",
    "\n",
    "<tr><td> toxic </td><td> bern </td><td> 1 </td><td> count </td><td> 0 </td><td> 0 </td><td> 10000 </td><td> english </td><td> TRUE </td><td> unicode </td><td> recall </td><td> 0.886398 </td></tr>\n",
    "<tr><td> severe_toxic </td><td> bern </td><td> 0.5 </td><td> count </td><td> 0 </td><td> 0 </td><td> 10000 </td><td> english </td><td> TRUE </td><td> None </td><td> recall </td><td> 0.953782 </td></tr>\n",
    "<tr><td> obscene </td><td> bern </td><td> 1 </td><td> count </td><td> 1 </td><td> 0 </td><td> 10000 </td><td> english </td><td> FALSE </td><td> None </td><td> recall </td><td> 0.897975 </td></tr>\n",
    "<tr><td> threat </td><td> bern </td><td> 0.5 </td><td> count </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> None </td><td> TRUE </td><td> ascii </td><td> recall </td><td> 0.868421 </td></tr>\n",
    "<tr><td> insult </td><td> bern </td><td> 1 </td><td> count </td><td> 0 </td><td> 0 </td><td> 10000 </td><td> english </td><td> TRUE </td><td> unicode </td><td> recall </td><td> 0.885738 </td></tr>\n",
    "<tr><td> identity_hate </td><td> bern </td><td> 0.5 </td><td> count </td><td> 1 </td><td> 0 </td><td> 4000 </td><td> None </td><td> FALSE </td><td> None </td><td> recall </td><td> 0.88785 </td></tr>\n",
    "<tr><td> <strong><font color=red>Average</font></strong>  </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> <strong><font color=red>0.896694 </font></strong></td></tr>\n",
    "\n",
    "<tr><td> toxic </td><td> bern </td><td> 15 </td><td> count </td><td> 1 </td><td> 0 </td><td> 4000 </td><td> english </td><td> FALSE </td><td> None </td><td> roc_auc </td><td> 0.858937 </td></tr>\n",
    "<tr><td> severe_toxic </td><td> bern </td><td> 2 </td><td> count </td><td> 1 </td><td> 0 </td><td> 4000 </td><td> english </td><td> FALSE </td><td> None </td><td> roc_auc </td><td> 0.93949 </td></tr>\n",
    "<tr><td> obscene </td><td> bern </td><td> 10 </td><td> count </td><td> 1 </td><td> 0 </td><td> 4000 </td><td> english </td><td> FALSE </td><td> None </td><td> roc_auc </td><td> 0.888569 </td></tr>\n",
    "<tr><td> threat </td><td> bern </td><td> 0.5 </td><td> count </td><td> 1 </td><td> 0 </td><td> 4000 </td><td> None </td><td> FALSE </td><td> None </td><td> roc_auc </td><td> 0.899946 </td></tr>\n",
    "<tr><td> insult </td><td> bern </td><td> 10 </td><td> count </td><td> 1 </td><td> 0 </td><td> 4000 </td><td> english </td><td> FALSE </td><td> None </td><td> roc_auc </td><td> 0.868953 </td></tr>\n",
    "<tr><td> identity_hate </td><td> bern </td><td> 1 </td><td> count </td><td> 1 </td><td> 0 </td><td> 4000 </td><td> english </td><td> FALSE </td><td> None </td><td> roc_auc </td><td> 0.886601 </td></tr>\n",
    "<tr><td> <strong><font color=red>Average</font></strong>  </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> <strong><font color=red>0.890416 </font></strong></td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "from sklearn import metrics\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total training observations:', 111828)\n",
      "('training data shape:', (111828,))\n",
      "('training label shape:', (111828, 6))\n",
      "('dev label shape:', (47743, 6))\n",
      "('labels names:', ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"w207_FinalProject/data/new_train.csv\")\n",
    "test_df = pd.read_csv(\"w207_FinalProject/data/new_test.csv\")\n",
    "\n",
    "# # Random index generator for splitting training data\n",
    "# # Note: Each rerun of cell will create new splits.\n",
    "# randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "# #S plit up data\n",
    "# test_data = test_df[\"comment_text\"]\n",
    "# dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "# train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "dev_data, dev_labels = test_df[\"comment_text\"], test_df[target_names]\n",
    "train_data, train_labels = train_df[\"comment_text\"], train_df[target_names]\n",
    "\n",
    "print('total training observations:', train_df.shape[0])\n",
    "print('training data shape:', train_data.shape)\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print('labels names:', target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Courtesy of Walt\n",
    "\n",
    "import nltk\n",
    "# These imports enable the use of NLTKPreprocessor in an sklearn Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import punkt as punkt\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Text preprocessor using NLTK tokenization and Lemmatization\n",
    "\n",
    "    This class is to be used in an sklean Pipeline, prior to other processers like PCA/LSA/classification\n",
    "    Attributes:\n",
    "        lower: A boolean indicating whether text should be lowercased by preprocessor\n",
    "                default: True\n",
    "        strip: A boolean indicating whether text should be stripped of surrounding whitespace, underscores and '*'\n",
    "                default: True\n",
    "        stopwords: A set of words to be used as stop words and thus ignored during tokenization\n",
    "                default: built-in English stop words\n",
    "        punct: A set of punctuation characters that should be ignored\n",
    "                default: None\n",
    "        lemmatizer: An object that should be used to lemmatize tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(unicode(document, 'utf8')):\n",
    "\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                \n",
    "                # S\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of scores on dev set and training set\n",
    "def score_classifier_train_on_dev(dev_vector, train_vector, dev_labels, train_labels, name, ctype, pscoring):\n",
    "    \"\"\"This function takes two vectors, one for training and one for dev, trains them\n",
    "    on the selected Naive Bayes model, then depending on the schoring required it\n",
    "    finds the optimal alpha for the particular scoring and calculates that score from\n",
    "    predictions on the dev set.\n",
    "    \n",
    "    Args:\n",
    "        dev_vector: the processed vector of dev data\n",
    "        train_vector: the processed vector of training data\n",
    "        dev_labels: the vector of each of the 6 lables for the dev set\n",
    "        train_labels: the vector of labels for the training set\n",
    "        name (string) : the label name to test\n",
    "        ctype: multi, gaus or bern, choses between multinomial or bernoulli\n",
    "        scoring: should be one of roc_auc, precision, or recall\n",
    "        \n",
    "    Returns:\n",
    "        alpha: the best alpha value for this classifier\n",
    "        score: the score when using this classifier to predict dev\n",
    "    \"\"\"\n",
    "    alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 15.0, 20.0, 50.0, 100.0]}\n",
    "\n",
    "    if pscoring != 'precision' and pscoring != 'recall' and pscoring != 'roc_auc' and pscoring != 'f1':\n",
    "        print('score_classifier_train_on_dev: Invalid input parameter %s' %(pscoring))\n",
    "        return\n",
    "    \n",
    "    if ctype == 'multi':\n",
    "        nb_class = MultinomialNB().fit(train_vector, train_labels[name])\n",
    "    elif ctype == 'bern':\n",
    "        nb_class = BernoulliNB().fit(train_vector, train_labels[name])\n",
    "    elif ctype == 'gaus':\n",
    "        nb_class = GaussianNB().fit(train_vector, train_labels[name])\n",
    "    else:\n",
    "        print('ctype = %s, error' % (ctype))\n",
    "        return\n",
    "    \n",
    "    # use this to generate the best fitting model for AUC scoring\n",
    "    clf = GridSearchCV(nb_class, param_grid = alphas, scoring=pscoring)\n",
    "    clf.fit(train_vector, train_labels[name])\n",
    "    \n",
    "    # Predict the dev vector\n",
    "    predicted_labels_dev = clf.predict(dev_vector)\n",
    "    \n",
    "    rscore = 0 # return score\n",
    "    # now calculate the score of interested based on the function parameter pscoring\n",
    "    if pscoring == 'precision':\n",
    "        rscore = metrics.precision_score(dev_labels[name], predicted_labels_dev)\n",
    "    elif pscoring == 'recall':\n",
    "        rscore = metrics.recall_score(dev_labels[name], predicted_labels_dev)\n",
    "    elif pscoring == 'f1':\n",
    "        rscore = metrics.f1_score(dev_labels[name], predicted_labels_dev)\n",
    "    else:\n",
    "        rscore = metrics.roc_auc_score(dev_labels[name], predicted_labels_dev)\n",
    "\n",
    "    return clf.best_params_, rscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_all_vectors(my_feature_sizes = [None, 3000, 4000, 5000, 6000, 10000],\n",
    "                       my_stop_words = [None, 'english'],\n",
    "                       my_strip_accents = [None, 'ascii', 'unicode'],\n",
    "                       my_lowercase = [True, False],\n",
    "                       basetrain_data=[],\n",
    "                       basedev_data=[],\n",
    "                       preprocessedtrain_data=[],\n",
    "                       preprocesseddev_data=[],\n",
    "                       verbose=False):\n",
    "    \n",
    "    vectors_all=pd.DataFrame(columns=['vectortrain', 'vectordata','type','preprocessor', 'tokenizer',\n",
    "                                      'max_features', 'stop_words', 'lowercase', 'strip_accents' ])\n",
    "\n",
    "    index=1\n",
    "    if len(basetrain_data) != 0 and len(basedev_data) != 0:\n",
    "        # we have unprocessed data so create vectors for it\n",
    "        for i in my_feature_sizes:\n",
    "            for x in my_stop_words:\n",
    "                for y in my_strip_accents:\n",
    "                    for z in my_lowercase:\n",
    "                        if (verbose == True):\n",
    "                            print(\"%s: Processing the next two vectors from base data, index %d\" % (str(datetime.datetime.now().time()),index))\n",
    "                            index +=1\n",
    "\n",
    "                        # Create a count vectorizer with the provided parameters\n",
    "                        vect = CountVectorizer(max_features=i, stop_words=x, strip_accents=y, lowercase=z)\n",
    "                        # Train the unpreprocessed training set\n",
    "                        vect_train = vect.fit_transform(train_data)\n",
    "                        # Transform the dev set for fuuture predictions\n",
    "                        vect_dev = vect.transform(dev_data)\n",
    "                        # add into the output data frame with the list of vectors chosen\n",
    "                        vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 0, 0, i, x, z, y]\n",
    "\n",
    "                        # Now create tfidf vectorizer for the set of parameters\n",
    "                        vect = TfidfVectorizer(max_features=i, stop_words=x, strip_accents=y, analyzer='word',lowercase=z)\n",
    "                        vect_train = vect.fit_transform(train_data)\n",
    "                        vect_dev = vect.transform(dev_data)\n",
    "                        vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 0, 0, i, x, z, y]\n",
    "\n",
    "    index=1\n",
    "    if len(preprocessedtrain_data) != 0 and len(preprocesseddev_data) != 0:\n",
    "        # Separate loop for the preprocessed data as we cannot set the lowercase or strip accents parameters on these\n",
    "        for i in my_feature_sizes:\n",
    "            for x in my_stop_words:\n",
    "                print(\"%s: Processing the next two vectors from preprocessed data, index %d\" % (str(datetime.datetime.now().time()),index))\n",
    "                index +=1\n",
    "\n",
    "                # Same but with the preprocessed data\n",
    "                vect = CountVectorizer(tokenizer=identity, max_features=i, stop_words=x,strip_accents=None, lowercase=False)\n",
    "                vect_train= vect.fit_transform(train_preproc_data)\n",
    "                vect_dev= vect.transform(dev_preproc_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 1, 0, i, x, False, None]\n",
    "\n",
    "                # Create a tfidf but fit with the preprocessed data\n",
    "                vect = TfidfVectorizer(tokenizer=identity,max_features=i, stop_words=x,strip_accents=None, lowercase=False)\n",
    "                vect_train = vect.fit_transform(train_preproc_data)\n",
    "                vect_dev = vect.transform(dev_preproc_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 1, 0, i, x, False, None]\n",
    "\n",
    "    if verbose == True:\n",
    "        print('%s: Completed create_all_vectors' % (str(datetime.datetime.now().time())))\n",
    "        \n",
    "    return vectors_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1auc_all_models (vectors_all, verbose=False):\n",
    "    \"\"\"This function takes a vector of type vectors_all (defined above) acts as a wrapper\n",
    "    to send each vector to the score_classifier_train_on_dev function selecting first multinomialNB\n",
    "    and after that Bernoulli.  It collects the resulting scores and the best alpha and stores\n",
    "    the results in a dataframe which is returned once all the results are calculated\n",
    "    \n",
    "    Args:\n",
    "        Vectors_all (datafram) : a dataframe defined above that stores the vector data in each row\n",
    "    Returns\n",
    "        dataframe: A dataframe of all the resulting scores and the details for each model\n",
    "    \"\"\"\n",
    "    data_all=pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'score_type', 'score'])\n",
    "    score_types = ['precision', 'recall', 'roc_auc']\n",
    "    model_types = ['gaus','multi', 'bern']\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('%s: Starting calculate_f1auc_all_models' % (str(datetime.datetime.now().time())))        \n",
    "    for index,row in vectors_all.iterrows():\n",
    "        if verbose == True:\n",
    "            print('%s: checking row %d' % (str(datetime.datetime.now().time()),index))\n",
    "        for name in target_names:\n",
    "            for score_type in score_types:\n",
    "                for model_type in model_types:\n",
    "                    # Calculate the score for this pair of vectors with a variety of scoring\n",
    "                    # parameters and types of NB classifier\n",
    "                    alpha, score = score_classifier_train_on_dev(train_vector=row['vectortrain'], \n",
    "                                        dev_vector=row['vectordata'], dev_labels=dev_labels,\n",
    "                                        train_labels=train_labels, name=name, ctype=model_type, pscoring=score_type )\n",
    "                    \n",
    "                    # Store all the results in the dataframe\n",
    "                    data_all.loc[data_all.shape[0]] = [index,name,model_type,alpha,row['type'], \n",
    "                                        row['preprocessor'], row['tokenizer'], row['max_features'], \n",
    "                                        row['stop_words'], row['lowercase'], row['strip_accents'],\n",
    "                                        score_type, score]\n",
    "    if verbose == True:\n",
    "        print('%s: finished calculate_f1auc_all_models' % (str(datetime.datetime.now().time())))               \n",
    "    return data_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:06:56.527687: starting\n",
      "10:06:56.528066: completed NLTK preprocessor\n",
      "10:06:56.530569: Processing the next two vectors from base data, index 1\n",
      "10:07:22.553611: Processing the next two vectors from base data, index 2\n",
      "10:07:48.544374: Processing the next two vectors from base data, index 3\n",
      "10:08:16.261657: Processing the next two vectors from base data, index 4\n",
      "10:08:44.098041: Processing the next two vectors from base data, index 5\n",
      "10:09:13.216635: Processing the next two vectors from base data, index 6\n",
      "10:09:42.508882: Processing the next two vectors from base data, index 7\n",
      "10:10:07.574175: Processing the next two vectors from base data, index 8\n",
      "10:10:33.089771: Processing the next two vectors from base data, index 9\n",
      "10:11:00.191211: Processing the next two vectors from base data, index 10\n",
      "10:11:27.760421: Processing the next two vectors from base data, index 11\n",
      "10:11:55.988898: Processing the next two vectors from base data, index 12\n",
      "10:12:24.834770: Processing the next two vectors from base data, index 13\n",
      "10:12:50.579737: Processing the next two vectors from base data, index 14\n",
      "10:13:16.394922: Processing the next two vectors from base data, index 15\n",
      "10:13:44.241323: Processing the next two vectors from base data, index 16\n",
      "10:14:11.904706: Processing the next two vectors from base data, index 17\n",
      "10:14:40.911605: Processing the next two vectors from base data, index 18\n",
      "10:15:09.901580: Processing the next two vectors from base data, index 19\n",
      "10:15:35.615007: Processing the next two vectors from base data, index 20\n",
      "10:16:01.059174: Processing the next two vectors from base data, index 21\n",
      "10:16:28.270546: Processing the next two vectors from base data, index 22\n",
      "10:16:56.678835: Processing the next two vectors from base data, index 23\n",
      "10:17:25.843669: Processing the next two vectors from base data, index 24\n",
      "10:17:54.825884: Processing the next two vectors from base data, index 25\n",
      "10:18:21.056855: Processing the next two vectors from base data, index 26\n",
      "10:18:47.267056: Processing the next two vectors from base data, index 27\n",
      "10:19:15.470497: Processing the next two vectors from base data, index 28\n",
      "10:19:43.459379: Processing the next two vectors from base data, index 29\n",
      "10:20:12.611898: Processing the next two vectors from base data, index 30\n",
      "10:20:42.133618: Processing the next two vectors from base data, index 31\n",
      "10:21:07.418076: Processing the next two vectors from base data, index 32\n",
      "10:21:33.278940: Processing the next two vectors from base data, index 33\n",
      "10:22:01.018196: Processing the next two vectors from base data, index 34\n",
      "10:22:28.644048: Processing the next two vectors from base data, index 35\n",
      "10:22:57.377274: Processing the next two vectors from base data, index 36\n",
      "10:23:26.141872: Processing the next two vectors from base data, index 37\n",
      "10:23:52.166434: Processing the next two vectors from base data, index 38\n",
      "10:24:18.375003: Processing the next two vectors from base data, index 39\n",
      "10:24:46.387198: Processing the next two vectors from base data, index 40\n",
      "10:25:14.454854: Processing the next two vectors from base data, index 41\n",
      "10:25:43.856049: Processing the next two vectors from base data, index 42\n",
      "10:26:13.526202: Processing the next two vectors from base data, index 43\n",
      "10:26:39.420945: Processing the next two vectors from base data, index 44\n",
      "10:27:05.787669: Processing the next two vectors from base data, index 45\n",
      "10:27:34.158919: Processing the next two vectors from base data, index 46\n",
      "10:28:02.725737: Processing the next two vectors from base data, index 47\n",
      "10:28:35.239641: Processing the next two vectors from base data, index 48\n",
      "10:29:10.049443: Processing the next two vectors from base data, index 49\n"
     ]
    }
   ],
   "source": [
    "verbose=True\n",
    "top_score_results = pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'score_type', 'score'])\n",
    "score_types = ['precision', 'recall', 'roc_auc']\n",
    "\n",
    "print('%s: starting' %(str(datetime.datetime.now().time())))\n",
    "\n",
    "# # Set up all the NLTK Preprocessing\n",
    "# print('%s: Creating new NLTK Preprocessor' %(str(datetime.datetime.now().time())))\n",
    "# nltkPreprocessor = NLTKPreprocessor()\n",
    "# print('%s: Converting training data with NLTK Preprocessor' %(str(datetime.datetime.now().time())))\n",
    "# nltkPreprocessor.fit(train_data)\n",
    "# train_preproc_data = nltkPreprocessor.transform(train_data)\n",
    "# print('%s: Converting dev data with NLTK Preprocessor' %(str(datetime.datetime.now().time())))\n",
    "# nltkPreprocessor.fit(dev_data)\n",
    "# dev_preproc_data = nltkPreprocessor.transform(dev_data)\n",
    "# print('%s: done' %(str(datetime.datetime.now().time())))\n",
    "\n",
    "if verbose == True:\n",
    "    print('%s: transforming training data with NLTK preprocessor' %(str(datetime.datetime.now().time())))\n",
    "train_preproc_data = NLTKPreprocessor().fit(train_data).transform(train_data)\n",
    "if verbose == True:\n",
    "    print('%s: transforming dev data with NLTK preprocessor' %(str(datetime.datetime.now().time())))\n",
    "dev_preproc_data = NLTKPreprocessor().fit(dev_data).transform(dev_data)\n",
    "if verbose == True:\n",
    "    print('%s: completed NLTK preprocessor' %(str(datetime.datetime.now().time())))\n",
    "\n",
    "# Create the set of vectors:\n",
    "vectors_all = create_all_vectors(basetrain_data=train_data, basedev_data=dev_data,\n",
    "                        preprocessedtrain_data=train_preproc_data, preprocesseddev_data=dev_preproc_data,\n",
    "                        verbose=verbose)\n",
    "\n",
    "# calculate the f1 and auc for all the models\n",
    "result_df = calculate_f1auc_all_models(vectors_all,verbose=verbose)\n",
    "\n",
    "for label in target_names:\n",
    "    df_tmp = result_df.loc[resultdf['label'] == label]\n",
    "    for score_type in score_types:\n",
    "        df_tmp2 = df_tmp[df_tmp.score_type == score_type]\n",
    "        top_score_results.loc[top_score_results.shape[0]] = df_tmp2.loc[df_tmp2['score'].idxmax()]\n",
    "    \n",
    "print(top_score_results)\n",
    "print('%s: ending' %(str(datetime.datetime.now().time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for overfitting\n",
    "# Do a new dev split create the best models as defined by the previous step, train them on the new train\n",
    "# Graph the data for presentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
