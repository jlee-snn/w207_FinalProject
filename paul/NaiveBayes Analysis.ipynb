{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of things were tested for this model\n",
    "\n",
    "* A variety of different parameters to countVectorizer and tfidfVectorizor including\n",
    "  * a number of different preprocessing steps\n",
    "    * regular expressions to remove some oddities\n",
    "    * an attempt to correct spellings and conjoined words\n",
    "  * ngrams from (1,10) - code not here as it did not have any success\n",
    "* Use of GridSearchCV to find the optimal alpha for each classifier\n",
    "  \n",
    "Interesting observations\n",
    "\n",
    "* Depending on which method we use to score the classifier, AUC or F1, the choice of parameters differ. All the top scores for AUC are MultinomianNB using TfidfVectorizer, and all the top scores for F1 are BernoulliNB using CountVectorizer.  See https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/ for the details.\n",
    "* Spell checking is very slow.  One model replacing the tokenizer with one that did a spell check on the words took over 24 hours to run.  Instead we decided to run the spell checker on the data and save it so that the effort was not repeated.  The result took 4 processing cores (4 threads doing a portion of the work each) 24 hours to complete.\n",
    "\n",
    "\n",
    "Results:  \n",
    "Best scores for AUC:\n",
    "\n",
    "<table>\n",
    "<tr><th> label </th><th> model </th><th> alpha </th><th> type </th><th> preprocessor </th><th> tokenizer </th><th> max_features </th><th> stop_words </th><th> lowercase </th><th> strip_accents </th><th> aucdev</th></tr>\n",
    "<tr><td> toxic </td><td> bern </td><td> 0.5 </td><td> count </td><td> 0 </td><td> 0 </td><td>  </td><td> english </td><td> TRUE </td><td> unicode </td><td> 0.852321727</td></tr>\n",
    "<tr><td> severe_toxic </td><td> bern </td><td> 2.0 </td><td> count </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.942104059</td></tr>\n",
    "<tr><td> obscene </td><td> bern </td><td> 10.0 </td><td> count </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.893736805</td></tr>\n",
    "<tr><td> threat </td><td> bern </td><td> 0.5 </td><td> count </td><td> 0 </td><td> 0 </td><td> 5000 </td><td> english </td><td> TRUE </td><td>  </td><td> 0.838735331</td></tr>\n",
    "<tr><td> insult </td><td> bern </td><td> 10.0 </td><td> count </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.878101445</td></tr>\n",
    "<tr><td> identity_hate </td><td> bern </td><td> 2.0 </td><td> count </td><td> 0 </td><td> 0 </td><td> 6000 </td><td>  </td><td> TRUE </td><td> ascii </td><td> 0.82812169</td></tr>\n",
    "<tr><td> </td></tr>\n",
    "<tr><td> Average </td><td> &nbsp; </td><td> &nbsp;</td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td>  </td><td> &nbsp; </td><td> &nbsp; </td><td> 0.872187</td></tr>\n",
    "<tr><td> </td></tr>\n",
    "</table>\n",
    "\n",
    "Best scores for F1:\n",
    "<table>\n",
    "<tr><th> label </th><th> model </th><th> alpha </th><th> type </th><th> preprocessor </th><th> tokenizer </th><th> max_features </th><th> stop_words </th><th> lowercase </th><th> strip_accents </th><th> f1dev </th></tr>\n",
    "<tr><td> toxic </td><td> multi </td><td> 0.1 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 6000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.950819672 </td></tr>\n",
    "<tr><td> severe_toxic </td><td> multi </td><td> 0.5 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 10000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.990763086 </td></tr>\n",
    "<tr><td> obscene </td><td> multi </td><td> 0.5 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td> ascii </td><td> 0.9735999 </td></tr>\n",
    "<tr><td> threat </td><td> multi </td><td> 0.1 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 4000 </td><td> english </td><td> TRUE </td><td>  </td><td> 0.996879421 </td></tr>\n",
    "<tr><td> insult </td><td> multi </td><td> 0.1 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 5000 </td><td>  </td><td> TRUE </td><td> ascii </td><td> 0.969231089 </td></tr>\n",
    "<tr><td> identity_hate </td><td> multi </td><td> 0.5 </td><td> tfidf </td><td> 0 </td><td> 0 </td><td> 6000 </td><td> english </td><td> TRUE </td><td>\n",
    " ascii </td><td> 0.991303986 </td></tr>\n",
    "<tr><td> Average </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> 0.978766 </td></tr>  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burgew/Library/Python/3.6/lib/python/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/burgew/Library/Python/3.6/lib/python/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 159571\n",
      "training data shape: (111471,)\n",
      "training label shape: (111471, 6)\n",
      "dev label shape: (48100, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "print('total training observations:', train_df.shape[0])\n",
    "print('training data shape:', train_data.shape)\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print('labels names:', target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'enchant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0783147cf886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menchant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictWithPWL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'enchant'"
     ]
    }
   ],
   "source": [
    "# Imports etc. used in this analysis\n",
    "import string\n",
    "import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from enchant import DictWithPWL\n",
    "from enchant.checker import SpellChecker\n",
    "import difflib\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "punctuation = \"[\\!\\?\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\,\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\\']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://norvig.com/spell-correct.html\n",
    "# This is the Norvig spell checker and requires the storage of a \"big.txt\"\n",
    "# file with a corpus of words that it uses for predictions\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../data/big.txt').read()))\n",
    "\n",
    "def norvig_P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def norvig_correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(norvig_candidates(word), key=norvig_P)\n",
    "\n",
    "def norvig_candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyenchant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f936cf8d5f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Functions to support finding and correcting spellings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# using pyenchant for spell checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyenchant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictWithPWL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyenchant'"
     ]
    }
   ],
   "source": [
    "# Functions to support finding and correcting spellings\n",
    "# using pyenchant for spell checking\n",
    "from pyenchant import DictWithPWL\n",
    "from enchant.checker import SpellChecker\n",
    "import difflib\n",
    "# import splitter # not useful, does a worse job than my implementation\n",
    "\n",
    "# mywords.txt currently contains:\n",
    "# - list of firstnames and surnames gathered from internet searches\n",
    "# http://www.birkenhoerdt.net/surnames-all.php?tree=1\n",
    "# www.tysto.com/uk-us-spelling-list.html\n",
    "my_dict=DictWithPWL('en_US', \"../data/mywords.txt\")\n",
    "my_checker = SpellChecker(my_dict)\n",
    "\n",
    "punctuation = \"[\\!\\?\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\,\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\\']\"\n",
    "\n",
    "# list of swear words correctly spelt courtesy of https://www.noswearing.com/\n",
    "\n",
    "def my_preprocessor(textblock):\n",
    "    # u -> you\n",
    "    # c -> see\n",
    "    # k -> okay\n",
    "    return_words = textblock\n",
    "\n",
    "#     return_words = re.sub(r\"[^A-Za-z0-9,!?*.;\\u2019´'\\/]\", \" \", return_words)\n",
    "    return_words = re.sub(r\"[^A-Za-z0-9]\", \" \", return_words)\n",
    "    return_words = re.sub(r\",\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\.\\.+\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\.\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\(\",\" \", return_words)\n",
    "    return_words = re.sub(r\"\\)\",\" \", return_words)\n",
    "    return_works = re.sub(r\"\\;\", \" \", return_words)\n",
    "    return_words = re.sub(r\":\",\" \", return_words)\n",
    "    return_words = re.sub(r\"´\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"`\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"''+\", \"'\", return_words)\n",
    "    return_words = re.sub(r\" '\", \" \", return_words)\n",
    "    return_words = re.sub(r\"' \", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\\"\", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\/\", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\!\\!+\", \"!!\", return_words)\n",
    "    return_words = re.sub(r\"\\?\\?+\", \"?!\", return_words)\n",
    "    return_words = re.sub(r\"\\!\", \" !\", return_words)\n",
    "    return_words = re.sub(r\"\\?\", \" ?\", return_words)\n",
    "    return_words = re.sub(r\"\\b\\d+\\b\", \"999\", return_words)\n",
    "    # slang and abbreviations, need to be aware of capitolization and spaces\n",
    "    return_words = re.sub(r\"[Ww]on't\", \"will not\", return_words)\n",
    "    return_words = re.sub(r\"n't\", \" not\", return_words)\n",
    "    return_words = re.sub(r\"'s\\b\", \" is\", return_words)\n",
    "    return_words = re.sub(r\"\\b[Aa]bt\\b\", \"about\", return_words)\n",
    "    return return_words\n",
    "\n",
    "def trysplit(word, verbose=False):\n",
    "    split_candidates = []\n",
    "    max_proba = 0.0\n",
    "    for i in range(1,len(word)):\n",
    "        # I will only allow single letters of 'a' and 'i', all others ignored.  Pyenchant allows for\n",
    "        # any single letter to be a legitimate word, and so too does norvig.  The dictionary defines\n",
    "        # them as nouns that represent the letter, however even though several can be used in slang\n",
    "        # (e.g. k->okay, c->see, u->you) using them in conjoined words would make the splitting far\n",
    "        # too difficult and also human understanding much more difficult #howucthisk, u c?\n",
    "        if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "            len(word[i:]) != 1 or (word[i:].lower() == 'a' or word[i:].lower() == 'i')):\n",
    "            if my_checker.check(word[:i]) and my_checker.check(word[i:]):\n",
    "                norvig_score = norvig_P(word[:i]) + norvig_P(word[i:])\n",
    "                if norvig_score > max_proba:\n",
    "                    max_proba = norvig_score\n",
    "                    split_candidates = [word[:i],word[i:]]\n",
    "    for i in range(1,len(word)):\n",
    "        for j in range(i+1,len(word)):        \n",
    "            if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "                len(word[i:j]) != 1 or (word[i:j].lower() == 'a' or word[i:j].lower() == 'i')) and (\n",
    "                len(word[i:]) != 1 or (word[i:].lower() == 'a' or word[i:].lower() == 'i')):\n",
    "                \n",
    "                if my_checker.check(word[:i]) and my_checker.check(word[i:j]) and my_checker.check(word[j:]):\n",
    "                    norvig_score = norvig_P(word[:i]) + norvig_P(word[i:j]) + norvig_P(word[j:])\n",
    "                    if norvig_score > max_proba:\n",
    "                        max_proba = norvig_score\n",
    "                        split_candidates = [word[:i],word[i:j],word[j:]]\n",
    "    for i in range(1,len(word)):\n",
    "        for j in range(i+1,len(word)):\n",
    "            for k in range(j+1,len(word)):\n",
    "                if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "                    len(word[i:j]) != 1 or (word[i:j].lower() == 'a' or word[i:j].lower() == 'i')) and (\n",
    "                    len(word[j:k]) != 1 or (word[j:k].lower() == 'a' or word[j:k].lower() == 'i')) and (\n",
    "                    len(word[k:]) != 1 or (word[k:].lower() == 'a' or word[k:].lower() == 'i')):\n",
    "                    verbose and print(\"making it here with i=%s j=%s k=%s %s  max_proba=%d\" %(word[:i],word[i:j],word[j:k],word[k:], max_proba))\n",
    "                    verbose and print(\"lengths are %d %d %d %d\" % (len(word[:i]), len(word[i:j]),len(word[j:k]),len(word[k:])))\n",
    "                    if my_checker.check(word[:i]) and my_checker.check(word[i:j]) and my_checker.check(word[j:k]) and my_checker.check(word[k:]):\n",
    "                        verbose and print('found words ' + word[i:] + ' ' + word[k:])\n",
    "                        norvig_score = norvig_P(word[:i]) + norvig_P(word[i:j]) + norvig_P(word[j:k]) + norvig_P(word[k:])\n",
    "                        if norvig_score > max_proba:\n",
    "                            verbose and print(\"found higher probability %d with %s %s %s %s\" % (norvig_score, word[:i], word[i:j], word[j:k], word[k:]))\n",
    "                            max_proba = norvig_score\n",
    "                            split_candidates = [word[:i],word[i:j],word[j:k],word[k:]]\n",
    "    return split_candidates\n",
    "\n",
    "def get_best_candidates(word):\n",
    "    best_words = []\n",
    "    best_ratio = 0\n",
    "    a = set(my_checker.suggest(word))\n",
    "    for b in a:\n",
    "        if not '-' in b:\n",
    "            tmp = difflib.SequenceMatcher(None, word, b).ratio()\n",
    "            if tmp > best_ratio:\n",
    "                best_words=[b]\n",
    "                best_ratio=tmp\n",
    "            elif tmp == best_ratio:\n",
    "                best_words.append(b)\n",
    "    return best_words\n",
    "    \n",
    "def fix_spellings(textblock, verbose=False):\n",
    "    textblock = re.sub(\"[^A-Za-z0-9,!?*.;\\\\u2019\\´\\'\\\"\\\\\\/] \", \"\", textblock)\n",
    "    textblock = re.sub(r\"\\(\\)\", \" \", textblock)\n",
    "    textblock = re.sub(r'([a-zA-Z_ ])\\1+', r'\\1\\1',textblock)\n",
    "    words = textblock.split()\n",
    "    return_list = []\n",
    "\n",
    "    for word in words:\n",
    "        if my_checker.check(word) or my_checker.check(word.lower()) or word in punctuation or\\\n",
    "            any(i.isdigit() or i == '_' for i in word) or (word[-1].lower() == 's' and my_checker.check(word[:-1].lower())):\n",
    "            return_list.append(word)\n",
    "        elif len(word) < 100:            \n",
    "            candidates = get_best_candidates(word)\n",
    "            if len(candidates) == 1:\n",
    "                return_list.append(candidates.pop())\n",
    "            elif len(candidates) > 1:\n",
    "                # try another spell checker\n",
    "                nv_candidates = norvig_candidates(word)\n",
    "                tmp_set = set(nv_candidates).intersection(set(candidates))\n",
    "                if len(tmp_set) == 1:\n",
    "                    # only 1 overlap, should be correct\n",
    "                    return_list.append(tmp_set.pop())\n",
    "                elif len(nv_candidates) == 1 and next(iter(nv_candidates)) == word:\n",
    "                        # this is suspicious, pyenchants' \"suggest\" method always returns something, however if\n",
    "                        # norvigs method cannot find a suitable match within a short distance then it simply\n",
    "                        # returns the orignal word.  This section is for potentially conjoined words\n",
    "                        tmp_list=trysplit(word)\n",
    "\n",
    "                        # If we get back a list of split words then use these\n",
    "                        if len(tmp_list) != 0:\n",
    "                            return_list.extend(tmp_list)\n",
    "                        else:\n",
    "                            return_list.append(word)\n",
    "                else:\n",
    "                    # arbitrary now, just going to use the first one found from pyenchant, even though\n",
    "                    # I have seen norvig get the correct word sometimes when pyenchant gets it wrong\n",
    "                    return_list.append(candidates[0])\n",
    "            else:\n",
    "                nv_candidates = norvig_candidates(word)\n",
    "                if len(nv_candidates) > 0:\n",
    "                    return_list.append(nv_candidates[0])\n",
    "                else:\n",
    "                    return_list.append(word)\n",
    "        else:\n",
    "            return_list.append(word)\n",
    "\n",
    "    return ' '.join(return_list)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions:\n",
    "\n",
    "def my_preprocessor_eng(textblock):\n",
    "    \"\"\" This function is a simple set of regular expressions to remove/replace some punctuation\n",
    "    and replace some abbreviations\n",
    "    \n",
    "    Args:\n",
    "        textbloc (string): a string of words to run the expressions against\n",
    "    Returns:\n",
    "        a string of adjusted text\n",
    "    \"\"\"\n",
    "    return_words = textblock\n",
    "    return_words = re.sub(r\"[^A-Za-z0-9]?!'`:´()\", \" \", return_words)\n",
    "    return_words = re.sub(r\",\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\.\\.+\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\.\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\(\",\" \", return_words)\n",
    "    return_words = re.sub(r\"\\)\",\" \", return_words)\n",
    "    return_works = re.sub(r\"\\;\", \" \", return_words)\n",
    "    return_words = re.sub(r\":\",\" \", return_words)\n",
    "    return_words = re.sub(r\"´\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"`\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"''+\", \"'\", return_words)\n",
    "    return_words = re.sub(r\" '\", \" \", return_words)\n",
    "    return_words = re.sub(r\"' \", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\\"\", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\/\", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\!\\!+\", \"!!\", return_words)\n",
    "    return_words = re.sub(r\"\\?\\?+\", \"?!\", return_words)\n",
    "    return_words = re.sub(r\"\\!\", \" !\", return_words)\n",
    "    return_words = re.sub(r\"\\?\", \" ?\", return_words)\n",
    "    return_words = re.sub(r\"\\b\\d+\\b\", \"999\", return_words)\n",
    "    # slang and abbreviations, need to be aware of capitolization and spaces\n",
    "    return_words = re.sub(r\"[Ww]on't\", \"will not\", return_words)\n",
    "    return_words = re.sub(r\"n't\", \" not\", return_words)\n",
    "    return_words = re.sub(r\"'s\\b\", \" is\", return_words)\n",
    "    return_words = re.sub(r\"\\b[Aa]bt\\b\", \"about\", return_words)\n",
    "    return return_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/burgew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/burgew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "12:23:18.319466: Converting training data with NLTK Preprocessor\n",
      "12:32:36.947439: Converting dev data with NLTK Preprocessor\n",
      "12:36:06.838004: done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# These imports enable the use of NLTKPreprocessor in an sklearn Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import punkt as punkt\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Text preprocessor using NLTK tokenization and Lemmatization\n",
    "\n",
    "    This class is to be used in an sklean Pipeline, prior to other processers like PCA/LSA/classification\n",
    "    Attributes:\n",
    "        lower: A boolean indicating whether text should be lowercased by preprocessor\n",
    "                default: True\n",
    "        strip: A boolean indicating whether text should be stripped of surrounding whitespace, underscores and '*'\n",
    "                default: True\n",
    "        stopwords: A set of words to be used as stop words and thus ignored during tokenization\n",
    "                default: built-in English stop words\n",
    "        punct: A set of punctuation characters that should be ignored\n",
    "                default: None\n",
    "        lemmatizer: An object that should be used to lemmatize tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(str(document)):\n",
    "\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                \n",
    "                # S\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n",
    "\n",
    "nltkPreprocessor = NLTKPreprocessor()\n",
    "print('%s: Converting training data with NLTK Preprocessor' %(str(datetime.datetime.now().time())))\n",
    "nltkPreprocessor.fit(train_data)\n",
    "train_preproc_data = nltkPreprocessor.transform(train_data)\n",
    "print('%s: Converting dev data with NLTK Preprocessor' %(str(datetime.datetime.now().time())))\n",
    "nltkPreprocessor.fit(dev_data)\n",
    "dev_preproc_data = nltkPreprocessor.transform(dev_data)\n",
    "print('%s: done' %(str(datetime.datetime.now().time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of scores on dev set and training set\n",
    "def score_f1_auc_on_train_dev(dev_vector, train_vector, name, ctype='multi'):\n",
    "    \"\"\"This function creates a Naive Bayes classifier with the input vectors\n",
    "    and then calculates both the AUC score and F1 score for the training and dev data.\n",
    "    \n",
    "    Args:\n",
    "        dev_vector: the processed vector of dev data\n",
    "        train_vector: the processed vector of training data\n",
    "        name (string) : the label name to test\n",
    "        ctype: multi, gaus or bern, choses between multinomial or bernoulli\n",
    "    Returns:\n",
    "        alpha: the best alpha value for this classifier\n",
    "        f1scoredev: the F1 score for dev\n",
    "        aucdev: the AUC score for dev\n",
    "        f1scoretrain: the F1 score for training\n",
    "        auctrain: the AUC score for training\n",
    "    \"\"\"\n",
    "    alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 15.0, 20.0, 50.0, 100.0]}\n",
    "\n",
    "    if ctype == 'multi':\n",
    "        nb_class = MultinomialNB().fit(train_vector, train_labels[name])\n",
    "    elif ctype == 'bern':\n",
    "        nb_class = BernoulliNB().fit(train_vector, train_labels[name])\n",
    "    elif ctype == 'gaus':\n",
    "        nb_class = GaussianNB().fit(train_vector, train_labels[name])\n",
    "    else:\n",
    "        print('ctype = %s, error' % (ctype))\n",
    "    \n",
    "    # use this to generate the best fitting model\n",
    "    clf = GridSearchCV(nb_class, param_grid = alphas, scoring='roc_auc')\n",
    "    clf.fit(train_vector, train_labels[name])\n",
    "    \n",
    "    predicted_labels_dev = clf.predict(dev_vector)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dev_labels[name], predicted_labels_dev)\n",
    "    \n",
    "    predicted_labels_train = clf.predict(train_vector)\n",
    "    fpr1, tpr1, thresholds1 = metrics.roc_curve(train_labels[name], predicted_labels_train)\n",
    "    \n",
    "    f1scoredev = metrics.f1_score(dev_labels[name],predicted_labels_dev,average='micro')\n",
    "    f1scoretrain = metrics.f1_score(train_labels[name],predicted_labels_train,average='micro')\n",
    "    \n",
    "    aucdev = metrics.auc(fpr,tpr)\n",
    "    auctrain = metrics.auc(fpr1,tpr1)\n",
    "    \n",
    "    return clf.best_params_, f1scoredev,aucdev,f1scoretrain,auctrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:47:43.904467\n",
      "12:47:43.904943: Doing i = None\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "12:50:05.973100: Doing i = 4000\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "12:52:32.184922: Doing i = 5000\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "12:54:53.674513: Doing i = 6000\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "12:57:15.779264: Doing i = 10000\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "12:59:37.631506\n",
      "(60, 9)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "vectors_all=pd.DataFrame(columns=['vectortrain', 'vectordata','type','preprocessor', 'tokenizer',\n",
    "                                  'max_features', 'stop_words', 'lowercase', 'strip_accents' ])\n",
    "\n",
    "# this set of loops works through the chosen parameters creating a count and tfidf vectorizer\n",
    "# and using the preprocessor or not (4 per iteration).  These are stored in the vectors_all\n",
    "# dataframe along with a list of the parameters that were used to create each on\n",
    "print(str(datetime.datetime.now().time()))\n",
    "index=1\n",
    "\n",
    "for i in None, 4000, 5000, 6000, 10000:\n",
    "    print('%s: Doing i = %s' %(str(datetime.datetime.now().time()), i))\n",
    "    for x in None, 'english':\n",
    "        for y in None, 'ascii', 'unicode':\n",
    "#             for z in True, False:\n",
    "            z=False  # always came out best in prior tests\n",
    "            print(index)\n",
    "            index +=1\n",
    "\n",
    "#             vect = CountVectorizer(max_features=i, stop_words=x, strip_accents=y, lowercase=z)\n",
    "#             vect_train = vect.fit_transform(train_data)\n",
    "#             vect_dev = vect.transform(dev_data)\n",
    "#             vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 0, 0, i, x, z, y]\n",
    "            # Same but with the preprocessor\n",
    "    \n",
    "# Note that this vectorizer is created with a passthru tokenizer(identity), no preprocessor and no lowercasing\n",
    "# This is to account for the NLTKPreprocessor already taking care of these.\n",
    "            vect = CountVectorizer(tokenizer=identity, max_features=i, stop_words=x, \n",
    "                                   preprocessor=None,lowercase=False)\n",
    "            vect_train= vect.fit_transform(train_preproc_data)\n",
    "            vect_dev= vect.fit_transform(dev_preproc_data)\n",
    "            vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 1, 0, i, x, z, y]\n",
    "#             vect = TfidfVectorizer(max_features=i, stop_words=x, strip_accents=y, analyzer='word',lowercase=z)\n",
    "#             vect_train = vect.fit_transform(train_data)\n",
    "#             vect_dev = vect.transform(dev_data)\n",
    "#             vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 0, 0, i, x, z, y]\n",
    "            # Same but with the preprocessor\n",
    "#            vect = TfidfVectorizer(max_features=i, stop_words=x, analyzer='word',strip_accents=y, lowercase=z)\n",
    "\n",
    "# Note that this vectorizer is created with a passthru tokenizer(identity), no preprocessor and no lowercasing\n",
    "# This is to account for the NLTKPreprocessor already taking care of these.\n",
    "            vect = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=.7, max_features=i, analyzer='word',\n",
    "                              tokenizer=identity, preprocessor=None, lowercase=False, stop_words=x)\n",
    "#             vect_train= map(vect.fit_transform,train_preproc_data)\n",
    "#             vect_dev= map(vect.fit_transform,dev_preproc_data)\n",
    "            vect_train = vect.fit_transform(train_preproc_data)\n",
    "            vect_dev = vect.transform(dev_preproc_data)\n",
    "            vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 1, 0, i, x, z, y]\n",
    "\n",
    "print(str(datetime.datetime.now().time()))\n",
    "print(vectors_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1auc_all_models (vectors_all):\n",
    "    \"\"\"This function takes a vector of type vectors_all (defined above) acts as a wrapper\n",
    "    to send each vector to the score_f1_auc_on_train_dev function selecting first multinomialNB\n",
    "    and after that Bernoulli.  It collects the resulting scores and the best alpha and stores\n",
    "    the results in a dataframe which is returned once all the results are calculated\n",
    "    \n",
    "    Args:\n",
    "        Vectors_all (datafram) : a dataframe defined above that stores the vector data in each row\n",
    "    Returns\n",
    "        dataframe: A dataframe of all the resulting scores and the details for each model\n",
    "    \"\"\"\n",
    "    data_all=pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'f1dev','aucdev','f1train','auctrain'])\n",
    "    for index,row in vectors_all.iterrows():\n",
    "        for name in target_names:\n",
    "            alpha, tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(\n",
    "                train_vector=row['vectortrain'],\n",
    "                dev_vector=row[1],name=name, ctype='multi')\n",
    "            data_all.loc[data_all.shape[0]] = [index,name,'multi',alpha,row['type'], \n",
    "                                row['preprocessor'], row['tokenizer'], row['max_features'], \n",
    "                                row['stop_words'], row['lowercase'], row['strip_accents'],\n",
    "                                tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "            alpha, tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(\n",
    "                train_vector=row[0],\n",
    "                dev_vector=row[1],name=name, ctype='bern')\n",
    "            data_all.loc[data_all.shape[0]] = [index,name,'bern',alpha,row['type'], \n",
    "                                row['preprocessor'], row['tokenizer'], row['max_features'], \n",
    "                                row['stop_words'], row['lowercase'], row['strip_accents'],\n",
    "                                tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    print('done')\n",
    "    return data_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:50:26.156608\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fix_spellings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4d6a3abee4b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m count_vect_plain_pre_token10k = CountVectorizer(tokenizer=fix_spellings, max_features=10000, \n\u001b[0m\u001b[1;32m      7\u001b[0m                                                 strip_accents='ascii', lowercase=True)\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fix_spellings' is not defined"
     ]
    }
   ],
   "source": [
    "# separated out.  This took over 24 hours to run as the fix_spellings tokenizer\n",
    "# seems to have been very slow.  As an alternative we will preprocess our data\n",
    "# before generating the dataframe\n",
    "\n",
    "print(str(datetime.datetime.now().time()))\n",
    "count_vect_plain_pre_token10k = CountVectorizer(tokenizer=fix_spellings, max_features=10000, \n",
    "                                                strip_accents='ascii', lowercase=True)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "X_train_counts_plain_pre_token10k = count_vect_plain_pre_token6k.fit_transform(train_data)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "X_dev_counts_plain_pre_token10k = count_vect_plain_pre_token6k.transform(dev_data)\n",
    "print(str(datetime.datetime.now().time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_counts_plain_pre_token6k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6317a9d3429a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# insert the vectors from the previous cell into the large dataframe of vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m vectors_all.loc[vectors_all.shape[0]] = [X_train_counts_plain_pre_token6k, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                          \u001b[0mX_dev_counts_plain_pre_token6k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          'count', 0, 1, 10000, None, True, 'ascii']\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_counts_plain_pre_token6k' is not defined"
     ]
    }
   ],
   "source": [
    "# insert the vectors from the previous cell into the large dataframe of vectors\n",
    "vectors_all.loc[vectors_all.shape[0]] = [X_train_counts_plain_pre_token6k, \n",
    "                                         X_dev_counts_plain_pre_token6k,\n",
    "                                         'count', 0, 1, 10000, None, True, 'ascii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burgew/Library/Python/3.6/lib/python/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Users/burgew/Library/Python/3.6/lib/python/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Users/burgew/Library/Python/3.6/lib/python/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5f6b83e2bf3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# calculate the f1 and auc for all the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresultdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_f1auc_all_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2d6a4ab19355>\u001b[0m in \u001b[0;36mcalculate_f1auc_all_models\u001b[0;34m(vectors_all)\u001b[0m\n\u001b[1;32m     17\u001b[0m             alpha, tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(\n\u001b[1;32m     18\u001b[0m                 \u001b[0mtrain_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vectortrain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 dev_vector=row[1],name=name, ctype='multi')\n\u001b[0m\u001b[1;32m     20\u001b[0m             data_all.loc[data_all.shape[0]] = [index,name,'multi',alpha,row['type'], \n\u001b[1;32m     21\u001b[0m                                 \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-6ee2ded77829>\u001b[0m in \u001b[0;36mscore_f1_auc_on_train_dev\u001b[0;34m(dev_vector, train_vector, name, ctype)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mpredicted_labels_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \"\"\"\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_estimator_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    726\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "# Finally pull everything together and look for the top results for\n",
    "# both F1 and AUC scores\n",
    "\n",
    "top_aucf1_results = pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'f1dev','aucdev','f1train','auctrain'])\n",
    "\n",
    "# calculate the f1 and auc for all the models\n",
    "resultdf = calculate_f1auc_all_models(vectors_all)\n",
    "\n",
    "for label in target_names:\n",
    "    df_tmp = resultdf.loc[resultdf['label'] == label]\n",
    "    top_aucf1_results.loc[top_aucf1_results.shape[0]] = df_tmp.loc[df_tmp['aucdev'].idxmax()]\n",
    "    top_aucf1_results.loc[top_aucf1_results.shape[0]] = df_tmp.loc[df_tmp['f1dev'].idxmax()]\n",
    "    \n",
    "print(top_aucf1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:10:41.176595 starting\n",
      "done\n",
      "    vectorno          label  model            alpha   type  preprocessor  \\\n",
      "0         60          toxic   bern  {'alpha': 15.0}  count             0   \n",
      "1         86          toxic  multi   {'alpha': 0.5}  tfidf             0   \n",
      "2         40   severe_toxic   bern   {'alpha': 2.0}  count             0   \n",
      "3         10   severe_toxic  multi  {'alpha': 0.01}  tfidf             0   \n",
      "4         40        obscene   bern  {'alpha': 10.0}  count             0   \n",
      "5         94        obscene  multi   {'alpha': 0.5}  tfidf             0   \n",
      "6         32         threat   bern   {'alpha': 1.0}  count             0   \n",
      "7         54         threat  multi   {'alpha': 0.1}  tfidf             0   \n",
      "8         44         insult   bern  {'alpha': 10.0}  count             0   \n",
      "9        102         insult  multi   {'alpha': 0.1}  tfidf             0   \n",
      "10        60  identity_hate   bern   {'alpha': 1.0}  count             0   \n",
      "11       110  identity_hate  multi   {'alpha': 0.1}  tfidf             0   \n",
      "\n",
      "    tokenizer max_features stop_words lowercase strip_accents     f1dev  \\\n",
      "0           0         5000    english      True          None  0.904538   \n",
      "1           0         6000    english      True          None  0.950063   \n",
      "2           0         4000    english      True         ascii  0.953032   \n",
      "3           0         None       None      True       unicode  0.990590   \n",
      "4           0         4000    english      True         ascii  0.939335   \n",
      "5           0         6000    english      True       unicode  0.972919   \n",
      "6           0         4000       None      True       unicode  0.951276   \n",
      "7           0         5000       None      True         ascii  0.997407   \n",
      "8           0         4000    english      True       unicode  0.935048   \n",
      "9           0        10000       None      True         ascii  0.968737   \n",
      "10          0         5000    english      True          None  0.937432   \n",
      "11          0        10000    english      True          None  0.991593   \n",
      "\n",
      "      aucdev   f1train  auctrain  \n",
      "0   0.858012  0.905558  0.860328  \n",
      "1   0.762880  0.952788  0.772927  \n",
      "2   0.931754  0.952743  0.939355  \n",
      "3   0.662864  0.994183  0.839316  \n",
      "4   0.893749  0.939464  0.896084  \n",
      "5   0.780444  0.975535  0.801792  \n",
      "6   0.915132  0.950112  0.921677  \n",
      "7   0.560522  0.997101  0.582213  \n",
      "8   0.871067  0.936117  0.878791  \n",
      "9   0.743943  0.971884  0.775224  \n",
      "10  0.882604  0.938435  0.919297  \n",
      "11  0.608765  0.992045  0.674801  \n",
      "09:59:04.318233 ending\n"
     ]
    }
   ],
   "source": [
    "# copy and paste from the previous section as I don't want to overwrite the results there\n",
    "\n",
    "top_aucf1_results = pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'f1dev','aucdev','f1train','auctrain'])\n",
    "print('%s starting' %(str(datetime.datetime.now().time())))\n",
    "# calculate the f1 and auc for all the models\n",
    "resultdf = calculate_f1auc_all_models(vectors_all)\n",
    "\n",
    "for label in target_names:\n",
    "    df_tmp = resultdf.loc[resultdf['label'] == label]\n",
    "    top_aucf1_results.loc[top_aucf1_results.shape[0]] = df_tmp.loc[df_tmp['aucdev'].idxmax()]\n",
    "    top_aucf1_results.loc[top_aucf1_results.shape[0]] = df_tmp.loc[df_tmp['f1dev'].idxmax()]\n",
    "    \n",
    "print(top_aucf1_results)\n",
    "print('%s ending' %(str(datetime.datetime.now().time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:47:22.163790 starting\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7d3643952ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s starting'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# calculate the f1 and auc for all the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresultdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_f1auc_all_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-2d6a4ab19355>\u001b[0m in \u001b[0;36mcalculate_f1auc_all_models\u001b[0;34m(vectors_all)\u001b[0m\n\u001b[1;32m     17\u001b[0m             alpha, tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(\n\u001b[1;32m     18\u001b[0m                 \u001b[0mtrain_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vectortrain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 dev_vector=row[1],name=name, ctype='multi')\n\u001b[0m\u001b[1;32m     20\u001b[0m             data_all.loc[data_all.shape[0]] = [index,name,'multi',alpha,row['type'], \n\u001b[1;32m     21\u001b[0m                                 \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocessor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-6ee2ded77829>\u001b[0m in \u001b[0;36mscore_f1_auc_on_train_dev\u001b[0;34m(dev_vector, train_vector, name, ctype)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mctype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'multi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mnb_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mctype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bern'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnb_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \"\"\"\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# make sure we actually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'map'"
     ]
    }
   ],
   "source": [
    "# copy and paste from the previous section as I don't want to overwrite the results there\n",
    "\n",
    "top_aucf1_results = pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'f1dev','aucdev','f1train','auctrain'])\n",
    "print('%s starting' %(str(datetime.datetime.now().time())))\n",
    "# calculate the f1 and auc for all the models\n",
    "resultdf = calculate_f1auc_all_models(vectors_all)\n",
    "\n",
    "for label in target_names:\n",
    "    df_tmp = resultdf.loc[resultdf['label'] == label]\n",
    "    top_aucf1_results.loc[top_aucf1_results.shape[0]] = df_tmp.loc[df_tmp['aucdev'].idxmax()]\n",
    "    top_aucf1_results.loc[top_aucf1_results.shape[0]] = df_tmp.loc[df_tmp['f1dev'].idxmax()]\n",
    "    \n",
    "print(top_aucf1_results)\n",
    "print('%s ending' %(str(datetime.datetime.now().time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_aucf1_results).to_csv('f1auc_scores.csv')\n",
    "pd.DataFrame(resultdf).to_csv('all_NB_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
