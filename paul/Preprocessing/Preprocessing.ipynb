{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../../data/test.csv\")\n",
    "train_data = train_df[\"comment_text\"]\n",
    "\n",
    "# starting from the /usr/share/dict/words set of half a million words with abbreviations\n",
    "# and others, and adding to them any obvious missing values\n",
    "filename = 'words.txt'\n",
    "file = open(filename, 'rt')\n",
    "all_words = file.read()\n",
    "file.close\n",
    "all_words = all_words.split()\n",
    "all_words = [ word.lower() for word in all_words]\n",
    "all_words = set(all_words)\n",
    "\n",
    "from nltk.corpus import brown\n",
    "good_words_list = brown.words()\n",
    "good_word_set = set([word.lower() for word in good_words_list])\n",
    "#punctuation = re.sub(\"[\\'\\-]\",'',string.punctuation)\n",
    "punctuation = \"[\\!\\?\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\,\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\\']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "\n",
    "def find_spelling_errors(textdoc):\n",
    "    words = textdoc.split()\n",
    "    words = [ word.lower() for word in words]\n",
    "    misspelled_words = []\n",
    "    misspelled_words.append([re.sub(punctuation,'',word) for word in words if re.sub(punctuation,'',word) not in all_words])\n",
    "    return misspelled_words\n",
    "\n",
    "def trysplit(word):\n",
    "    lentmp = len(word)\n",
    "    return_words=[]\n",
    "    for i in range(lentmp):\n",
    "        if all_words.intersection(set([word[:lentmp-i].lower()])):\n",
    "            return_words.append(word[:lentmp-i])\n",
    "            if len(word[lentmp-i:]) > 0:\n",
    "                return_words.extend(trysplit(word[lentmp-i:]))\n",
    "                print(return_words)\n",
    "            else:\n",
    "                return [word[:lentmp-i]]\n",
    "            break\n",
    "    return return_words \n",
    "\n",
    "def fix_spelling_errors(textdoc):\n",
    "    words = textdoc.split()\n",
    "    return_list = []\n",
    "    for word in words:\n",
    "        \n",
    "        if all_words.intersection(set([re.sub(punctuation,'',word.lower())])):\n",
    "            return_list.extend(word)\n",
    "        else:\n",
    "            # word is not found in the dictionary, try to correct the spelling\n",
    "            if word == spell(word): # no changes made by the spell checker\n",
    "                return_list.extend(trysplit(word))\n",
    "            else:\n",
    "                return_list.extend(spell(word))\n",
    "    return return_list\n",
    "    #return [ spell(word.lower()) if re.sub(punctuation,'',word.lower()) not in all_words else word for word in words]\n",
    "    #return [spell(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169\n",
      "Invalid spellings: [['articlewow', '93161107169']]\n",
      "['article']\n",
      "['S', 'o', 'r', 'r', 'y', 'i', 'f', 't', 'h', 'e', 'w', 'o', 'r', 'd', \"'\", 'n', 'o', 'n', 's', 'e', 'n', 's', 'e', \"'\", 'w', 'a', 's', 'o', 'f', 'f', 'e', 'n', 's', 'i', 'v', 'e', 't', 'o', 'y', 'o', 'u', '.', 'A', 'n', 'y', 'w', 'a', 'y', ',', 'I', \"'\", 'm', 'n', 'o', 't', 'i', 'n', 't', 'e', 'n', 'd', 'i', 'n', 'g', 't', 'o', 'w', 'r', 'i', 't', 'e', 'a', 'n', 'y', 't', 'h', 'i', 'n', 'g', 'i', 'n', 't', 'h', 'e', 'article', 't', 'h', 'e', 'y', 'w', 'o', 'u', 'l', 'd', 'j', 'u', 'm', 'p', 'o', 'n', 'm', 'e', 'f', 'o', 'r', 'v', 'a', 'n', 'd', 'a', 'l', 'i', 's', 'm', ')', ',', 'I', \"'\", 'm', 'm', 'e', 'r', 'e', 'l', 'y', 'r', 'e', 'q', 'u', 'e', 's', 't', 'i', 'n', 'g', 't', 'h', 'a', 't', 'i', 't', 'b', 'e', 'm', 'o', 'r', 'e', 'e', 'n', 'c', 'y', 'c', 'l', 'o', 'p', 'e', 'd', 'i', 'c', 's', 'o', 'o', 'n', 'e', 'c', 'a', 'n', 'u', 's', 'e', 'i', 't', 'f', 'o', 'r', 's', 'c', 'h', 'o', 'o', 'l', 'a', 's', 'a', 'r', 'e', 'f', 'e', 'r', 'e', 'n', 'c', 'e', '.', 'I', 'h', 'a', 'v', 'e', 'b', 'e', 'e', 'n', 't', 'o', 't', 'h', 'e', 's', 'e', 'l', 'e', 'c', 't', 'i', 'v', 'e', 'b', 'r', 'e', 'e', 'd', 'i', 'n', 'g', 'p', 'a', 'g', 'e', 'b', 'u', 't', 'i', 't', \"'\", 's', 'a', 'l', 'm', 'o', 's', 't', 'a', 's', 't', 'u', 'b', '.', 'I', 't', 'p', 'o', 'i', 'n', 't', 's', 't', 'o', \"'\", 'a', 'n', 'i', 'm', 'a', 'l', 'b', 'r', 'e', 'e', 'd', 'i', 'n', 'g', \"'\", 'w', 'h', 'i', 'c', 'h', 'i', 's', 'a', 's', 'h', 'o', 'r', 't', 'm', 'e', 's', 's', 'y', 'a', 'r', 't', 'i', 'c', 'l', 'e', 't', 'h', 'a', 't', 'g', 'i', 'v', 'e', 's', 'y', 'o', 'u', 'n', 'o', 'i', 'n', 'f', 'o', '.', 'T', 'h', 'e', 'r', 'e', 'm', 'u', 's', 't', 'b', 'e', 's', 'o', 'm', 'e', 'o', 'n', 'e', 'a', 'r', 'o', 'u', 'n', 'd', 'w', 'i', 't', 'h', 'e', 'x', 'p', 'e', 'r', 't', 'i', 's', 'e', 'i', 'n', 'e', 'u', 'g', 'e', 'n', 'i', 'c', 's', '?']\n"
     ]
    }
   ],
   "source": [
    "# find all the spelling errors\n",
    "spelling_errors=[]\n",
    "for index in range(1):\n",
    "    spelling_errors.append(find_spelling_errors(train_data[index]))\n",
    "\n",
    "flat_list = []\n",
    "for sublist in spelling_errors:\n",
    "    for sublist2 in sublist:\n",
    "        for item in sublist2:\n",
    "            flat_list.append(item)\n",
    "            \n",
    "# y = np.bincount(np.unique(flat_list))\n",
    "# ii = np.nonzero(y)[0]\n",
    "# out = np.vstack((ii, y[ii])).T\n",
    "import collections\n",
    "collections.Counter(flat_list)\n",
    "# print(np.unique(flat_list, return_counts=True))\n",
    "# print(flat_list)\n",
    "x=8\n",
    "print(train_data[x])\n",
    "print('Invalid spellings: %s' % (find_spelling_errors(train_data[x])))\n",
    "print(fix_spelling_errors(train_data[x]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word incoming is :ajdlfjaljsdf\n",
      "Checking ajdlfjaljsdf\n",
      "Checking ajdlfjaljsd\n",
      "Checking ajdlfjaljs\n",
      "Checking ajdlfjalj\n",
      "Checking ajdlfjal\n",
      "Checking ajdlfja\n",
      "Checking ajdlfj\n",
      "Checking ajdlf\n",
      "Checking ajdl\n",
      "Checking ajd\n",
      "Checking aj\n",
      "Found word:aj\n",
      "word incoming is :dlfjaljsdf\n",
      "Checking dlfjaljsdf\n",
      "Checking dlfjaljsd\n",
      "Checking dlfjaljs\n",
      "Checking dlfjalj\n",
      "Checking dlfjal\n",
      "Checking dlfja\n",
      "Checking dlfj\n",
      "Checking dlf\n",
      "Checking dl\n",
      "Found word:dl\n",
      "word incoming is :fjaljsdf\n",
      "Checking fjaljsdf\n",
      "Checking fjaljsd\n",
      "Checking fjaljs\n",
      "Checking fjalj\n",
      "Checking fjal\n",
      "Checking fja\n",
      "Checking fj\n",
      "Checking f\n",
      "Found word:f\n",
      "word incoming is :jaljsdf\n",
      "Checking jaljsdf\n",
      "Checking jaljsd\n",
      "Checking jaljs\n",
      "Checking jalj\n",
      "Checking jal\n",
      "Found word:jal\n",
      "word incoming is :jsdf\n",
      "Checking jsdf\n",
      "Checking jsd\n",
      "Found word:jsd\n",
      "word incoming is :f\n",
      "Checking f\n",
      "Found word:f\n",
      "Returning last word f\n",
      "Current return words are: \n",
      "['jsd', 'f']\n",
      "Current return words are: \n",
      "['jal', 'jsd', 'f']\n",
      "Current return words are: \n",
      "['f', 'jal', 'jsd', 'f']\n",
      "Current return words are: \n",
      "['dl', 'f', 'jal', 'jsd', 'f']\n",
      "Current return words are: \n",
      "['aj', 'dl', 'f', 'jal', 'jsd', 'f']\n",
      "['aj', 'dl', 'f', 'jal', 'jsd', 'f']\n"
     ]
    }
   ],
   "source": [
    "def trysplit_orig(word):\n",
    "    lentmp = len(word)\n",
    "    return_words=[]\n",
    "    print(\"word incoming is :\" + word)\n",
    "    for i in range(lentmp):\n",
    "        print(\"Checking \" + word[:lentmp-i])\n",
    "        if all_words.intersection(set([word[:lentmp-i].lower()])):\n",
    "            print(\"Found word:\" + word[:lentmp-i] )\n",
    "            return_words.append(word[:lentmp-i])\n",
    "            if len(word[lentmp-i:]) > 0:\n",
    "                return_words.extend(trysplit(word[lentmp-i:]))\n",
    "                print(\"Current return words are: \")\n",
    "                print(return_words)\n",
    "            else:\n",
    "                print(\"Returning last word \" + word[:lentmp-i])\n",
    "                return [word[:lentmp-i]]\n",
    "            break\n",
    "    return return_words   \n",
    "    \n",
    "print(trysplit('ajdlfjaljsdf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fuck'}\n"
     ]
    }
   ],
   "source": [
    "myword=\"fuck\"\n",
    "if all_words.intersection(set([myword])):\n",
    "    print(all_words.intersection(set([myword])))\n",
    "#print(all_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no match for the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ism'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=\"the\"\n",
    "if x == spell(x):\n",
    "    print(\"no match for \" + x)\n",
    "    \n",
    "spell(\"I'm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
