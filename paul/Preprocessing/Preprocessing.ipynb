{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../../data/test.csv\")\n",
    "# train_data = train_df[\"comment_text\"]\n",
    "# train_labels = train_df[target_names]\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.8\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "# starting from the /usr/share/dict/words set of half a million words with abbreviations\n",
    "# and others, and adding to them any obvious missing values\n",
    "filename = 'words.txt'\n",
    "file = open(filename, 'rt')\n",
    "all_words = file.read()\n",
    "file.close\n",
    "all_words = all_words.split()\n",
    "all_words = [ word.lower() for word in all_words]\n",
    "all_words = set(all_words)\n",
    "\n",
    "from nltk.corpus import brown\n",
    "good_words_list = brown.words()\n",
    "good_word_set = set([word.lower() for word in good_words_list])\n",
    "#punctuation = re.sub(\"[\\'\\-]\",'',string.punctuation)\n",
    "punctuation = \"[\\!\\?\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\,\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\\']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "\n",
    "def find_spelling_errors(textdoc):\n",
    "    words = textdoc.split()\n",
    "    words = [ word.lower() for word in words]\n",
    "    misspelled_words = []\n",
    "    misspelled_words.append([re.sub(punctuation,'',word) for word in words if re.sub(punctuation,'',word) not in large_word_corpus])\n",
    "    return misspelled_words\n",
    "\n",
    "def trysplit(word):\n",
    "    lentmp = len(word)\n",
    "    return_words=[]\n",
    "    for i in range(lentmp):\n",
    "        if large_word_corpus.intersection(set([word[:lentmp-i].lower()])):\n",
    "            return_words.append(word[:lentmp-i])\n",
    "            if len(word[lentmp-i:]) > 0:\n",
    "                return_words.extend(trysplit(word[lentmp-i:]))\n",
    "            else:\n",
    "                return [word[:lentmp-i]]\n",
    "            break\n",
    "    return return_words \n",
    "\n",
    "def fix_spelling_errors(textdoc):\n",
    "    words = textdoc.split()\n",
    "    return_list = []\n",
    "    for word in words:\n",
    "        if all_words.intersection(set([re.sub(punctuation,'',word.lower())])):\n",
    "            return_list.append(word)\n",
    "        else:\n",
    "            # word is not found in the dictionary, try to correct the spelling\n",
    "            if word == spell(word): # no changes made by the spell checker\n",
    "                return_list.extend(trysplit(word))\n",
    "            else:\n",
    "                return_list.append(spell(word))\n",
    "    return return_list\n",
    "    #return [ spell(word.lower()) if re.sub(punctuation,'',word.lower()) not in all_words else word for word in words]\n",
    "    #return [spell(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169\n",
      "Invalid spellings: [['articlewow', '93161107169']]\n",
      "['Sorry', 'if', 'the', 'word', \"'nonsense'\", 'was', 'offensive', 'to', 'you.', 'Anyway,', \"I'm\", 'not', 'intending', 'to', 'write', 'anything', 'in', 'the', 'article', 'they', 'would', 'jump', 'on', 'me', 'for', 'vandalism),', \"I'm\", 'merely', 'requesting', 'that', 'it', 'be', 'more', 'encyclopedic', 'so', 'one', 'can', 'use', 'it', 'for', 'school', 'as', 'a', 'reference.', 'I', 'have', 'been', 'to', 'the', 'selective', 'breeding', 'page', 'but', \"it's\", 'almost', 'a', 'stub.', 'It', 'points', 'to', \"'animal\", \"breeding'\", 'which', 'is', 'a', 'short', 'messy', 'article', 'that', 'gives', 'you', 'no', 'info.', 'There', 'must', 'be', 'someone', 'around', 'with', 'expertise', 'in', 'eugenics?']\n"
     ]
    }
   ],
   "source": [
    "# find all the spelling errors\n",
    "spelling_errors=[]\n",
    "for index in range(1):\n",
    "    spelling_errors.append(find_spelling_errors(train_data[index]))\n",
    "\n",
    "flat_list = []\n",
    "for sublist in spelling_errors:\n",
    "    for sublist2 in sublist:\n",
    "        for item in sublist2:\n",
    "            flat_list.append(item)\n",
    "            \n",
    "# y = np.bincount(np.unique(flat_list))\n",
    "# ii = np.nonzero(y)[0]\n",
    "# out = np.vstack((ii, y[ii])).T\n",
    "import collections\n",
    "collections.Counter(flat_list)\n",
    "# print(np.unique(flat_list, return_counts=True))\n",
    "# print(flat_list)\n",
    "x=8\n",
    "print(train_data[x])\n",
    "print('Invalid spellings: %s' % (find_spelling_errors(train_data[x])))\n",
    "print(fix_spelling_errors(train_data[x]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value from the input set alpha is:  {'alpha': 10.0}\n",
      "The F1 score for MultinomialNB with alpha {'alpha': 10.0} is 0.879169\n",
      "The best value from the input set alpha is:  {'alpha': 10.0}\n",
      "The F1 score for BernoulliNB with alpha {'alpha': 10.0} is 0.641099\n"
     ]
    }
   ],
   "source": [
    "def bernoulli_toxic_labels (label_vector):\n",
    "    return [1 if (label_vector['toxic'][x] + label_vector['severe_toxic'][x] +\n",
    "                label_vector['obscene'][x] + label_vector['threat'][x] +\n",
    "                label_vector['insult'][x] + label_vector['identity_hate'][x]) > 0 else 0 \n",
    "            for x in label_vector.index.values]\n",
    "\n",
    "def binarize_toxic_labels (label_vector):\n",
    "    return [(label_vector['toxic'][x]*32 + label_vector['severe_toxic'][x]*16 +\n",
    "                label_vector['obscene'][x]*8 + label_vector['threat'][x]*4 +\n",
    "                label_vector['insult'][x]*2 + label_vector['identity_hate'][x]) \n",
    "            for x in label_vector.index.values]\n",
    "\n",
    "# Need to convert the training labels, either a 1 to indicate that some value was set\n",
    "# or use the binary and create a 64 value training lables.  Eventually should split off\n",
    "# one for each and have a separate classifier\n",
    "\n",
    "\n",
    "binary_train_labels = binarize_toxic_labels(train_labels)\n",
    "binary_dev_labels = binarize_toxic_labels(dev_labels)\n",
    "\n",
    "count_vect = CountVectorizer(max_features=5000,stop_words='english')\n",
    "tfidf_vect = TfidfVectorizer(max_features=5000,stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(train_data)\n",
    "X_dev_counts = count_vect.transform(dev_data)\n",
    "X_train_tfidf = tfidf_vect.fit_transform(train_data)\n",
    "X_dev_tfidf = tfidf_vect.transform(dev_data)\n",
    "\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "\n",
    "# -- Step 2 train a multinomial Bayes and find the optimal alpha --\n",
    "multinomial_nb_class = MultinomialNB().fit(X_train_counts, binary_train_labels)\n",
    "clf = GridSearchCV(multinomial_nb_class, param_grid = alphas)\n",
    "clf.fit(X_train_counts, binary_train_labels)\n",
    "print(\"The best value from the input set alpha is: \", clf.best_params_)\n",
    "# I think get the f1 score for this alpha\n",
    "predicted_labels = multinomial_nb_class.predict(X_dev_counts)\n",
    "print('The F1 score for MultinomialNB with alpha %s is %f' \n",
    "       % (clf.best_params_, metrics.f1_score(binary_dev_labels,predicted_labels, average='micro')))\n",
    "\n",
    "\n",
    "bernoulli_nb_class = BernoulliNB().fit(X_train_counts, binary_train_labels)\n",
    "clf = GridSearchCV(bernoulli_nb_class, param_grid = alphas)\n",
    "clf.fit(X_train_counts, binary_train_labels)\n",
    "print(\"The best value from the input set alpha is: \", clf.best_params_)\n",
    "# I think get the f1 score for this alpha\n",
    "predicted_labels = bernoulli_nb_class.predict(X_dev_counts)\n",
    "print('The F1 score for BernoulliNB with alpha %s is %f' \n",
    "       % (clf.best_params_, metrics.f1_score(binary_dev_labels,predicted_labels, average='micro')))\n",
    "\n",
    "\n",
    "# gaussian_nb_class = GaussianNB().fit(X_train_counts, binary_train_labels)\n",
    "# clf = GridSearchCV(gaussian_nb_class, param_grid = alphas)\n",
    "# clf.fit(X_train_counts, binary_train_labels)\n",
    "# print(\"The best value from the input set alpha is: \", clf.best_params_)\n",
    "# # I think get the f1 score for this alpha\n",
    "# predicted_labels = gaussian_nb_class.predict(X_dev_counts)\n",
    "# print('The F1 score for GaussianNB with alpha %s is %f' \n",
    "#        % (clf.best_params_, metrics.f1_score(binary_dev_labels,predicted_labels, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score for train MultinomialNB toxic is 0.951274\n",
      "Train score for class toxic is 0.842156401391639\n",
      "The F1 score for dev MultinomialNB toxic is 0.946908\n",
      "Dev score for class toxic is 0.812182953651046\n",
      "\n",
      "The F1 score for train MultinomialNB severe_toxic is 0.987213\n",
      "Train score for class severe_toxic is 0.7571986548632726\n",
      "The F1 score for dev MultinomialNB severe_toxic is 0.987726\n",
      "Dev score for class severe_toxic is 0.7044201879954483\n",
      "\n",
      "The F1 score for train MultinomialNB obscene is 0.968099\n",
      "Train score for class obscene is 0.8353591391246766\n",
      "The F1 score for dev MultinomialNB obscene is 0.966802\n",
      "Dev score for class obscene is 0.803674443269784\n",
      "\n",
      "The F1 score for train MultinomialNB threat is 0.995704\n",
      "Train score for class threat is 0.5282727952345427\n",
      "The F1 score for dev MultinomialNB threat is 0.996471\n",
      "Dev score for class threat is 0.5249549155285191\n",
      "\n",
      "The F1 score for train MultinomialNB insult is 0.965449\n",
      "Train score for class insult is 0.8118920284726971\n",
      "The F1 score for dev MultinomialNB insult is 0.963523\n",
      "Dev score for class insult is 0.7729273839140397\n",
      "\n",
      "The F1 score for train MultinomialNB identity_hate is 0.988303\n",
      "Train score for class identity_hate is 0.6052019668611597\n",
      "The F1 score for dev MultinomialNB identity_hate is 0.988663\n",
      "Dev score for class identity_hate is 0.5593778632581373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name in target_names:\n",
    "    multinomial_nb_class = MultinomialNB().fit(X_train_counts, train_labels[name])\n",
    "    predicted_labels_dev = multinomial_nb_class.predict(X_dev_counts)\n",
    "    predicted_labels_train = multinomial_nb_class.predict(X_train_counts)\n",
    "    print('The F1 score for train MultinomialNB %s is %f' \n",
    "       % (name, metrics.f1_score(train_labels[name],predicted_labels_train, average='micro')))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(train_labels[name], predicted_labels_train)\n",
    "    print('Train score for class {} is {}'.format(name, metrics.auc(fpr,tpr))) \n",
    "    print('The F1 score for dev MultinomialNB %s is %f' \n",
    "       % (name, metrics.f1_score(dev_labels[name],predicted_labels_dev, average='micro')))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dev_labels[name], predicted_labels_dev)\n",
    "    print('Dev score for class {} is {}\\n'.format(name, metrics.auc(fpr,tpr)))\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
