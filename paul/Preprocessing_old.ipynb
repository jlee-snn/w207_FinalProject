{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 159571\n",
      "training data shape: (111829,)\n",
      "training label shape: (111829, 6)\n",
      "dev label shape: (47742, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "tiny_data,small_data = train_df[:200][\"comment_text\"],train_df[:1000][\"comment_text\"]\n",
    "tiny_labels,small_labels = train_df[:200][target_names],train_df[:1000][target_names]\n",
    "\n",
    "\n",
    "print('total training observations:', train_df.shape[0])\n",
    "print('training data shape:', train_data.shape)\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print ('labels names:', target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus 462986\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "word_corpus = '../data/words.txt'\n",
    "word_file = open(word_corpus, 'rt')\n",
    "large_word_corpus = word_file.read()\n",
    "word_file.close\n",
    "large_word_corpus = large_word_corpus.split()\n",
    "large_word_corpus = [ word.lower() for word in large_word_corpus]\n",
    "large_word_corpus = set(large_word_corpus)\n",
    "\n",
    "good_words_list = brown.words()\n",
    "good_word_set = set([word.lower() for word in good_words_list])\n",
    "#punctuation = re.sub(\"[\\'\\-]\",'',string.punctuation)\n",
    "punctuation = \"[\\!\\?\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\,\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\\']\"\n",
    "print('Size of corpus ' + str(len(large_word_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://norvig.com/spell-correct.html\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../data/big.txt').read()))\n",
    "\n",
    "def norvig_P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def norvig_correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(norvig_candidates(word), key=norvig_P)\n",
    "\n",
    "def norvig_candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to support finding and correcting spellings\n",
    "# using pyenchant for spell checking\n",
    "from enchant import DictWithPWL\n",
    "from enchant.checker import SpellChecker\n",
    "import difflib\n",
    "# import splitter # not useful, does a worse job than my implementation\n",
    "\n",
    "# mywords.txt currently contains:\n",
    "# - list of firstnames and surnames gathered from internet searches\n",
    "# http://www.birkenhoerdt.net/surnames-all.php?tree=1\n",
    "my_dict=DictWithPWL('en_US', \"../data/mywords.txt\")\n",
    "my_checker = SpellChecker(my_dict)\n",
    "\n",
    "# list of swear words correctly spelt courtesy of https://www.noswearing.com/\n",
    "\n",
    "def my_preprocessor(textblock):\n",
    "    # u -> you\n",
    "    # c -> see\n",
    "    # k -> okay\n",
    "    return_words = textblock\n",
    "\n",
    "#     return_words = re.sub(r\"[^A-Za-z0-9,!?*.;’´'\\/]\", \" \", return_words)\n",
    "    return_words = re.sub(r\"[^A-Za-z0-9]\", \" \", return_words)\n",
    "    return_words = re.sub(r\",\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\.\\.+\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\.\",\" \",return_words)\n",
    "    return_words = re.sub(r\"\\(\",\" \", return_words)\n",
    "    return_words = re.sub(r\"\\)\",\" \", return_words)\n",
    "    return_works = re.sub(r\"\\;\", \" \", return_words)\n",
    "    return_words = re.sub(r\":\",\" \", return_words)\n",
    "    return_words = re.sub(r\"´\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"`\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"''+\", \"'\", return_words)\n",
    "    return_words = re.sub(r\" '\", \" \", return_words)\n",
    "    return_words = re.sub(r\"' \", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\\"\", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\/\", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\!\\!+\", \"!!\", return_words)\n",
    "    return_words = re.sub(r\"\\?\\?+\", \"?!\", return_words)\n",
    "    return_words = re.sub(r\"\\!\", \" !\", return_words)\n",
    "    return_words = re.sub(r\"\\?\", \" ?\", return_words)\n",
    "    return_words = re.sub(r\"\\b\\d+\\b\", \"999\", return_words)\n",
    "    # slang and abbreviations, need to be aware of capitolization and spaces\n",
    "    return_words = re.sub(r\"[Ww]on't\", \"will not\", return_words)\n",
    "    return_words = re.sub(r\"n't\", \" not\", return_words)\n",
    "    return_words = re.sub(r\"'s\\b\", \" is\", return_words)\n",
    "    return_words = re.sub(r\"\\b[Aa]bt\\b\", \"about\", return_words)\n",
    "    return return_words\n",
    "\n",
    "def trysplit(word, verbose=False):\n",
    "    split_candidates = []\n",
    "    max_proba = 0.0\n",
    "    for i in range(1,len(word)):\n",
    "        # I will only allow single letters of 'a' and 'i', all others ignored.  Pyenchant allows for\n",
    "        # any single letter to be a legitimate word, and so too does norvig.  The dictionary defines\n",
    "        # them as nouns that represent the letter, however even though several can be used in slang\n",
    "        # (e.g. k->okay, c->see, u->you) using them in conjoined words would make the splitting far\n",
    "        # too difficult and also human understanding much more difficult #howucthisk, u c?\n",
    "        if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "            len(word[i:]) != 1 or (word[i:].lower() == 'a' or word[i:].lower() == 'i')):\n",
    "            if my_checker.check(word[:i]) and my_checker.check(word[i:]):\n",
    "                norvig_score = norvig_P(word[:i]) + norvig_P(word[i:])\n",
    "                if norvig_score > max_proba:\n",
    "                    max_proba = norvig_score\n",
    "                    split_candidates = [word[:i],word[i:]]\n",
    "    for i in range(1,len(word)):\n",
    "        for j in range(i+1,len(word)):        \n",
    "            if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "                len(word[i:j]) != 1 or (word[i:j].lower() == 'a' or word[i:j].lower() == 'i')) and (\n",
    "                len(word[i:]) != 1 or (word[i:].lower() == 'a' or word[i:].lower() == 'i')):\n",
    "                \n",
    "                if my_checker.check(word[:i]) and my_checker.check(word[i:j]) and my_checker.check(word[j:]):\n",
    "                    norvig_score = norvig_P(word[:i]) + norvig_P(word[i:j]) + norvig_P(word[j:])\n",
    "                    if norvig_score > max_proba:\n",
    "                        max_proba = norvig_score\n",
    "                        split_candidates = [word[:i],word[i:j],word[j:]]\n",
    "    for i in range(1,len(word)):\n",
    "        for j in range(i+1,len(word)):\n",
    "            for k in range(j+1,len(word)):\n",
    "                if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "                    len(word[i:j]) != 1 or (word[i:j].lower() == 'a' or word[i:j].lower() == 'i')) and (\n",
    "                    len(word[j:k]) != 1 or (word[j:k].lower() == 'a' or word[j:k].lower() == 'i')) and (\n",
    "                    len(word[k:]) != 1 or (word[k:].lower() == 'a' or word[k:].lower() == 'i')):\n",
    "                    verbose and print(\"making it here with i=%s j=%s k=%s %s  max_proba=%d\" %(word[:i],word[i:j],word[j:k],word[k:], max_proba))\n",
    "                    verbose and print(\"lengths are %d %d %d %d\" % (len(word[:i]), len(word[i:j]),len(word[j:k]),len(word[k:])))\n",
    "                    if my_checker.check(word[:i]) and my_checker.check(word[i:j]) and my_checker.check(word[j:k]) and my_checker.check(word[k:]):\n",
    "                        verbose and print('found words ' + word[i:] + ' ' + word[k:])\n",
    "                        norvig_score = norvig_P(word[:i]) + norvig_P(word[i:j]) + norvig_P(word[j:k]) + norvig_P(word[k:])\n",
    "                        if norvig_score > max_proba:\n",
    "                            verbose and print(\"found higher probability %d with %s %s %s %s\" % (norvig_score, word[:i], word[i:j], word[j:k], word[k:]))\n",
    "                            max_proba = norvig_score\n",
    "                            split_candidates = [word[:i],word[i:j],word[j:k],word[k:]]\n",
    "    return split_candidates\n",
    "\n",
    "def get_best_candidates(word):\n",
    "    best_words = []\n",
    "    best_ratio = 0\n",
    "    a = set(my_checker.suggest(word))\n",
    "    for b in a:\n",
    "        if not '-' in b:\n",
    "            tmp = difflib.SequenceMatcher(None, word, b).ratio()\n",
    "            if tmp > best_ratio:\n",
    "                best_words=[b]\n",
    "                best_ratio=tmp\n",
    "            elif tmp == best_ratio:\n",
    "                best_words.append(b)\n",
    "    return best_words\n",
    "    \n",
    "def fix_spellings(textinput, verbose=False):\n",
    "    words = textinput.split()\n",
    "    return_list = []\n",
    "    for word in words:\n",
    "        if my_checker.check(word) or my_checker.check(word.lower()) or word in punctuation or\\\n",
    "            any(i.isdigit() for i in word) or (word[-1].lower() == 's' and my_checker.check(word[:-1].lower())):\n",
    "            return_list.append(word)\n",
    "            # continue\n",
    "        else:            \n",
    "            candidates = get_best_candidates(word)\n",
    "            if len(candidates) == 1:\n",
    "                return_list.append(candidates.pop())\n",
    "            elif len(candidates) > 1:\n",
    "                # try another spell checker\n",
    "                nv_candidates = norvig_candidates(word)\n",
    "                tmp_set = set(nv_candidates).intersection(set(candidates))\n",
    "                if len(tmp_set) == 1:\n",
    "                    # only 1 overlap, should be correct\n",
    "                    return_list.append(tmp_set.pop())\n",
    "                elif len(nv_candidates) == 1 and next(iter(nv_candidates)) == word:\n",
    "                        # this is suspicious, pyenchants' \"suggest\" method always returns something, however if\n",
    "                        # norvigs method cannot find a suitable match within a short distance then it simply\n",
    "                        # returns the orignal word.  This section is for potentially conjoined words\n",
    "                        tmp_list=trysplit(word)\n",
    "\n",
    "                        # If we get back a list of split words then use these\n",
    "                        if len(tmp_list) != 0:\n",
    "                            return_list.extend(tmp_list)\n",
    "                            continue\n",
    "                else:\n",
    "                    # arbitrary now, just going to use the first one found from pyenchant, even though\n",
    "                    # I have seen norvig get the correct word sometimes when pyenchant gets it wrong\n",
    "                    return_list.append(candidates[0])\n",
    "    return return_list\n",
    "\n",
    "# myword='In a long discussion about thisismessedup what should I do askd'\n",
    "# print(fix_spellings(myword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my love']\n",
      "False\n",
      "['my love', 'my-love', 'ladylove', 'Mylo', 'lovely', 'Lovejoy', 'Melville', 'Malvin', 'mylo', 'malone', 'milone', 'love']\n",
      "{'move', 'glove', 'love'}\n",
      "['my love']\n"
     ]
    }
   ],
   "source": [
    "myword2='mylove'\n",
    "# trysplit(myword)\n",
    "#norvig_P('is')\n",
    "#fix_spellings('alit')\n",
    "# help(enchant)\n",
    "#myword2=\"I'm\"\n",
    "print(get_best_candidates(myword2))\n",
    "print(my_checker.check(myword2))\n",
    "print(my_checker.suggest(myword2))\n",
    "print(norvig_candidates(myword2))\n",
    "print(fix_spellings(myword2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Sorry \n",
      "\n",
      "I'm sorry I screwed around with someones talk page.  It was very bad to do.  I know how having the templates on their talk page helps you assert your dominance over them.  I know I should bow down to the almighty administrators.  But then again, I'm going to go play outside....with your mom.   76.122.79.82\n",
      "[\"I'm\", 'Sorry', \"I'm\", 'sorry', 'I', 'screwed', 'around', 'with', 'someones', 'talk', 'page', 'It', 'was', 'very', 'bad', 'to', 'do', 'I', 'know', 'how', 'having', 'the', 'templates', 'on', 'their', 'talk', 'page', 'helps', 'you', 'assert', 'your', 'dominance', 'over', 'them', 'I', 'know', 'I', 'should', 'bow', 'down', 'to', 'the', 'almighty', 'administrators', 'But', 'then', 'again', \"I'm\", 'going', 'to', 'go', 'play', 'outside', 'with', 'your', 'mom', '999', '999', '999', '999']\n"
     ]
    }
   ],
   "source": [
    "index=44\n",
    "print(train_data[index])\n",
    "print(fix_spellings(my_preprocessor(train_data[index]), verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a flag for any level of toxix or a unique number for each combination\n",
    "def bernoulli_toxic_labels (label_vector):\n",
    "    return [1 if (label_vector['toxic'][x] + label_vector['severe_toxic'][x] +\n",
    "                label_vector['obscene'][x] + label_vector['threat'][x] +\n",
    "                label_vector['insult'][x] + label_vector['identity_hate'][x]) > 0 else 0 \n",
    "            for x in label_vector.index.values]\n",
    "\n",
    "def binarize_toxic_labels (label_vector):\n",
    "    return [(label_vector['toxic'][x]*32 + label_vector['severe_toxic'][x]*16 +\n",
    "                label_vector['obscene'][x]*8 + label_vector['threat'][x]*4 +\n",
    "                label_vector['insult'][x]*2 + label_vector['identity_hate'][x]) \n",
    "            for x in label_vector.index.values]\n",
    "\n",
    "binary_train_labels = binarize_toxic_labels(train_labels)\n",
    "binary_dev_labels = binarize_toxic_labels(dev_labels)\n",
    "\n",
    "bernoulli_train_labels = bernoulli_toxic_labels(train_labels)\n",
    "bernoulli_dev_labels = bernoulli_toxic_labels(dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "count_vect_plain = CountVectorizer(preprocessor=my_preprocessor)\n",
    "X_train_counts_plain = count_vect_plain.fit_transform(train_data)\n",
    "X_dev_counts_plain = count_vect_plain.transform(dev_data)\n",
    "\n",
    "tfidf_vect_plain = TfidfVectorizer(preprocessor=my_preprocessor)\n",
    "X_train_tfidf_plain = tfidf_vect_plain.fit_transform(train_data)\n",
    "X_dev_tfidf_plain = tfidf_vect_plain.transform(dev_data)\n",
    "\n",
    "count_vect_stop_words = CountVectorizer(stop_words='english',preprocessor=my_preprocessor)\n",
    "X_train_counts_stop_words = count_vect_stop_words.fit_transform(train_data)\n",
    "X_dev_counts_stop_words = count_vect_stop_words.transform(dev_data)\n",
    "\n",
    "tfidf_vect_stop_words = TfidfVectorizer(stop_words='english',preprocessor=my_preprocessor)\n",
    "X_train_tfidf_stop_words = tfidf_vect_stop_words.fit_transform(train_data)\n",
    "X_dev_tfidf_stop_words = tfidf_vect_stop_words.transform(dev_data)\n",
    "\n",
    "count_vect_stop_words_max10k = CountVectorizer(stop_words='english', max_features=10000,preprocessor=my_preprocessor)\n",
    "X_train_counts_stop_words_max10k = count_vect_stop_words_max10k.fit_transform(train_data)\n",
    "X_dev_counts_stop_words_max10k = count_vect_stop_words_max10k.transform(dev_data)\n",
    "\n",
    "tfidf_vect_stop_words_max10k = TfidfVectorizer(stop_words='english', max_features=10000,preprocessor=my_preprocessor)\n",
    "X_train_tfidf_stop_words_max10k = tfidf_vect_stop_words_max10k.fit_transform(train_data)\n",
    "X_dev_tfidf_stop_words_max10k = tfidf_vect_stop_words_max10k.transform(dev_data)\n",
    "\n",
    "count_vect_stop_words_max5k = CountVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_counts_stop_words_max5k = count_vect_stop_words_max5k.fit_transform(train_data)\n",
    "X_dev_counts_stop_words_max5k = count_vect_stop_words_max5k.transform(dev_data)\n",
    "\n",
    "tfidf_vect_stop_words_max5k = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf_stop_words_max5k = tfidf_vect_stop_words_max5k.fit_transform(train_data)\n",
    "X_dev_tfidf_stop_words_max5k = tfidf_vect_stop_words_max5k.transform(dev_data)\n",
    "\n",
    "count_vect_max5k = CountVectorizer(max_features=5000)\n",
    "X_train_counts_max5k = count_vect_max5k.fit_transform(train_data)\n",
    "X_dev_counts_max5k = count_vect_max5k.transform(dev_data)\n",
    "\n",
    "tfidf_vect_max5k = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf_max5k = tfidf_vect_max5k.fit_transform(train_data)\n",
    "X_dev_tfidf_max5k = tfidf_vect_max5k.transform(dev_data)\n",
    "\n",
    "count_vect_max4k = CountVectorizer(max_features=4000)\n",
    "X_train_counts_max4k = count_vect_max4k.fit_transform(train_data)\n",
    "X_dev_counts_max4k = count_vect_max4k.transform(dev_data)\n",
    "\n",
    "tfidf_vect_max4k = TfidfVectorizer(max_features=4000)\n",
    "X_train_tfidf_max4k = tfidf_vect_max4k.fit_transform(train_data)\n",
    "X_dev_tfidf_max4k = tfidf_vect_max4k.transform(dev_data)\n",
    "\n",
    "count_vect_max6k = CountVectorizer(max_features=6000)\n",
    "X_train_counts_max6k = count_vect_max6k.fit_transform(train_data)\n",
    "X_dev_counts_max6k = count_vect_max6k.transform(dev_data)\n",
    "\n",
    "tfidf_vect_max6k = TfidfVectorizer(max_features=6000)\n",
    "X_train_tfidf_max6k = tfidf_vect_max6k.fit_transform(train_data)\n",
    "X_dev_tfidf_max6k = tfidf_vect_max6k.transform(dev_data)\n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_f1_auc_on_train_dev(dev_vector, train_vector, name):\n",
    "    multinomial_nb_class = MultinomialNB().fit(train_vector, train_labels[name])\n",
    "    predicted_labels_dev = multinomial_nb_class.predict(dev_vector)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dev_labels[name], predicted_labels_dev)\n",
    "    predicted_labels_train = multinomial_nb_class.predict(train_vector)\n",
    "    fpr1, tpr1, thresholds1 = metrics.roc_curve(train_labels[name], predicted_labels_train)\n",
    "    f1scoredev = metrics.f1_score(dev_labels[name],predicted_labels_dev,average='micro')\n",
    "    f1scoretrain = metrics.f1_score(train_labels[name],predicted_labels_train,average='micro')\n",
    "    aucdev = metrics.auc(fpr,tpr)\n",
    "    auctrain = metrics.auc(fpr1,tpr1)\n",
    "    return f1scoredev,aucdev,f1scoretrain,auctrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  set          label     f1dev    aucdev   f1train  auctrain\n",
      "0          CountPlain          toxic  0.942498  0.773851  0.947786  0.818918\n",
      "1          TfidfPlain          toxic  0.921717  0.599574  0.923317  0.616814\n",
      "2      CountStopWords          toxic  0.943746  0.776646  0.951295  0.827418\n",
      "3      TfidfStopWords          toxic  0.924466  0.611925  0.927531  0.635766\n",
      "4   CountStopWords10k          toxic  0.946748  0.812008  0.948961  0.822992\n",
      "5   TfidfStopWords10k          toxic  0.946621  0.760780  0.947991  0.770859\n",
      "6    CountStopWords5k          toxic  0.947805  0.823325  0.948730  0.828803\n",
      "7    TfidfStopWords5k          toxic  0.949559  0.747556  0.949246  0.750520\n",
      "8             Count5k          toxic  0.938714  0.839973  0.941132  0.848808\n",
      "9            TfidfS5k          toxic  0.949559  0.747556  0.949246  0.750520\n",
      "10            Count4k          toxic  0.939052  0.833640  0.940838  0.841222\n",
      "11           TfidfS5k          toxic  0.947868  0.738899  0.947973  0.743093\n",
      "12            Count6k          toxic  0.938757  0.842705  0.941586  0.853184\n",
      "13           TfidfS6k          toxic  0.949475  0.748713  0.949353  0.751652\n",
      "14         CountPlain   severe_toxic  0.980847  0.704307  0.978827  0.755346\n",
      "15         TfidfPlain   severe_toxic  0.990339  0.500000  0.989828  0.501721\n",
      "16     CountStopWords   severe_toxic  0.981692  0.720986  0.980297  0.785660\n",
      "17     TfidfStopWords   severe_toxic  0.990339  0.500000  0.989864  0.501305\n",
      "18  CountStopWords10k   severe_toxic  0.977930  0.813343  0.978427  0.841683\n",
      "19  TfidfStopWords10k   severe_toxic  0.988563  0.596611  0.988394  0.615802\n",
      "20   CountStopWords5k   severe_toxic  0.984060  0.845691  0.984261  0.851153\n",
      "21   TfidfStopWords5k   severe_toxic  0.990994  0.589171  0.990719  0.593059\n",
      "22            Count5k   severe_toxic  0.981819  0.871645  0.981553  0.865005\n",
      "23           TfidfS5k   severe_toxic  0.990994  0.589171  0.990719  0.593059\n",
      "24            Count4k   severe_toxic  0.982242  0.869692  0.981829  0.861666\n",
      "25           TfidfS5k   severe_toxic  0.991142  0.600080  0.990630  0.607364\n",
      "26            Count6k   severe_toxic  0.981904  0.874938  0.981562  0.869793\n",
      "27           TfidfS6k   severe_toxic  0.990994  0.578337  0.990594  0.579515\n",
      "28         CountPlain        obscene  0.960362  0.766805  0.960202  0.809068\n",
      "29         TfidfPlain        obscene  0.953470  0.563253  0.954555  0.583406\n",
      "..                ...            ...       ...       ...       ...       ...\n",
      "54            Count6k         threat  0.985688  0.762753  0.987040  0.865527\n",
      "55           TfidfS6k         threat  0.996977  0.500000  0.996989  0.499987\n",
      "56         CountPlain         insult  0.958311  0.740321  0.958528  0.789896\n",
      "57         TfidfPlain         insult  0.953682  0.535318  0.954956  0.553488\n",
      "58     CountStopWords         insult  0.959432  0.749046  0.960853  0.801033\n",
      "59     TfidfStopWords         insult  0.953724  0.532086  0.955170  0.550352\n",
      "60  CountStopWords10k         insult  0.960003  0.819513  0.961396  0.833028\n",
      "61  TfidfStopWords10k         insult  0.963110  0.720677  0.963400  0.733142\n",
      "62   CountStopWords5k         insult  0.965372  0.833930  0.965618  0.836959\n",
      "63   TfidfStopWords5k         insult  0.968607  0.719296  0.968629  0.725380\n",
      "64            Count5k         insult  0.958523  0.856971  0.958002  0.858337\n",
      "65           TfidfS5k         insult  0.968607  0.719296  0.968629  0.725380\n",
      "66            Count4k         insult  0.958692  0.853399  0.957869  0.851686\n",
      "67           TfidfS5k         insult  0.967972  0.713878  0.968646  0.724791\n",
      "68            Count6k         insult  0.958480  0.858982  0.958314  0.862604\n",
      "69           TfidfS6k         insult  0.968353  0.715502  0.968673  0.724036\n",
      "70         CountPlain  identity_hate  0.982454  0.594858  0.980431  0.633983\n",
      "71         TfidfPlain  identity_hate  0.990804  0.500000  0.991333  0.501008\n",
      "72     CountStopWords  identity_hate  0.986512  0.587796  0.984867  0.655126\n",
      "73     TfidfStopWords  identity_hate  0.990804  0.500000  0.991342  0.500502\n",
      "74  CountStopWords10k  identity_hate  0.973828  0.724879  0.975478  0.786310\n",
      "75  TfidfStopWords10k  identity_hate  0.990614  0.524957  0.991084  0.535629\n",
      "76   CountStopWords5k  identity_hate  0.981164  0.752494  0.982791  0.803794\n",
      "77   TfidfStopWords5k  identity_hate  0.991269  0.530981  0.991859  0.544195\n",
      "78            Count5k  identity_hate  0.977147  0.759577  0.977705  0.805317\n",
      "79           TfidfS5k  identity_hate  0.991269  0.530981  0.991859  0.544195\n",
      "80            Count4k  identity_hate  0.977803  0.747382  0.978213  0.793820\n",
      "81           TfidfS5k  identity_hate  0.991100  0.536590  0.991788  0.547736\n",
      "82            Count6k  identity_hate  0.976999  0.758364  0.977580  0.807809\n",
      "83           TfidfS6k  identity_hate  0.991206  0.525255  0.991832  0.537028\n",
      "\n",
      "[84 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "scores_all=pd.DataFrame(columns=['set','label','f1dev','aucdev','f1train','auctrain'])\n",
    "\n",
    "for name in target_names:\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_counts_plain,\n",
    "                                                   dev_vector=X_dev_counts_plain,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['CountPlain',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_tfidf_plain,\n",
    "                                                   dev_vector=X_dev_tfidf_plain,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['TfidfPlain',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_counts_stop_words,\n",
    "                                                   dev_vector=X_dev_counts_stop_words,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['CountStopWords',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_tfidf_stop_words,\n",
    "                                                   dev_vector=X_dev_tfidf_stop_words,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['TfidfStopWords',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_counts_stop_words_max10k,\n",
    "                                                   dev_vector=X_dev_counts_stop_words_max10k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['CountStopWords10k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_tfidf_stop_words_max10k,\n",
    "                                                   dev_vector=X_dev_tfidf_stop_words_max10k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['TfidfStopWords10k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_counts_stop_words_max5k,\n",
    "                                                   dev_vector=X_dev_counts_stop_words_max5k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['CountStopWords5k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_tfidf_stop_words_max5k,\n",
    "                                                   dev_vector=X_dev_tfidf_stop_words_max5k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['TfidfStopWords5k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_counts_max5k,\n",
    "                                                   dev_vector=X_dev_counts_max5k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['Count5k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_tfidf_max5k,\n",
    "                                                   dev_vector=X_dev_tfidf_max5k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['TfidfS5k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_counts_max4k,\n",
    "                                                   dev_vector=X_dev_counts_max4k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['Count4k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_tfidf_max4k,\n",
    "                                                   dev_vector=X_dev_tfidf_max4k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['TfidfS5k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_counts_max6k,\n",
    "                                                   dev_vector=X_dev_counts_max6k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['Count6k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_tfidf_max6k,\n",
    "                                                   dev_vector=X_dev_tfidf_max6k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['TfidfS6k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "    \n",
    "\n",
    "    # not measuring here for each name\n",
    "print(scores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:50:33.894302\n",
      "11:50:34.625534\n",
      "11:50:34.952166\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9cea9571fbe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcount_vect_plain_pre_token6k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfix_spellings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train_counts_plain_pre_token6k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect_plain_pre_token6k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX_dev_counts_plain_pre_token6k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect_plain_pre_token6k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-50522a1be550>\u001b[0m in \u001b[0;36mfix_spellings\u001b[0;34m(textinput, verbose)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mreturn_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-50522a1be550>\u001b[0m in \u001b[0;36mget_best_candidates\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mbest_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mbest_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_checker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/enchant/checker/__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0msuggs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuggs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/enchant/__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturning\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpossibilities\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \"\"\"\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0msuggs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m         \u001b[0msuggs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpwl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuggs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuggs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/enchant/__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can't suggest spellings for empty string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0msuggs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_suggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_this\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuggs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/enchant/_enchant.py\u001b[0m in \u001b[0;36mdict_suggest\u001b[0;34m(dict, word)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdict_suggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mnumSuggsP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_size_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0msuggs_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_suggest1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumSuggsP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0msuggs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "count_vect_plain_pre = CountVectorizer(preprocessor=my_preprocessor)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "X_train_counts_plain_pre = count_vect_plain_pre.fit_transform(tiny_data)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "count_vect_plain_pre_token6k = CountVectorizer(tokenizer=fix_spellings, max_features=10000, strip_accents='ascii', lowercase=True)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "X_train_counts_plain_pre_token6k = count_vect_plain_pre_token6k.fit_transform(train_data)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "X_dev_counts_plain_pre_token6k = count_vect_plain_pre_token6k.transform(dev_data)\n",
    "print(str(datetime.datetime.now().time()))\n",
    "#X_dev_counts_plain_pre = count_vect_plain_pre.transform(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  set          label     f1dev    aucdev   f1train  auctrain\n",
      "0  CountPlainPreTok6k          toxic  0.940576  0.820492  0.942385  0.827347\n",
      "1  CountPlainPreTok6k   severe_toxic  0.975849  0.828395  0.977018  0.840311\n",
      "2  CountPlainPreTok6k        obscene  0.958318  0.847900  0.958472  0.853475\n",
      "3  CountPlainPreTok6k         threat  0.981128  0.794024  0.981355  0.797693\n",
      "4  CountPlainPreTok6k         insult  0.955092  0.825678  0.956559  0.838370\n",
      "5  CountPlainPreTok6k  identity_hate  0.969649  0.764698  0.970875  0.786283\n"
     ]
    }
   ],
   "source": [
    "scores_all=pd.DataFrame(columns=['set','label','f1dev','aucdev','f1train','auctrain'])\n",
    "for name in target_names:\n",
    "    tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain = score_f1_auc_on_train_dev(train_vector=X_train_counts_plain_pre_token6k,\n",
    "                                                   dev_vector=X_dev_counts_plain_pre_token6k,name=name)\n",
    "    scores_all.loc[scores_all.shape[0]] = ['CountPlainPreTok6k',name,tmpf1dev,tmpaucdev,tmpf1train,tmpauctrain]\n",
    "\n",
    "print(scores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model counts_plain_pre_token_toxic to saved/counts_plain_pre_token_toxic.sav\n",
      "Finished saving saved/counts_plain_pre_token_toxic.sav\n",
      "Saving to saved/counts_plain_pre_token_toxic_predict_dev.csv\n",
      "Finished saving saved/counts_plain_pre_token_toxic_predict_dev.csv\n",
      "Saving to saved/counts_plain_pre_token_toxic_predict_dev_proba.csv\n",
      "Finished saving saved/counts_plain_pre_token_toxic_predict_dev_proba.csv\n",
      "Saving to saved/counts_plain_pre_token_toxic_predict_dev_proba_log.csv\n",
      "Finished saving saved/counts_plain_pre_token_toxic_predict_dev_proba_log.csv\n",
      "Saving model counts_plain_pre_token_severe_toxic to saved/counts_plain_pre_token_severe_toxic.sav\n",
      "Finished saving saved/counts_plain_pre_token_severe_toxic.sav\n",
      "Saving to saved/counts_plain_pre_token_severe_toxic_predict_dev.csv\n",
      "Finished saving saved/counts_plain_pre_token_severe_toxic_predict_dev.csv\n",
      "Saving to saved/counts_plain_pre_token_severe_toxic_predict_dev_proba.csv\n",
      "Finished saving saved/counts_plain_pre_token_severe_toxic_predict_dev_proba.csv\n",
      "Saving to saved/counts_plain_pre_token_severe_toxic_predict_dev_proba_log.csv\n",
      "Finished saving saved/counts_plain_pre_token_severe_toxic_predict_dev_proba_log.csv\n",
      "Saving model counts_plain_pre_token_obscene to saved/counts_plain_pre_token_obscene.sav\n",
      "Finished saving saved/counts_plain_pre_token_obscene.sav\n",
      "Saving to saved/counts_plain_pre_token_obscene_predict_dev.csv\n",
      "Finished saving saved/counts_plain_pre_token_obscene_predict_dev.csv\n",
      "Saving to saved/counts_plain_pre_token_obscene_predict_dev_proba.csv\n",
      "Finished saving saved/counts_plain_pre_token_obscene_predict_dev_proba.csv\n",
      "Saving to saved/counts_plain_pre_token_obscene_predict_dev_proba_log.csv\n",
      "Finished saving saved/counts_plain_pre_token_obscene_predict_dev_proba_log.csv\n",
      "Saving model counts_plain_pre_token_threat to saved/counts_plain_pre_token_threat.sav\n",
      "Finished saving saved/counts_plain_pre_token_threat.sav\n",
      "Saving to saved/counts_plain_pre_token_threat_predict_dev.csv\n",
      "Finished saving saved/counts_plain_pre_token_threat_predict_dev.csv\n",
      "Saving to saved/counts_plain_pre_token_threat_predict_dev_proba.csv\n",
      "Finished saving saved/counts_plain_pre_token_threat_predict_dev_proba.csv\n",
      "Saving to saved/counts_plain_pre_token_threat_predict_dev_proba_log.csv\n",
      "Finished saving saved/counts_plain_pre_token_threat_predict_dev_proba_log.csv\n",
      "Saving model counts_plain_pre_token_insult to saved/counts_plain_pre_token_insult.sav\n",
      "Finished saving saved/counts_plain_pre_token_insult.sav\n",
      "Saving to saved/counts_plain_pre_token_insult_predict_dev.csv\n",
      "Finished saving saved/counts_plain_pre_token_insult_predict_dev.csv\n",
      "Saving to saved/counts_plain_pre_token_insult_predict_dev_proba.csv\n",
      "Finished saving saved/counts_plain_pre_token_insult_predict_dev_proba.csv\n",
      "Saving to saved/counts_plain_pre_token_insult_predict_dev_proba_log.csv\n",
      "Finished saving saved/counts_plain_pre_token_insult_predict_dev_proba_log.csv\n",
      "Saving model counts_plain_pre_token_identity_hate to saved/counts_plain_pre_token_identity_hate.sav\n",
      "Finished saving saved/counts_plain_pre_token_identity_hate.sav\n",
      "Saving to saved/counts_plain_pre_token_identity_hate_predict_dev.csv\n",
      "Finished saving saved/counts_plain_pre_token_identity_hate_predict_dev.csv\n",
      "Saving to saved/counts_plain_pre_token_identity_hate_predict_dev_proba.csv\n",
      "Finished saving saved/counts_plain_pre_token_identity_hate_predict_dev_proba.csv\n",
      "Saving to saved/counts_plain_pre_token_identity_hate_predict_dev_proba_log.csv\n",
      "Finished saving saved/counts_plain_pre_token_identity_hate_predict_dev_proba_log.csv\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model_name, model):\n",
    "    filename='saved/' + model_name + '.sav'\n",
    "    print('Saving model %s to %s' % (model_name, filename))\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print('Finished saving %s' % (filename))\n",
    "\n",
    "def save_csv_results(name, data):\n",
    "    filename='saved/' + name + '.csv'\n",
    "    print('Saving to %s' % (filename))\n",
    "    pd.DataFrame(data).to_csv(filename)\n",
    "    print('Finished saving %s' % (filename))\n",
    "\n",
    "for name in target_names:\n",
    "    multinomial_nb_class = MultinomialNB().fit(X_train_counts_plain_pre_token, train_labels[name])\n",
    "    predicted_labels_dev = multinomial_nb_class.predict(X_dev_counts_plain_pre_token)\n",
    "    predicted_labels_proba_dev = multinomial_nb_class.predict_proba(X_dev_counts_plain_pre_token)\n",
    "    predicted_labels_log_proba_dev = multinomial_nb_class.predict_log_proba(X_dev_counts_plain_pre_token)\n",
    "    model_name = 'counts_plain_pre_token_' + name\n",
    "    model_name_predict = model_name + '_predict_dev'\n",
    "    model_name_predict_proba = model_name_predict + '_proba'\n",
    "    model_name_predict_log_proba = model_name_predict_proba + '_log'\n",
    "    save_model(model_name=model_name, model=multinomial_nb_class)\n",
    "    save_csv_results(name=model_name_predict, data=predicted_labels_dev)\n",
    "    save_csv_results(name=model_name_predict_proba, data=predicted_labels_proba_dev)\n",
    "    save_csv_results(name=model_name_predict_log_proba, data=predicted_labels_log_proba_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_counts_plain_pre_token.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
