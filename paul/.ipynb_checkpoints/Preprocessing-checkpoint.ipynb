{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 159571\n",
      "training data shape: (112098,)\n",
      "training label shape: (112098, 6)\n",
      "dev label shape: (47473, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#S plit up data\n",
    "test_data = test_df[\"comment_text\"]\n",
    "dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "\n",
    "print('total training observations:', train_df.shape[0])\n",
    "print('training data shape:', train_data.shape)\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print ('labels names:', target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus 462986\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "word_corpus = '../data/words.txt'\n",
    "word_file = open(word_corpus, 'rt')\n",
    "large_word_corpus = word_file.read()\n",
    "word_file.close\n",
    "large_word_corpus = large_word_corpus.split()\n",
    "large_word_corpus = [ word.lower() for word in large_word_corpus]\n",
    "large_word_corpus = set(large_word_corpus)\n",
    "\n",
    "good_words_list = brown.words()\n",
    "good_word_set = set([word.lower() for word in good_words_list])\n",
    "#punctuation = re.sub(\"[\\'\\-]\",'',string.punctuation)\n",
    "punctuation = \"[\\!\\?\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\,\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\\']\"\n",
    "print('Size of corpus ' + str(len(large_word_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://norvig.com/spell-correct.html\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../data/big.txt').read()))\n",
    "\n",
    "def norvig_P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def norvig_correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(norvig_candidates(word), key=norvig_P)\n",
    "\n",
    "def norvig_candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to support finding and correcting spellings\n",
    "# using pyenchant for spell checking\n",
    "from enchant import DictWithPWL\n",
    "from enchant.checker import SpellChecker\n",
    "import difflib\n",
    "# import splitter # not useful, does a worse job than my implementation\n",
    "\n",
    "my_dict=DictWithPWL('en_US', \"../data/mywords.txt\")\n",
    "my_checker = SpellChecker(my_dict)\n",
    "\n",
    "def clean_slang_abbreviations(textblock):\n",
    "    # u -> you\n",
    "    # c -> see\n",
    "    # k -> okay\n",
    "    return_words = textblock\n",
    "    #return_words = re.sub(r\"[^A-Za-z0-9,\\!\\?\\*\\.\\;\\’\\´\\'\\\\\\/]\", \" \", return_words)\n",
    "    return_words = return_words.lower()\n",
    "    return_words = re.sub(\",\",\"\",return_words)\n",
    "    return_words = re.sub(\"\\.\",\"\",return_words)\n",
    "    return_words = re.sub(r\"\\(\",\" \", return_words)\n",
    "    return_words = re.sub(r\"\\)\",\" \", return_words)\n",
    "    return_words = re.sub(r\"´\", \"'\", return_words)\n",
    "    return_words = re.sub(r\"`\", \"'\", return_words)\n",
    "    return_words = re.sub(r\" '\", \" \", return_words)\n",
    "    return_words = re.sub(r\"' \", \" \", return_words)\n",
    "    return_words = re.sub(r\"\\!\\!*\", \"\\!Many\", return_words)\n",
    "    return_words = re.sub(r\"\\?\\?*\", \"\\?Many\", return_words)\n",
    "    return_words = re.sub(r\"\\!\", \" !\", return_words)\n",
    "    return_words = re.sub(r\"\\?\", \" \\?\", return_words)\n",
    "    return_words = re.sub(r\"n't\", \" not\", return_words)\n",
    "    return return_words\n",
    "\n",
    "def trysplit(word):\n",
    "    split_candidates = []\n",
    "    max_proba = 0.0\n",
    "    for i in range(1,len(word)):\n",
    "        # I will only allow single letters of 'a' and 'i', all others ignored.  Pyenchant allows for\n",
    "        # any single letter to be a legitimate word, and so too does norvig.  The dictionary defines\n",
    "        # them as nouns that represent the letter, however even though several can be used in slang\n",
    "        # (e.g. k->okay, c->see, u->you) using them in conjoined words would make the splitting far\n",
    "        # too difficult and also human understanding much more difficult #howucthisk, u c?\n",
    "        if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "            len(word[i:]) != 1 or (word[i:].lower() == 'a' or word[i:].lower() == 'i')):\n",
    "            if my_checker.check(word[:i]) and my_checker.check(word[i:]):\n",
    "                norvig_score = norvig_P(word[:i]) + norvig_P(word[i:])\n",
    "                if norvig_score > max_proba:\n",
    "                    max_proba = norvig_score\n",
    "                    split_candidates = [word[:i],word[i:]]\n",
    "    for i in range(1,len(word)):\n",
    "        for j in range(i+1,len(word)):        \n",
    "            if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "                len(word[i:j]) != 1 or (word[i:j].lower() == 'a' or word[i:j].lower() == 'i')) and (\n",
    "                len(word[i:]) != 1 or (word[i:].lower() == 'a' or word[i:].lower() == 'i')):\n",
    "                \n",
    "                if my_checker.check(word[:i]) and my_checker.check(word[i:j]) and my_checker.check(word[j:]):\n",
    "                    norvig_score = norvig_P(word[:i]) + norvig_P(word[i:j]) + norvig_P(word[j:])\n",
    "                    if norvig_score > max_proba:\n",
    "                        max_proba = norvig_score\n",
    "                        split_candidates = [word[:i],word[i:j],word[j:]]\n",
    "    for i in range(1,len(word)):\n",
    "        for j in range(i+1,len(word)):\n",
    "            for k in range(j+1,len(word)):\n",
    "                if (len(word[:i]) != 1 or (word[:i].lower() == 'a' or word[:i].lower() == 'i')) and (\n",
    "                    len(word[i:j]) != 1 or (word[i:j].lower() == 'a' or word[i:j].lower() == 'i')) and (\n",
    "                    len(word[j:k]) != 1 or (word[j:k].lower() == 'a' or word[j:k].lower() == 'i')) and (\n",
    "                    len(word[k:]) != 1 or (word[k:].lower() == 'a' or word[k:].lower() == 'i')):\n",
    "#                     print(\"making it here with i=%s j=%s k=%s %s  max_proba=%d\" %(word[:i],word[i:j],word[j:k],word[k:], max_proba))\n",
    "#                     print(\"lengths are %d %d %d %d\" % (len(word[:i]), len(word[i:j]),len(word[j:k]),len(word[k:])))\n",
    "                    if my_checker.check(word[:i]) and my_checker.check(word[i:j]) and my_checker.check(word[j:k]) and my_checker.check(word[k:]):\n",
    "#                         print('found words ' + word[i:] + ' ' + word[k:])\n",
    "                        norvig_score = norvig_P(word[:i]) + norvig_P(word[i:j]) + norvig_P(word[j:k]) + norvig_P(word[k:])\n",
    "                        if norvig_score > max_proba:\n",
    "#                             print(\"found higher probability %d with %s %s %s %s\" % (norvig_score, word[:i], word[i:j], word[j:k], word[k:]))\n",
    "                            max_proba = norvig_score\n",
    "                            split_candidates = [word[:i],word[i:j],word[j:k],word[k:]]\n",
    "    return split_candidates\n",
    "\n",
    "def fix_spelling_errors(textdoc):\n",
    "    words = textdoc.split()\n",
    "    return_list = []\n",
    "    for word in words:\n",
    "        if large_word_corpus.intersection(set([re.sub(punctuation,'',word.lower())])):\n",
    "            return_list.append(word)\n",
    "        else:\n",
    "            # word is not found in the dictionary, try to correct the spelling\n",
    "            if word == spell(word): # no changes made by the spell checker\n",
    "                return_list.extend(trysplit(word))\n",
    "            else:\n",
    "                return_list.append(spell(word))\n",
    "    return return_list\n",
    "\n",
    "def get_best_candidates(word):\n",
    "    best_words = []\n",
    "    best_ratio = 0\n",
    "    a = set(my_checker.suggest(word))\n",
    "    for b in a:\n",
    "        tmp = difflib.SequenceMatcher(None, word, b).ratio()\n",
    "        if tmp > best_ratio:\n",
    "            best_words=[b]\n",
    "            best_ratio=tmp\n",
    "        elif tmp == best_ratio:\n",
    "            best_words.append(b)\n",
    "    return best_words\n",
    "    \n",
    "def fix_spellings(textinput):\n",
    "    words = textinput.split()\n",
    "    return_list = []\n",
    "    for word in words:\n",
    "        if my_checker.check(word):\n",
    "            return_list.append(word)\n",
    "            # continue\n",
    "        else:\n",
    "            if len(norvig_candidates(word)) == 1 and norvig_candidates(word).pop() == word:\n",
    "                # this is suspicious, pyenchants' \"suggest\" method always returns something, however if\n",
    "                # norvigs method cannot find a suitable match within a short distance then it simply\n",
    "                # returns the orignal word.  This section is for potentially conjoined words\n",
    "                tmp_list=trysplit(word)\n",
    "                \n",
    "                # If we get back a list of split words then use these\n",
    "                if len(tmp_list) != 0:\n",
    "                    return_list.extend(tmp_list)\n",
    "                    continue\n",
    "            \n",
    "            candidates = get_best_candidates(word)\n",
    "            if len(candidates) == 1:\n",
    "                    return_list.append(candidates)\n",
    "            elif len(candidates) > 1:\n",
    "                # try another spell checker\n",
    "                candidates2 = norvig_candidates(word)\n",
    "                tmp_len = len(candidates2.intersection(candidates))\n",
    "                if tmp_len == 1:\n",
    "                    # only 1 overlap, should be correct\n",
    "                    print(candidates2.intersection(candidates))\n",
    "                    return_list.append(candidates2.intersection(candidates).pop())\n",
    "                else:\n",
    "                    # arbitrary now, just going to use the first one found\n",
    "                    return_list.append(candidates[0])\n",
    "    return return_list\n",
    "\n",
    "# myword='In a long discussion about thisismessedup what should I do askd'\n",
    "# print(fix_spellings(myword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\"]\n"
     ]
    }
   ],
   "source": [
    "myword2='findthiscara'\n",
    "# trysplit(myword)\n",
    "#norvig_P('is')\n",
    "#fix_spellings('alit')\n",
    "# help(enchant)\n",
    "print(get_best_candidates(\"I'm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169\n",
      "{'ism'}\n",
      "{'ism'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'intersection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a3d56d97092a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfix_spellings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_slang_abbreviations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-19524c6a9891>\u001b[0m in \u001b[0;36mfix_spellings\u001b[0;34m(textinput)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;31m# try another spell checker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mcandidates2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorvig_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mtmp_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtmp_len\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0;31m# only 1 overlap, should be correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'intersection'"
     ]
    }
   ],
   "source": [
    "index=8\n",
    "print(train_data[index])\n",
    "print(fix_spellings(clean_slang_abbreviations(train_data[index])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
