{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of things were tested for this model\n",
    "\n",
    "* A variety of different parameters and vectorizers\n",
    "  * Count and Tfidf vectors\n",
    "  * Variety of feature sizes\n",
    "  * Data preprocessed or not\n",
    "  * Removing stop words and accents\n",
    "* Calculation of three types of scoring, precision, recall and auc\n",
    "\n",
    "Lessons learned\n",
    "* Experience is key: being novice has made this process take much longer\n",
    "\n",
    "* Interpreters (I'm sure everyone knows this) are slow, and for the moment this is single threaded so even slower.  A side requiment of this is that hardware matters, particularly faster cores and plenty of memory.\n",
    "\n",
    "* While we can in some cases brute force the best parameters there are situations where unless we have time and a cluster for processing power, we must instead rely on educated guesswork and compromises.  The educated guesswork can obviously be helped by research and experience.\n",
    "\n",
    "* Gaussian Naive Bayes is not suitable for the very sparse inputs we are using so not testing these out.\n",
    "\n",
    "\n",
    "Results:  \n",
    "\n",
    "<TABLE>\n",
    "<TR><TH> label</TH><TH> model</TH><TH> alpha</TH><TH> type</TH><TH> preprocessor</TH><TH> tokenizer</TH><TH> max_features</TH><TH> stop_words</TH><TH> lowercase</TH><TH> strip_accents</TH><TH> score_type</TH><TH> score </TH></TR>\n",
    "<TR><TD> toxic</TD><TD> multi</TD><TD> 10</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> None</TD><TD> None</TD><TD> TRUE</TD><TD> None</TD><TD> precision</TD><TD> 1 </TD></TR>\n",
    "<TR><TD> severe_toxic</TD><TD> multi</TD><TD> 10</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> 6000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> precision</TD><TD> 0.8 </TD></TR>\n",
    "<TR><TD> obscene</TD><TD> multi</TD><TD> 2</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> None</TD><TD> None</TD><TD> TRUE</TD><TD> None</TD><TD> precision</TD><TD> 1 </TD></TR>\n",
    "<TR><TD> threat</TD><TD> multi</TD><TD> 0.5</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> None</TD><TD> None</TD><TD> FALSE</TD><TD> None</TD><TD> precision</TD><TD> 1 </TD></TR>\n",
    "<TR><TD> insult</TD><TD> multi</TD><TD> 2</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> None</TD><TD> None</TD><TD> FALSE</TD><TD> None</TD><TD> precision</TD><TD> 1 </TD></TR>\n",
    "<TR><TD> identity_hate</TD><TD> multi</TD><TD> 2</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 3000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> precision</TD><TD> 0.884615 </TD></TR>\n",
    "<TR><TH> Average</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> 0.947435833 </TH></TR>\n",
    "</TABLE>\n",
    "\n",
    "<TABLE>\n",
    "<TR><TH> label</TH><TH> model</TH><TH> alpha</TH><TH> type</TH><TH> preprocessor</TH><TH> tokenizer</TH><TH> max_features</TH><TH> stop_words</TH><TH> lowercase</TH><TH> strip_accents</TH><TH> score_type</TH><TH> score </TH></TR>\n",
    "<TR><TD> toxic</TD><TD> bern</TD><TD> 1</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> 10000</TD><TD> english</TD><TD> TRUE</TD><TD> unicode</TD><TD> recall</TD><TD> 0.886398 </TD></TR>\n",
    "<TR><TD> severe_toxic</TD><TD> bern</TD><TD> 0.5</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> 10000</TD><TD> english</TD><TD> TRUE</TD><TD> None</TD><TD> recall</TD><TD> 0.953782 </TD></TR>\n",
    "<TR><TD> obscene</TD><TD> bern</TD><TD> 1</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 10000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> recall</TD><TD> 0.897975 </TD></TR>\n",
    "<TR><TD> threat</TD><TD> bern</TD><TD> 0.5</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> 3000</TD><TD> None</TD><TD> TRUE</TD><TD> None</TD><TD> recall</TD><TD> 0.868421 </TD></TR>\n",
    "<TR><TD> insult</TD><TD> bern</TD><TD> 1</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> 10000</TD><TD> english</TD><TD> TRUE</TD><TD> unicode</TD><TD>recall</TD><TD> 0.885738 </TD></TR>\n",
    "<TR><TD> identity_hate</TD><TD> bern</TD><TD> 0.5</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> None</TD><TD> FALSE</TD><TD> None</TD><TD> recall</TD><TD> 0.88785 </TD></TR>\n",
    "<TR><TH> Average</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> 0.896694 </TH></TR>\n",
    "</TABLE>\n",
    "\n",
    "<TABLE>\n",
    "<TR><TH> label</TH><TH> model</TH><TH> alpha</TH><TH> type</TH><TH> preprocessor</TH><TH> tokenizer</TH><TH> max_features</TH><TH> stop_words</TH><TH> lowercase</TH><TH> strip_accents</TH><TH> score_type</TH><TH> score </TH></TR>\n",
    "<TR><TD> toxic</TD><TD> bern</TD><TD> 15</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.858937 </TD></TR>\n",
    "<TR><TD> severe_toxic</TD><TD> bern</TD><TD> 2</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.93949 </TD></TR>\n",
    "<TR><TD> obscene</TD><TD> bern</TD><TD> 10</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.888569 </TD></TR>\n",
    "<TR><TD> threat</TD><TD> bern</TD><TD> 0.5</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> None</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.899946 </TD></TR>\n",
    "<TR><TD> insult</TD><TD> bern</TD><TD> 10</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 3000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.872004 </TD></TR>\n",
    "<TR><TD> identity_hate</TD><TD> bern</TD><TD> 1</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.886601 </TD></TR>\n",
    "<TR><TH> Average</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> 0.8909245 </TH></TR>\n",
    "</TABLE>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/pandas/core/nanops.py:39: UserWarning: The installed version of bottleneck 0.7.0 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 1.0.0\n",
      "\n",
      "  ver=ver, min_ver=_MIN_BOTTLENECK_VERSION), UserWarning)\n",
      "/usr/lib64/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/lib64/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "from sklearn import metrics\n",
    "import ast\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total training observations:', 111828)\n",
      "('training data shape:', (111828,))\n",
      "('training label shape:', (111828, 6))\n",
      "('dev label shape:', (47743, 6))\n",
      "('labels names:', ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/new_train.csv\")\n",
    "test_df = pd.read_csv(\"../data/new_test.csv\")\n",
    "\n",
    "# # Random index generator for splitting training data\n",
    "# # Note: Each rerun of cell will create new splits.\n",
    "# randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "# #S plit up data\n",
    "# test_data = test_df[\"comment_text\"]\n",
    "# dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "# train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "dev_data, dev_labels = test_df[\"comment_text\"], test_df[target_names]\n",
    "train_data, train_labels = train_df[\"comment_text\"], train_df[target_names]\n",
    "\n",
    "print('total training observations:', train_df.shape[0])\n",
    "print('training data shape:', train_data.shape)\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print('labels names:', target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Courtesy of Walt\n",
    "\n",
    "import nltk\n",
    "# These imports enable the use of NLTKPreprocessor in an sklearn Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import punkt as punkt\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Text preprocessor using NLTK tokenization and Lemmatization\n",
    "\n",
    "    This class is to be used in an sklean Pipeline, prior to other processers like PCA/LSA/classification\n",
    "    Attributes:\n",
    "        lower: A boolean indicating whether text should be lowercased by preprocessor\n",
    "                default: True\n",
    "        strip: A boolean indicating whether text should be stripped of surrounding whitespace, underscores and '*'\n",
    "                default: True\n",
    "        stopwords: A set of words to be used as stop words and thus ignored during tokenization\n",
    "                default: built-in English stop words\n",
    "        punct: A set of punctuation characters that should be ignored\n",
    "                default: None\n",
    "        lemmatizer: An object that should be used to lemmatize tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(unicode(document, 'utf8')):\n",
    "\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                \n",
    "                # S\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of scores on dev set and training set\n",
    "def score_classifier_train_on_dev(dev_vector, train_vector, dev_labels, train_labels, label, ctype, pscoring):\n",
    "    \"\"\"This function takes two vectors, one for training and one for dev, trains them\n",
    "    on the selected Naive Bayes model, then depending on the scoring required it\n",
    "    finds the optimal alpha for the particular scoring and calculates that score from\n",
    "    predictions on the dev set.\n",
    "    \n",
    "    Args:\n",
    "        dev_vector: the processed vector of dev data\n",
    "        train_vector: the processed vector of training data\n",
    "        dev_labels: the vector of each of the 6 lables for the dev set\n",
    "        train_labels: the vector of labels for the training set\n",
    "        label (string) : the label name to test\n",
    "        ctype: multi, gaus or bern, choses between multinomial, gaussian or bernoulli Naive Bayes\n",
    "        scoring: should be one of roc_auc, precision, or recall\n",
    "        \n",
    "    Returns:\n",
    "        alpha: the best alpha value for this classifier\n",
    "        score: the score when using this classifier to predict dev\n",
    "    \"\"\"\n",
    "    alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 15.0, 20.0, 50.0, 100.0]}\n",
    "\n",
    "    if pscoring != 'precision' and pscoring != 'recall' and pscoring != 'roc_auc' and pscoring != 'f1':\n",
    "        print('score_classifier_train_on_dev: Invalid input parameter %s' %(pscoring))\n",
    "        return\n",
    "    \n",
    "    if ctype == 'multi':\n",
    "        nb_class = MultinomialNB().fit(train_vector, train_labels[label])\n",
    "    elif ctype == 'bern':\n",
    "        nb_class = BernoulliNB().fit(train_vector, train_labels[label])\n",
    "    elif ctype == 'gaus':\n",
    "        nb_class = GaussianNB().fit(train_vector, train_labels[label])\n",
    "    else:\n",
    "        print('ctype = %s, error' % (ctype))\n",
    "        return\n",
    "    \n",
    "    # use this to generate the best fitting model for AUC scoring\n",
    "    clf = GridSearchCV(nb_class, param_grid = alphas, scoring=pscoring)\n",
    "    clf.fit(train_vector, train_labels[label])\n",
    "    \n",
    "    # Predict the dev vector\n",
    "    predicted_labels_dev = clf.predict(dev_vector)\n",
    "    \n",
    "    rscore = 0 # return score\n",
    "    # now calculate the score of interested based on the function parameter pscoring\n",
    "    if pscoring == 'precision':\n",
    "        rscore = metrics.precision_score(dev_labels[label], predicted_labels_dev)\n",
    "    elif pscoring == 'recall':\n",
    "        rscore = metrics.recall_score(dev_labels[label], predicted_labels_dev)\n",
    "    elif pscoring == 'f1':\n",
    "        rscore = metrics.f1_score(dev_labels[label], predicted_labels_dev)\n",
    "    else:\n",
    "        rscore = metrics.roc_auc_score(dev_labels[label], predicted_labels_dev)\n",
    "\n",
    "    return clf.best_params_, rscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_all_vectors(my_feature_sizes = [None, 3000, 4000, 5000, 6000, 10000],\n",
    "                       my_stop_words = [None, 'english'],\n",
    "                       my_strip_accents = [None, 'ascii', 'unicode'],\n",
    "                       my_lowercase = [True, False],\n",
    "                       basetrain_data=[],\n",
    "                       basedev_data=[],\n",
    "                       preprocessedtrain_data=[],\n",
    "                       preprocesseddev_data=[],\n",
    "                       verbose=False):\n",
    "    \"\"\"This loops through the lists in the parameters creating 2 vector sets for each combination, \n",
    "    one CountVectorizer and one for TfidfVectorizer.  It allows for both preprocessed and unprocessed\n",
    "    input data, and in the case of pre-processed it does not use the options to strip_accents or\n",
    "    lowercase the data, those options are assumed to have occurred when the data was preprocessed.\n",
    "    \n",
    "    Args:\n",
    "        my_feature_size (list of sizes): Non-empty list of feature sizes to use in vectors\n",
    "        my_stop_words (list of stop_words): Provided to the vectorizers\n",
    "        my_strip_accents (list of options): Provided to the vectorizers\n",
    "        my_lowercase (bool): Provided to the vectorizer\n",
    "        basetrain_data (Opt: list of input data): this is the base training data, no preprocessing\n",
    "        basedev_data (Opt: list of input data): this is the base dev data, no preprocessing\n",
    "        preprocessedtrain_data (Opt: list of input data): this is the base training data that received preprocessing\n",
    "        preprocesseddev_data= (Opt: list of input data): this is the base dev data that received preprocessing\n",
    "        verbose (bool): to write progress outputs\n",
    "        \n",
    "    Returns: \n",
    "        vectors_all (Pandas Datafram) : a dataframe where each line contains the unique count or tfidf vector\n",
    "            along with the set of parameters that were used to create it.\n",
    "    \"\"\"\n",
    "    \n",
    "    vectors_all=pd.DataFrame(columns=['vectortrain', 'vectordev','type','preprocessor', 'tokenizer',\n",
    "                                      'max_features', 'stop_words', 'lowercase', 'strip_accents' ])\n",
    "\n",
    "    index=1\n",
    "    if len(basetrain_data) != 0 and len(basedev_data) != 0:\n",
    "        # we have unprocessed data so create vectors for it\n",
    "        for i in my_feature_sizes:\n",
    "            for x in my_stop_words:\n",
    "                for y in my_strip_accents:\n",
    "                    for z in my_lowercase:\n",
    "                        if (verbose == True):\n",
    "                            print(\"%s: Processing the next vector from base data, index %d\" % (str(datetime.datetime.now().time()),index))\n",
    "                            index +=1\n",
    "#                        ### Count Vectorizer removed as the results are identical (to millions of a %) for count and tfidf\n",
    "#                        ### vectorizer.  We stick with just tfidf as this works best for the other models\n",
    "#                        # Create a count vectorizer with the provided parameters\n",
    "#                        vect = CountVectorizer(max_features=i, stop_words=x, strip_accents=y, lowercase=z)\n",
    "#                        # Train the unpreprocessed training set\n",
    "#                        vect_train = vect.fit_transform(train_data)\n",
    "#                        # Transform the dev set for fuuture predictions\n",
    "#                        vect_dev = vect.transform(dev_data)\n",
    "#                        # add into the output data frame with the list of vectors chosen\n",
    "#                        vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 0, 0, i, x, z, y]\n",
    "\n",
    "                        # Now create tfidf vectorizer for the set of parameters\n",
    "                        vect = TfidfVectorizer(max_features=i, stop_words=x, strip_accents=y, analyzer='word',lowercase=z)\n",
    "                        # Train the unpreprocessed training set\n",
    "                        vect_train = vect.fit_transform(train_data)\n",
    "                        # Transform the dev set for fuuture predictions\n",
    "                        vect_dev = vect.transform(dev_data)\n",
    "                        # add into the output data frame with the list of vectors chosen\n",
    "                        vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 0, 0, i, x, z, y]\n",
    "\n",
    "    index=1\n",
    "    if len(preprocessedtrain_data) != 0 and len(preprocesseddev_data) != 0:\n",
    "        # Separate loop for the preprocessed data as we cannot set the lowercase or strip accents parameters on these\n",
    "        for i in my_feature_sizes:\n",
    "            for x in my_stop_words:\n",
    "                print(\"%s: Processing the next vector from preprocessed data, index %d\" % (str(datetime.datetime.now().time()),index))\n",
    "                index +=1\n",
    "#               ### Count Vectorizer removed as the results are identical (to millions of a %) for count and tfidf\n",
    "#               ### vectorizer.  We stick with just tfidf as this works best for the other models\n",
    "#                # Same but with the preprocessed data\n",
    "#                vect = CountVectorizer(tokenizer=identity, max_features=i, stop_words=x,strip_accents=None, lowercase=False)\n",
    "#                vect_train= vect.fit_transform(train_preproc_data)\n",
    "#                vect_dev= vect.transform(dev_preproc_data)\n",
    "#                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 1, 0, i, x, False, None]\n",
    "\n",
    "                # Create a tfidf but fit with the preprocessed data\n",
    "                vect = TfidfVectorizer(tokenizer=identity,max_features=i, stop_words=x,strip_accents=None, lowercase=False)\n",
    "                vect_train = vect.fit_transform(train_preproc_data)\n",
    "                vect_dev = vect.transform(dev_preproc_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 1, 0, i, x, False, None]\n",
    "\n",
    "    if verbose == True:\n",
    "        print('%s: Completed create_all_vectors' % (str(datetime.datetime.now().time())))\n",
    "        \n",
    "    return vectors_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_all_models (vectors_all, score_types = ['precision', 'recall', 'roc_auc'], \n",
    "                                model_types = ['multi', 'bern'], verbose=False):\n",
    "    \"\"\"This function takes a vector of type vectors_all (defined above) acts as a wrapper\n",
    "    to send each vector to the score_classifier_train_on_dev function for scoring.  The\n",
    "    resulting scores are stored in a dataframe and returned\n",
    "    \n",
    "    Args:\n",
    "        vectors_all (dataframe) : a dataframe defined above that stores the vector data in each row\n",
    "        score_types (list) : a list of scoring types to be passed to the scoring\n",
    "        model_types (list) : a list of the Naive Bayes model types to create when scoring these vectors\n",
    "        verbose (bool): print out progress when true\n",
    "    Returns\n",
    "        dataframe: A dataframe of all the resulting scores and the details for each model\n",
    "    \"\"\"\n",
    "    data_all=pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'score_type', 'score'])\n",
    "#     score_types = ['precision', 'recall', 'roc_auc']\n",
    "#     model_types = ['multi', 'bern']\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('%s: Starting calculate_score_all_models' % (str(datetime.datetime.now().time())))        \n",
    "    for index,row in vectors_all.iterrows():\n",
    "        if verbose == True:\n",
    "            print('%s: checking row %d' % (str(datetime.datetime.now().time()),index))\n",
    "        for name in target_names:\n",
    "            for score_type in score_types:\n",
    "                for model_type in model_types:\n",
    "                    # Calculate the score for this pair of vectors with a variety of scoring\n",
    "                    # parameters and types of NB classifier\n",
    "                    alpha, score = score_classifier_train_on_dev(train_vector=row['vectortrain'], \n",
    "                                        dev_vector=row['vectordev'], dev_labels=dev_labels,\n",
    "                                        train_labels=train_labels, label=name, ctype=model_type, pscoring=score_type )\n",
    "                    \n",
    "                    # Store all the results in the dataframe\n",
    "                    data_all.loc[data_all.shape[0]] = [index,name,model_type,alpha,row['type'], \n",
    "                                        row['preprocessor'], row['tokenizer'], row['max_features'], \n",
    "                                        row['stop_words'], row['lowercase'], row['strip_accents'],\n",
    "                                        score_type, score]\n",
    "    if verbose == True:\n",
    "        print('%s: finished calculate_score_all_models' % (str(datetime.datetime.now().time())))               \n",
    "    return data_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out_predictions(vector_all_vectors,vectors_to_predict, output_file, verbose=True):\n",
    "    \"\"\"This function takes a list of all the vectors and a subset of results.  Using the subset\n",
    "    it extracts the training and dev vectors then creates the models, trains them and writes the predictions\n",
    "    to the output file.  This does a prediction of each label and writes them out to the file.\n",
    "    \n",
    "    Args:\n",
    "        vector_all_vectors (dataframe of vector information): This is the dataframe used to store the \n",
    "            vectors.  It has the following fields:\n",
    "                ['vectortrain', 'vectordev','type','preprocessor', 'tokenizer',\n",
    "                    'max_features', 'stop_words', 'lowercase', 'strip_accents' ]\n",
    "        vectors_to_predict (dataframe of results): This is hte dataframe used to store parameters and\n",
    "            results.  It has the following fields:\n",
    "                ['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                    'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                    'score_type', 'score'])\n",
    "        output_file (string): output file name\n",
    "        verbose (bool): extra printing of information\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    \n",
    "    df_store = pd.DataFrame()\n",
    "    if verbose == True:\n",
    "        print(\"%s: Starting write_out_predictions to %s\" % (str(datetime.datetime.now().time()),output_file))\n",
    "    for index,row in vectors_to_predict.iterrows():\n",
    "        stored_vector_entry = vector_all_vectors.loc[row['vectorno']]\n",
    "        if row['model'] == 'multi':\n",
    "            nb_class = MultinomialNB(alpha=row['alpha'].get('alpha')).fit(stored_vector_entry['vectortrain'], train_labels['toxic'])\n",
    "        elif row['model'] == 'bern':\n",
    "            nb_class = BernoulliNB(alpha=row['alpha'].get('alpha')).fit(stored_vector_entry['vectortrain'], train_labels[row['label']])\n",
    "        else:\n",
    "            print('%s: Error, row is %s' % (str(datetime.datetime.now().time()),row['model'] ))\n",
    "        result_tmp=nb_class.predict(stored_vector_entry['vectordev'])\n",
    "        df_store[row['label']] = result_tmp\n",
    "    if verbose == True:\n",
    "        print(\"%s: Predictions done, writing out\" % (str(datetime.datetime.now().time())))\n",
    "    df_store.to_csv(output_file, index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out_results(result_df):\n",
    "    \"\"\" This is a wrapper function around the writing out of results.  It writes out a large number\n",
    "    of csv and parm files.  These files are the prediction files and the parameters for each of the\n",
    "    classifiers in the predictions.  The idea here is to write out the following sets of results\n",
    "    1) Absolute best scores\n",
    "    2) Best scores with TFIDF (once I've removed the CountVectorizer this will be the same as 1)\n",
    "    3) Top scores for the NTLK preprocessed data\n",
    "    4) Top scores for TFIDF sorted by feature sizes\n",
    "    5) Top scores for TFIDF with NLTK preprocessing sorted by feature size\n",
    "    With each of these output sets we also write out a parm file which can be rearead later using \n",
    "    the function read_create_classifiers to recreate the vectors and classifiers to match these results.\n",
    "    \n",
    "    Args: \n",
    "        result_df (dataframe of results)\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    score_types = ['precision', 'recall', 'roc_auc']\n",
    "\n",
    "    top_score_results = pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                       'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                       'score_type', 'score'])\n",
    "    filename_prefix = 'NB_predictions_'\n",
    "    # Top scores irrespective\n",
    "    for score_type in score_types:\n",
    "        top_score_results_tmp = top_score_results[0:0]  # empty dataframe in each loop\n",
    "        for label in target_names:\n",
    "            df_tmp = result_df[(result_df['label'] == label) & (result_df['score_type'] == score_type)]\n",
    "            top_score_results_tmp.loc[top_score_results_tmp.shape[0]] = df_tmp.loc[df_tmp['score'].idxmax()]\n",
    "        filename_csv = filename_prefix + 'top_scores_nofilter_' + score_type + '.csv'\n",
    "        filename_parm = filename_prefix + 'top_scores_nofilter_' + score_type + '.parm'\n",
    "        write_out_predictions(vectors_all,top_score_results_tmp, filename_csv, verbose)\n",
    "        top_score_results_tmp.to_csv(filename_parm, index=False)\n",
    "        print(\"Output written to %s\" %(filename_csv))\n",
    "        top_score_results_tmp\n",
    "\n",
    "    # Top Scores for TFIDF - Should be identical\n",
    "    for score_type in score_types:\n",
    "        top_score_results_tmp = top_score_results[0:0]  # empty dataframe in each loop\n",
    "        for label in target_names:\n",
    "            df_tmp = result_df[(result_df['label'] == label) & (result_df['score_type'] == score_type) &\n",
    "                              (result_df['type'] == 'tfidf')]\n",
    "            top_score_results_tmp.loc[top_score_results_tmp.shape[0]] = df_tmp.loc[df_tmp['score'].idxmax()]\n",
    "        filename_csv = filename_prefix + 'top_scores_tfidf_' + score_type + '.csv'\n",
    "        filename_parm = filename_prefix + 'top_scores_tfidf_' + score_type + '.parm'\n",
    "        write_out_predictions(vectors_all,top_score_results_tmp, filename_csv, verbose)\n",
    "        top_score_results_tmp.to_csv(filename_parm, index=False)\n",
    "        print(\"Output written to %s\" %(filename_csv))\n",
    "        top_score_results_tmp\n",
    "\n",
    "    # Top Scores for TFIDF with NTLK preprocessed data\n",
    "    for score_type in score_types:\n",
    "        top_score_results_tmp = top_score_results[0:0]  # empty dataframe in each loop\n",
    "        for label in target_names:\n",
    "            df_tmp = result_df[(result_df['label'] == label) & (result_df['score_type'] == score_type) &\n",
    "                              (result_df['type'] == 'tfidf') & (result_df['preprocessor'] == 1)]\n",
    "            top_score_results_tmp.loc[top_score_results_tmp.shape[0]] = df_tmp.loc[df_tmp['score'].idxmax()]\n",
    "        filename_csv = filename_prefix + 'top_scores_tfidf_preproc_' + score_type + '.csv'\n",
    "        filename_parm = filename_prefix + 'top_scores_tfidf_preproc_' + score_type + '.parm'\n",
    "        write_out_predictions(vectors_all,top_score_results_tmp, filename_csv, verbose)\n",
    "        top_score_results_tmp.to_csv(filename_parm, index=False)\n",
    "        print(\"Output written to %s\" %(filename_csv))\n",
    "\n",
    "    sizes=[3000, 4000, 5000, 6000, 10000]\n",
    "    # Top Scores for TFIDF filtered by size\n",
    "    for size in sizes:\n",
    "        for score_type in score_types:\n",
    "            top_score_results_tmp = top_score_results[0:0]  # empty dataframe in each loop\n",
    "            for label in target_names:\n",
    "                df_tmp = result_df[(result_df['label'] == label) & (result_df['score_type'] == score_type) &\n",
    "                                  (result_df['type'] == 'tfidf') & (result_df['max_features'] == size)]\n",
    "                top_score_results_tmp.loc[top_score_results_tmp.shape[0]] = df_tmp.loc[df_tmp['score'].idxmax()]\n",
    "            filename_csv = filename_prefix + 'top_scores_tfidf_' + score_type + '_' + str(size) + '.csv'\n",
    "            filename_parm = filename_prefix + 'top_scores_tfidf_' + score_type + '_' + str(size) + '.parm'\n",
    "            write_out_predictions(vectors_all,top_score_results_tmp, filename_csv, verbose)\n",
    "            top_score_results_tmp.to_csv(filename_parm, index=False)\n",
    "            print(\"Output written to %s\" %(filename_csv))\n",
    "\n",
    "    # Top Scores for TFIDF with NTLK preprocessed data\n",
    "    for size in sizes:\n",
    "        for score_type in score_types:\n",
    "            top_score_results_tmp = top_score_results[0:0]  # empty dataframe in each loop\n",
    "            for label in target_names:\n",
    "                df_tmp = result_df[(result_df['label'] == label) & (result_df['score_type'] == score_type) &\n",
    "                                  (result_df['type'] == 'tfidf') & (result_df['preprocessor'] == 1) &\n",
    "                                  (result_df['max_features'] == size)]\n",
    "                top_score_results_tmp.loc[top_score_results_tmp.shape[0]] = df_tmp.loc[df_tmp['score'].idxmax()]\n",
    "            filename_csv = filename_prefix + 'top_scores_tfidf_preproc_' + score_type + '_' + str(size) + '.csv'\n",
    "            filename_parm = filename_prefix + 'top_scores_tfidf_preproc_' + score_type + '_' + str(size) + '.parm'\n",
    "            write_out_predictions(vectors_all,top_score_results_tmp, filename_csv, verbose)\n",
    "            top_score_results_tmp.to_csv(filename_parm, index=False)\n",
    "            print(\"Output written to %s\" %(filename_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# To be used after a full run through\n",
    "#######################################################################################\n",
    "\n",
    "def read_create_classifiers(parm_file, train_data, preproc_train_data, train_labels):\n",
    "    \"\"\"Once the notebook has been run once and the results finalized this is the\n",
    "    only necessary function.  It can be used with any of the parm files written out in by the\n",
    "    write_out_results function to recreate the classifiers and return them for use.\n",
    "    \n",
    "    Args:\n",
    "        parm_file (string filename): a stored dataframe of parameters to create the classifier\n",
    "        train_data (unprocessed training data): the unprocessed training data\n",
    "        preproc_train_data: preprocessed training data (some parameters in call to vectorizer are\n",
    "            different when dealing with preprocessed data)\n",
    "        train_labels: a set of training labels that match the training data\n",
    "    Returns:\n",
    "        dictionary of classifiers: the index is the label of the classifier.  These are fitted classifiers\n",
    "            and can be used for predictions.\n",
    "    \"\"\"\n",
    "    input_parameters = pd.read_csv(parm_file)\n",
    "    return_classifiers = pd.DataFrame(columns=['label', 'classifier'])\n",
    "    return_classifiers = {}\n",
    "    \n",
    "#     vectorno,label,model,alpha,type,preprocessor,tokenizer,max_features,stop_words,lowercase,strip_accents,score_type,score\n",
    "    for index,row in input_parameters.iterrows():\n",
    "        if row['stop_words'] == None or row['stop_words'] is np.nan:\n",
    "            stop_words = None\n",
    "        else:\n",
    "            stop_words = row['stop_words']\n",
    "        if row['preprocessor'] == 1:\n",
    "            vect = TfidfVectorizer(tokenizer=identity, max_features=row['max_features'],\n",
    "                                   stop_words=stop_words,\n",
    "                                   lowercase=False,\n",
    "                                   strip_accents=None)\n",
    "            vect_train = vect.fit_transform(preproc_train_data)\n",
    "\n",
    "        else:\n",
    "            vect = tfidfVectorizer(tokenizer=identity, max_features=row['max_features'],\n",
    "                                   stop_words=stop_words,\n",
    "                                   strip_accents=row['strip_accents'],\n",
    "                                   lowercase=row['lowercase'])\n",
    "            vect_train = vect.fit_transform(train_data)\n",
    "\n",
    "        alpha_loc = ast.literal_eval(row['alpha']).get('alpha')\n",
    "        if row['model'] == 'bern':\n",
    "            nb_class = BernoulliNB(alpha=alpha_loc).fit(vect_train, train_labels[row['label']])\n",
    "        elif row['model'] == 'multi':\n",
    "            nb_class = MultinomialNB(alpha=alpha_loc).fit(vect_train, train_labels[row['label']])\n",
    "        # add the classifier with the labels to the return dataframe\n",
    "        #return_classifiers.loc[return_classifiers.shape[0]] = [row['label'],nb_class]\n",
    "        return_classifiers[row['label']] = nb_class\n",
    "    return return_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:19:36.698878: Starting write_out_predictions to NB_predictions_top_scores_nofilter_precision.csv\n",
      "18:19:37.172265: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_nofilter_precision.csv\n",
      "18:19:38.784877: Starting write_out_predictions to NB_predictions_top_scores_nofilter_recall.csv\n",
      "18:19:39.208959: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_nofilter_recall.csv\n",
      "18:19:40.824726: Starting write_out_predictions to NB_predictions_top_scores_nofilter_roc_auc.csv\n",
      "18:19:41.197482: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_nofilter_roc_auc.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:19:42.849546: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision.csv\n",
      "18:19:43.311827: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision.csv\n",
      "18:19:44.951297: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall.csv\n",
      "18:19:45.369818: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall.csv\n",
      "18:19:47.181737: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc.csv\n",
      "18:19:47.548588: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:19:49.189539: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision.csv\n",
      "18:19:49.517279: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision.csv\n",
      "18:19:51.511571: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall.csv\n",
      "18:19:51.934591: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall.csv\n",
      "18:19:53.623529: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc.csv\n",
      "18:19:53.996420: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:19:55.765370: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision_3000.csv\n",
      "18:19:56.044802: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision_3000.csv\n",
      "18:19:57.708326: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall_3000.csv\n",
      "18:19:58.122060: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall_3000.csv\n",
      "18:19:59.809457: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc_3000.csv\n",
      "18:20:00.200865: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc_3000.csv\n",
      "18:20:01.851996: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision_4000.csv\n",
      "18:20:02.260101: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision_4000.csv\n",
      "18:20:04.245469: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall_4000.csv\n",
      "18:20:04.846151: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall_4000.csv\n",
      "18:20:06.552263: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc_4000.csv\n",
      "18:20:06.921346: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc_4000.csv\n",
      "18:20:08.546157: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision_5000.csv\n",
      "18:20:08.834564: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision_5000.csv\n",
      "18:20:10.451487: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall_5000.csv\n",
      "18:20:10.869881: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall_5000.csv\n",
      "18:20:12.491649: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc_5000.csv\n",
      "18:20:12.895032: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc_5000.csv\n",
      "18:20:14.508813: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision_6000.csv\n",
      "18:20:14.797049: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision_6000.csv\n",
      "18:20:16.441773: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall_6000.csv\n",
      "18:20:16.886288: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall_6000.csv\n",
      "18:20:18.493378: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc_6000.csv\n",
      "18:20:18.900530: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc_6000.csv\n",
      "18:20:20.514050: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision_10000.csv\n",
      "18:20:20.805280: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision_10000.csv\n",
      "18:20:22.412154: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall_10000.csv\n",
      "18:20:22.869948: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall_10000.csv\n",
      "18:20:24.472713: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc_10000.csv\n",
      "18:20:24.898705: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc_10000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:20:26.549568: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision_3000.csv\n",
      "18:20:26.781067: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision_3000.csv\n",
      "18:20:28.391620: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall_3000.csv\n",
      "18:20:28.763770: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall_3000.csv\n",
      "18:20:30.358156: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc_3000.csv\n",
      "18:20:30.713372: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc_3000.csv\n",
      "18:20:32.314468: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision_4000.csv\n",
      "18:20:32.554692: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision_4000.csv\n",
      "18:20:34.150083: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall_4000.csv\n",
      "18:20:34.528586: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall_4000.csv\n",
      "18:20:36.116970: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc_4000.csv\n",
      "18:20:36.479114: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc_4000.csv\n",
      "18:20:38.075618: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision_5000.csv\n",
      "18:20:38.324163: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision_5000.csv\n",
      "18:20:39.921177: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall_5000.csv\n",
      "18:20:40.305389: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall_5000.csv\n",
      "18:20:41.956608: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc_5000.csv\n",
      "18:20:42.349969: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc_5000.csv\n",
      "18:20:43.996755: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision_6000.csv\n",
      "18:20:44.256458: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision_6000.csv\n",
      "18:20:45.856576: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall_6000.csv\n",
      "18:20:46.251263: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall_6000.csv\n",
      "18:20:47.852849: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc_6000.csv\n",
      "18:20:48.248430: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc_6000.csv\n",
      "18:20:49.864462: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision_10000.csv\n",
      "18:20:50.129002: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision_10000.csv\n",
      "18:20:51.730176: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall_10000.csv\n",
      "18:20:52.142404: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall_10000.csv\n",
      "18:20:53.744414: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc_10000.csv\n",
      "18:20:54.138565: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc_10000.csv\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# MAIN\n",
    "#############################################\n",
    "# This block does the following:\n",
    "# 1. Create the NLTK Preprocessed data\n",
    "# 2. Create the set of vectors with the unprocessed and preprocessed data\n",
    "# 3. Calculates the scores on all models\n",
    "\n",
    "# More progress printing when this is True\n",
    "verbose=True\n",
    "\n",
    "\n",
    "# Create the NLTK preprocessed data\n",
    "if verbose == True:\n",
    "    print('%s: transforming training data with NLTK preprocessor' %(str(datetime.datetime.now().time())))\n",
    "train_preproc_data = NLTKPreprocessor().fit(train_data).transform(train_data)\n",
    "if verbose == True:\n",
    "    print('%s: transforming dev data with NLTK preprocessor' %(str(datetime.datetime.now().time())))\n",
    "dev_preproc_data = NLTKPreprocessor().fit(dev_data).transform(dev_data)\n",
    "if verbose == True:\n",
    "    print('%s: completed NLTK preprocessor' %(str(datetime.datetime.now().time())))\n",
    "\n",
    "# Create the set of vectors:\n",
    "vectors_all = create_all_vectors(basetrain_data=train_data, basedev_data=dev_data,\n",
    "                        preprocessedtrain_data=train_preproc_data, preprocesseddev_data=dev_preproc_data,\n",
    "                        verbose=verbose)\n",
    "\n",
    "# calculate the scores for all the models\n",
    "result_df = calculate_score_all_models(vectors_all,verbose=verbose)\n",
    "\n",
    "# Write out all the results\n",
    "result_df.to_csv('all_results_' + str(datetime.date.today()) + \".csv\", index=False)\n",
    "\n",
    "# this stores the results of the predictions and parameters to a set of output files\n",
    "write_out_results(result_df)\n",
    "if verbose == True:\n",
    "    print('%s: completed main' %(str(datetime.datetime.now().time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:25:29.375524: start test read_create_classifiers\n",
      "18:25:52.061786: completed test read_create_classifiers\n"
     ]
    }
   ],
   "source": [
    "# A quick test that we can reread and create classifiers with the read_create_classifiers function\n",
    "if verbose == True:\n",
    "    print('%s: start test read_create_classifiers' %(str(datetime.datetime.now().time())))\n",
    "result_classifiers = read_create_classifiers('NB_predictions_top_scores_tfidf_preproc_roc_auc.parm', \n",
    "                                             train_data, train_preproc_data, train_labels)\n",
    "\n",
    "if verbose == True:\n",
    "    print('%s: completed test read_create_classifiers' %(str(datetime.datetime.now().time())))\n",
    "    \n",
    "result_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
